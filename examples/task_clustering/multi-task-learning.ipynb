{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:46:24.772069Z",
     "iopub.status.busy": "2025-05-06T13:46:24.771707Z",
     "iopub.status.idle": "2025-05-06T13:46:24.778643Z",
     "shell.execute_reply": "2025-05-06T13:46:24.777159Z",
     "shell.execute_reply.started": "2025-05-06T13:46:24.772041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp, softmax\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.linalg import solve_triangular, cholesky\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:46:26.885702Z",
     "iopub.status.busy": "2025-05-06T13:46:26.885375Z",
     "iopub.status.idle": "2025-05-06T13:46:26.891280Z",
     "shell.execute_reply": "2025-05-06T13:46:26.890009Z",
     "shell.execute_reply.started": "2025-05-06T13:46:26.885679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:46:29.029363Z",
     "iopub.status.busy": "2025-05-06T13:46:29.028964Z",
     "iopub.status.idle": "2025-05-06T13:46:29.057202Z",
     "shell.execute_reply": "2025-05-06T13:46:29.055518Z",
     "shell.execute_reply.started": "2025-05-06T13:46:29.029338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiTaskNN:\n",
    "    def __init__(self, n_input, n_hidden, n_tasks, activation='tanh'):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_tasks = n_tasks\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize with larger scale for better convergence\n",
    "        self.W = np.random.randn(n_hidden, n_input + 1) * 0.5\n",
    "        self.m = np.random.randn(n_hidden + 1) * 0.5\n",
    "        self.Sigma = np.eye(n_hidden + 1) * 0.5\n",
    "        self.sigma = 1.0\n",
    "        self.A_map = [np.zeros(n_hidden + 1) for _ in range(n_tasks)]\n",
    "    \n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation == 'linear':\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Activation must be 'tanh' or 'linear'\")\n",
    "    \n",
    "    def compute_hidden_activations(self, X):\n",
    "        X_bias = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "        H = self._activate(np.dot(X_bias, self.W.T))\n",
    "        return np.hstack([H, np.ones((H.shape[0], 1))])\n",
    "    \n",
    "    def predict(self, X, task_idx):\n",
    "        H_bias = self.compute_hidden_activations(X)\n",
    "        return np.dot(H_bias, self.A_map[task_idx])\n",
    "    \n",
    "    def compute_sufficient_statistics(self, X, y):\n",
    "        H_bias = self.compute_hidden_activations(X)\n",
    "        return {\n",
    "            'sum_hhT': np.dot(H_bias.T, H_bias),\n",
    "            'sum_hy': np.dot(H_bias.T, y),\n",
    "            'sum_yy': np.dot(y, y),\n",
    "            'n_samples': X.shape[0]\n",
    "        }\n",
    "    \n",
    "    def log_likelihood(self, params, all_stats):\n",
    "        try:\n",
    "            # Unpack parameters\n",
    "            param_idx = 0\n",
    "            \n",
    "            # W\n",
    "            W_size = self.n_hidden * (self.n_input + 1)\n",
    "            W = params[param_idx:param_idx + W_size].reshape(self.n_hidden, self.n_input + 1)\n",
    "            param_idx += W_size\n",
    "            \n",
    "            # m\n",
    "            m_size = self.n_hidden + 1\n",
    "            m = params[param_idx:param_idx + m_size]\n",
    "            param_idx += m_size\n",
    "            \n",
    "            # Sigma (Cholesky decomposition)\n",
    "            L = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n",
    "            tril_indices = np.tril_indices(self.n_hidden + 1)\n",
    "            L[tril_indices] = params[param_idx:param_idx + len(tril_indices[0])]\n",
    "            param_idx += len(tril_indices[0])\n",
    "            \n",
    "            # sigma (log scale)\n",
    "            log_sigma = params[param_idx]\n",
    "            sigma = np.exp(log_sigma)\n",
    "            \n",
    "            total_log_lik = 0.0\n",
    "            self.A_map = []\n",
    "            \n",
    "            # Add regularization to Sigma\n",
    "            Sigma = np.dot(L, L.T) + 1e-6 * np.eye(self.n_hidden + 1)\n",
    "            \n",
    "            # Precompute Sigma inverse using Cholesky\n",
    "            try:\n",
    "                L_sigma = cholesky(Sigma, lower=True)\n",
    "                Sigma_inv = solve_triangular(L_sigma, np.eye(self.n_hidden + 1), lower=True)\n",
    "                Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n",
    "            except np.linalg.LinAlgError:\n",
    "                return -np.inf\n",
    "            \n",
    "            for stats in all_stats:\n",
    "                sum_hhT = stats['sum_hhT']\n",
    "                sum_hy = stats['sum_hy']\n",
    "                sum_yy = stats['sum_yy']\n",
    "                n_samples = stats['n_samples']\n",
    "                \n",
    "                # Add small constant to avoid division by zero\n",
    "                sigma_sq = max(sigma**2, 1e-8)\n",
    "                \n",
    "                # Compute Q_i with regularization\n",
    "                Q_i = (1.0 / sigma_sq) * sum_hhT + Sigma_inv\n",
    "                \n",
    "                try:\n",
    "                    L_Q = cholesky(Q_i + 1e-6*np.eye(self.n_hidden + 1), lower=True)\n",
    "                    Q_inv = solve_triangular(L_Q, np.eye(self.n_hidden + 1), lower=True)\n",
    "                    Q_inv = np.dot(Q_inv.T, Q_inv)\n",
    "                except np.linalg.LinAlgError:\n",
    "                    return -np.inf\n",
    "                \n",
    "                R_i = (1.0 / sigma_sq) * sum_hy + np.dot(Sigma_inv, m)\n",
    "                \n",
    "                # Compute MAP estimate with regularization\n",
    "                A_i = np.linalg.solve(Q_i + 1e-6*np.eye(self.n_hidden + 1), R_i)\n",
    "                self.A_map.append(A_i)\n",
    "                \n",
    "                # Compute log determinants\n",
    "                logdet_Q_i = 2 * np.sum(np.log(np.diag(L_Q)))\n",
    "                logdet_Sigma = 2 * np.sum(np.log(np.diag(L_sigma)))\n",
    "                \n",
    "                # Compute log likelihood terms\n",
    "                term1 = -0.5 * (logdet_Sigma + n_samples * 2 * log_sigma + logdet_Q_i)\n",
    "                term2 = 0.5 * (np.dot(R_i, np.dot(Q_inv, R_i)) - (1.0 / sigma_sq) * sum_yy - np.dot(m, np.dot(Sigma_inv, m)))\n",
    "                \n",
    "                if not np.isfinite(term1 + term2):\n",
    "                    return -np.inf\n",
    "                \n",
    "                total_log_lik += term1 + term2\n",
    "            \n",
    "            return total_log_lik if np.isfinite(total_log_lik) else -np.inf\n",
    "        \n",
    "        except:\n",
    "            return -np.inf\n",
    "    \n",
    "    def fit(self, X_list, y_list, max_iter=100):\n",
    "        # Normalize data\n",
    "        X_list = [(X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8) for X in X_list]\n",
    "        y_list = [(y - np.mean(y)) / (np.std(y) + 1e-8) for y in y_list]\n",
    "        \n",
    "        # Compute sufficient statistics\n",
    "        all_stats = [self.compute_sufficient_statistics(X, y) for X, y in zip(X_list, y_list)]\n",
    "        \n",
    "        # Initial parameters with better scaling\n",
    "        initial_params = []\n",
    "        initial_params.extend(self.W.flatten())\n",
    "        initial_params.extend(self.m)\n",
    "        \n",
    "        # Initialize Sigma with Cholesky decomposition\n",
    "        L = np.linalg.cholesky(self.Sigma + 1e-6 * np.eye(self.n_hidden + 1))\n",
    "        tril_indices = np.tril_indices(self.n_hidden + 1)\n",
    "        initial_params.extend(L[tril_indices])\n",
    "        \n",
    "        initial_params.append(np.log(self.sigma))\n",
    "        \n",
    "        # Optimize with bounds for stability\n",
    "        bounds = []\n",
    "        bounds.extend([(None, None)] * (self.n_hidden * (self.n_input + 1)))  # W\n",
    "        bounds.extend([(None, None)] * (self.n_hidden + 1))  # m\n",
    "        \n",
    "        # L - diagonal elements must be positive\n",
    "        for i in range(len(tril_indices[0])):\n",
    "            if tril_indices[0][i] == tril_indices[1][i]:  # diagonal\n",
    "                bounds.append((1e-8, None))\n",
    "            else:\n",
    "                bounds.append((None, None))\n",
    "                \n",
    "        bounds.append((np.log(1e-8), None))  # log_sigma\n",
    "        \n",
    "        # Optimization with error handling\n",
    "        try:\n",
    "            result = minimize(\n",
    "                lambda p: -self.log_likelihood(p, all_stats),\n",
    "                initial_params,\n",
    "                method='L-BFGS-B',\n",
    "                bounds=bounds,\n",
    "                options={\n",
    "                    'maxiter': max_iter,\n",
    "                    'disp': True,\n",
    "                    'maxfun': 15000,  # Increased function evaluations\n",
    "                    'maxls': 50  # Increased line searches\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Store optimized parameters\n",
    "            self._unpack_parameters(result.x)\n",
    "            \n",
    "            # Recompute MAP estimates\n",
    "            _ = self.log_likelihood(result.x, all_stats)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _unpack_parameters(self, params):\n",
    "        \"\"\"Helper to unpack optimized parameters\"\"\"\n",
    "        param_idx = 0\n",
    "        \n",
    "        # W\n",
    "        W_size = self.n_hidden * (self.n_input + 1)\n",
    "        self.W = params[param_idx:param_idx + W_size].reshape(self.n_hidden, self.n_input + 1)\n",
    "        param_idx += W_size\n",
    "        \n",
    "        # m\n",
    "        m_size = self.n_hidden + 1\n",
    "        self.m = params[param_idx:param_idx + m_size]\n",
    "        param_idx += m_size\n",
    "        \n",
    "        # Sigma (Cholesky)\n",
    "        L = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n",
    "        tril_indices = np.tril_indices(self.n_hidden + 1)\n",
    "        L[tril_indices] = params[param_idx:param_idx + len(tril_indices[0])]\n",
    "        param_idx += len(tril_indices[0])\n",
    "        self.Sigma = np.dot(L, L.T) + 1e-6 * np.eye(self.n_hidden + 1)\n",
    "        \n",
    "        # sigma\n",
    "        self.sigma = max(np.exp(params[param_idx]), 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:46:34.217502Z",
     "iopub.status.busy": "2025-05-06T13:46:34.217154Z",
     "iopub.status.idle": "2025-05-06T13:46:34.228008Z",
     "shell.execute_reply": "2025-05-06T13:46:34.226524Z",
     "shell.execute_reply.started": "2025-05-06T13:46:34.217476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_tasks=20, n_samples_train=10, n_samples_test=300, \n",
    "                           n_input=10, n_hidden=1, activation='tanh'):\n",
    "    # True parameters\n",
    "    true_W = np.random.randn(n_hidden, n_input + 1)\n",
    "\n",
    "    true_Sigma = np.eye(n_hidden + 1) * 0.5\n",
    "    \n",
    "    # Two cluster means\n",
    "    true_m = np.array(\n",
    "        [1.5, 0.5])\n",
    "    \n",
    "    \n",
    "    # Generate data for each task\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for i in range(n_tasks):\n",
    "        # Generate covariates\n",
    "        X_train = np.random.randn(n_samples_train, n_input)\n",
    "        X_test = np.random.randn(n_samples_test, n_input)\n",
    "        \n",
    "        # Scale per task to zero mean and unit variance\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Compute hidden activations\n",
    "        X_train_bias = np.hstack([X_train, np.ones((n_samples_train, 1))])\n",
    "        X_test_bias = np.hstack([X_test, np.ones((n_samples_test, 1))])\n",
    "        \n",
    "        if activation == 'tanh':\n",
    "            h_train = np.tanh(np.dot(X_train_bias, true_W.T))\n",
    "            h_test = np.tanh(np.dot(X_test_bias, true_W.T))\n",
    "        else:\n",
    "            h_train = np.dot(X_train_bias, true_W.T)\n",
    "            h_test = np.dot(X_test_bias, true_W.T)\n",
    "        \n",
    "        # Add bias term\n",
    "        h_train = np.hstack([h_train, np.ones((n_samples_train, 1))])\n",
    "        h_test = np.hstack([h_test, np.ones((n_samples_test, 1))])\n",
    "\n",
    "        # Generate task-specific weights from true distribution\n",
    "        A = np.random.multivariate_normal(true_m, true_Sigma)\n",
    "        \n",
    "        # Generate responses with noise\n",
    "        y_train = np.dot(h_train, A) + np.random.randn(n_samples_train)*0.1\n",
    "        y_test = np.dot(h_test, A) + np.random.randn(n_samples_test)*0.1\n",
    "        \n",
    "        train_data.append((X_train, y_train))\n",
    "        test_data.append((X_test, y_test))\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:46:37.519004Z",
     "iopub.status.busy": "2025-05-06T13:46:37.518560Z",
     "iopub.status.idle": "2025-05-06T13:46:37.524534Z",
     "shell.execute_reply": "2025-05-06T13:46:37.523363Z",
     "shell.execute_reply.started": "2025-05-06T13:46:37.518973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data):\n",
    "    # 1. Test MSE\n",
    "    mse = 0\n",
    "    for i, (X_test, y_test) in enumerate(test_data):\n",
    "        y_pred = model.predict(X_test, i)\n",
    "        mse += np.mean((y_test - y_pred)**2)\n",
    "    mse /= len(test_data)\n",
    "    \n",
    "    return {\n",
    "        'test_mse': mse,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:46:39.338255Z",
     "iopub.status.busy": "2025-05-06T13:46:39.337893Z",
     "iopub.status.idle": "2025-05-06T13:46:39.346131Z",
     "shell.execute_reply": "2025-05-06T13:46:39.345242Z",
     "shell.execute_reply.started": "2025-05-06T13:46:39.338232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_simulations(n_simulations=10, activation='tanh'):\n",
    "    results = []\n",
    "    successful_simulations = 0\n",
    "    \n",
    "    for sim in range(n_simulations):\n",
    "        print(f\"\\nSimulation {sim+1}/{n_simulations}\")\n",
    "        \n",
    "        try:\n",
    "            # Generate data\n",
    "            train_data, test_data = generate_synthetic_data(activation=activation)\n",
    "            \n",
    "            # Initialize and fit model\n",
    "            model = MultiTaskNN(\n",
    "                n_input=10,\n",
    "                n_hidden=1,\n",
    "                n_tasks=20,\n",
    "                activation=activation\n",
    "            )\n",
    "\n",
    "            X = []\n",
    "            y = []\n",
    "            for i, (X_i, y_i) in enumerate(train_data):\n",
    "                X.append(X_i)\n",
    "                y.append(y_i)\n",
    "            \n",
    "            # Fit model with error handling\n",
    "            fit_result = model.fit(X, y, max_iter=100)\n",
    "            if fit_result is None:\n",
    "                print(\"Skipping simulation due to fitting error\")\n",
    "                continue\n",
    "                \n",
    "            # Evaluate\n",
    "            metrics = evaluate_model(model, test_data)\n",
    "            results.append(metrics)\n",
    "            successful_simulations += 1\n",
    "            \n",
    "            print(f\"Test MSE: {metrics['test_mse']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in simulation {sim+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if successful_simulations == 0:\n",
    "        print(\"Warning: All simulations failed\")\n",
    "        return None\n",
    "    \n",
    "    # Aggregate results\n",
    "    avg_results = {\n",
    "        'avg_test_mse': np.mean([r['test_mse'] for r in results]),\n",
    "        'success_rate': successful_simulations / n_simulations\n",
    "    }\n",
    "    \n",
    "    return avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:14:26.397840Z",
     "iopub.status.busy": "2025-05-06T12:14:26.397184Z",
     "iopub.status.idle": "2025-05-06T12:14:47.721875Z",
     "shell.execute_reply": "2025-05-06T12:14:47.720993Z",
     "shell.execute_reply.started": "2025-05-06T12:14:26.397813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with tanh activation:\n",
      "\n",
      "Simulation 1/10\n",
      "Test MSE: 2.9261\n",
      "\n",
      "Simulation 2/10\n",
      "Test MSE: 2.6003\n",
      "\n",
      "Simulation 3/10\n",
      "Test MSE: 2.8722\n",
      "\n",
      "Simulation 4/10\n",
      "Test MSE: 2.8375\n",
      "\n",
      "Simulation 5/10\n",
      "Test MSE: 1.6876\n",
      "\n",
      "Simulation 6/10\n",
      "Test MSE: 3.7151\n",
      "\n",
      "Simulation 7/10\n",
      "Test MSE: 4.1093\n",
      "\n",
      "Simulation 8/10\n",
      "Test MSE: 3.7436\n",
      "\n",
      "Simulation 9/10\n",
      "Test MSE: 3.1874\n",
      "\n",
      "Simulation 10/10\n",
      "Test MSE: 3.2853\n",
      "\n",
      "Testing with linear activation:\n",
      "\n",
      "Simulation 1/10\n",
      "Test MSE: 51.3946\n",
      "\n",
      "Simulation 2/10\n",
      "Test MSE: 11.9994\n",
      "\n",
      "Simulation 3/10\n",
      "Test MSE: 28.1998\n",
      "\n",
      "Simulation 4/10\n",
      "Test MSE: 13.0579\n",
      "\n",
      "Simulation 5/10\n",
      "Test MSE: 68.7731\n",
      "\n",
      "Simulation 6/10\n",
      "Test MSE: 48.0725\n",
      "\n",
      "Simulation 7/10\n",
      "Test MSE: 45.6132\n",
      "\n",
      "Simulation 8/10\n",
      "Test MSE: 26.2639\n",
      "\n",
      "Simulation 9/10\n",
      "Test MSE: 83.5740\n",
      "\n",
      "Simulation 10/10\n",
      "Test MSE: 22.4269\n",
      "\n",
      "Final Results:\n",
      "Tanh activation: {'avg_test_mse': 3.0964356174038157, 'success_rate': 1.0}\n",
      "Linear activation: {'avg_test_mse': 39.93753318607524, 'success_rate': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing with tanh activation:\")\n",
    "tanh_results = run_simulations(activation='tanh')\n",
    "\n",
    "print(\"\\nTesting with linear activation:\")\n",
    "linear_results = run_simulations(activation='linear')\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"Tanh activation:\", tanh_results)\n",
    "print(\"Linear activation:\", linear_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-dependent Prior Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:46:46.292094Z",
     "iopub.status.busy": "2025-05-06T13:46:46.291774Z",
     "iopub.status.idle": "2025-05-06T13:46:46.317810Z",
     "shell.execute_reply": "2025-05-06T13:46:46.316815Z",
     "shell.execute_reply.started": "2025-05-06T13:46:46.292071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiTaskNNDependentMean:\n",
    "    def __init__(self, n_input, n_hidden, n_tasks, n_features, activation='tanh'):\n",
    "        \"\"\"\n",
    "        Initialize the multi-task neural network model.\n",
    "        \n",
    "        Args:\n",
    "            n_input: Number of input features\n",
    "            n_hidden: Number of hidden units\n",
    "            n_tasks: Number of tasks\n",
    "            n_features: Number of task features\n",
    "            activation: Activation function ('tanh' or 'linear')\n",
    "        \"\"\"\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_tasks = n_tasks\n",
    "        self.n_features = n_features\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize shared parameters with larger scale\n",
    "        self.W = np.random.randn(n_hidden, n_input + 1) * 0.5\n",
    "        \n",
    "        # Initialize hyperparameters\n",
    "        self.M = np.random.randn(n_hidden + 1, n_features) * 0.1\n",
    "        self.Sigma = np.eye(n_hidden + 1)\n",
    "        self.sigma = 1.0\n",
    "        \n",
    "        # Store MAP estimates\n",
    "        self.A_map = [np.zeros(n_hidden + 1) for _ in range(n_tasks)]\n",
    "    \n",
    "    def _activate(self, x):\n",
    "        \"\"\"Apply activation function to hidden units\"\"\"\n",
    "        if self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation == 'linear':\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Activation must be 'tanh' or 'linear'\")\n",
    "    \n",
    "    def compute_hidden_activations(self, X):\n",
    "        \"\"\"Compute hidden unit activations\"\"\"\n",
    "        X_bias = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "        H = self._activate(np.dot(X_bias, self.W.T))\n",
    "        return np.hstack([H, np.ones((H.shape[0], 1))])\n",
    "    \n",
    "    def predict(self, X, task_idx, task_features):\n",
    "        \"\"\"Make predictions for a specific task\"\"\"\n",
    "        H_bias = self.compute_hidden_activations(X)\n",
    "        return np.dot(H_bias, self.A_map[task_idx])\n",
    "    \n",
    "    def compute_sufficient_statistics(self, X, y):\n",
    "        \"\"\"Compute sufficient statistics for a single task\"\"\"\n",
    "        H_bias = self.compute_hidden_activations(X)\n",
    "        return {\n",
    "            'sum_hhT': np.dot(H_bias.T, H_bias),\n",
    "            'sum_hy': np.dot(H_bias.T, y),\n",
    "            'sum_yy': np.dot(y, y),\n",
    "            'n_samples': X.shape[0]\n",
    "        }\n",
    "    \n",
    "    def log_likelihood(self, params, all_stats, all_task_features):\n",
    "        \"\"\"Compute the log likelihood with numerical stability improvements\"\"\"\n",
    "        # Unpack parameters\n",
    "        param_idx = 0\n",
    "        \n",
    "        # W\n",
    "        W_size = self.n_hidden * (self.n_input + 1)\n",
    "        W = params[param_idx:param_idx + W_size].reshape(self.n_hidden, self.n_input + 1)\n",
    "        param_idx += W_size\n",
    "\n",
    "        # M: (n_hidden + 1 x n_features)\n",
    "        M_size = (self.n_hidden + 1) * self.n_features\n",
    "        M = params[param_idx:param_idx + M_size].reshape(self.n_hidden + 1, self.n_features)\n",
    "        param_idx += M_size\n",
    "        \n",
    "        \n",
    "        # Sigma (Cholesky decomposition)\n",
    "        L = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n",
    "        tril_indices = np.tril_indices(self.n_hidden + 1)\n",
    "        L[tril_indices] = params[param_idx:param_idx + len(tril_indices[0])]\n",
    "        param_idx += len(tril_indices[0])\n",
    "        \n",
    "        # sigma (log scale)\n",
    "        log_sigma = params[param_idx]\n",
    "        sigma = np.exp(log_sigma)\n",
    "        \n",
    "        total_log_lik = 0.0\n",
    "        self.A_map = []\n",
    "        \n",
    "        # Precompute Sigma inverse using Cholesky\n",
    "        try:\n",
    "            Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n",
    "            Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n",
    "        except np.linalg.LinAlgError:\n",
    "            return -np.inf  # Invalid covariance matrix\n",
    "            \n",
    "        for stats in all_stats:\n",
    "            sum_hhT = stats['sum_hhT']\n",
    "            sum_hy = stats['sum_hy']\n",
    "            sum_yy = stats['sum_yy']\n",
    "            n_samples = stats['n_samples']\n",
    "\n",
    "            # Compute task-dependent prior mean\n",
    "            m_i = np.dot(M, task_features)\n",
    "            \n",
    "            # Compute Q_i using Cholesky for stability\n",
    "            Q_i = (1.0 / (sigma**2)) * sum_hhT + Sigma_inv\n",
    "            \n",
    "            try:\n",
    "                L_Q = np.linalg.cholesky(Q_i)\n",
    "                Q_inv = solve_triangular(L_Q, np.eye(self.n_hidden + 1), lower=True)\n",
    "                Q_inv = np.dot(Q_inv.T, Q_inv)\n",
    "            except np.linalg.LinAlgError:\n",
    "                return -np.inf\n",
    "\n",
    "            R_i = (1.0 / (sigma**2)) * sum_hy + np.dot(Sigma_inv, m_i)\n",
    "            \n",
    "            # Compute MAP estimate\n",
    "            A_i = np.dot(Q_inv, R_i)\n",
    "            self.A_map.append(A_i)\n",
    "            \n",
    "            # Compute log determinants efficiently\n",
    "            logdet_Q_i = 2 * np.sum(np.log(np.diag(L_Q)))\n",
    "            logdet_Sigma = 2 * np.sum(np.log(np.diag(L)))\n",
    "            \n",
    "            # Compute log likelihood terms\n",
    "            term1 = -0.5 * (logdet_Sigma + n_samples * 2 * log_sigma + logdet_Q_i)\n",
    "            term2 = 0.5 * (np.dot(R_i, np.dot(Q_inv, R_i)) - (1.0 / (sigma**2)) * sum_yy - np.dot(m_i, np.dot(Sigma_inv, m_i)))\n",
    "            \n",
    "            total_log_lik += term1 + term2\n",
    "        \n",
    "        return total_log_lik\n",
    "    \n",
    "    def fit(self, X_list, y_list, task_features_list, max_iter=100):\n",
    "        \"\"\"Fit the model with improved optimization\"\"\"\n",
    "        # Normalize data\n",
    "        X_list = [(X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8) for X in X_list]\n",
    "        y_list = [(y - np.mean(y)) / (np.std(y) + 1e-8) for y in y_list]\n",
    "        \n",
    "        # Compute sufficient statistics\n",
    "        all_stats = [self.compute_sufficient_statistics(X, y) for X, y in zip(X_list, y_list)]\n",
    "        \n",
    "        # Initial parameters with better scaling\n",
    "        initial_params = []\n",
    "        initial_params.extend(self.W.flatten())\n",
    "        initial_params.extend(self.M.flatten())\n",
    "        \n",
    "        L = np.linalg.cholesky(self.Sigma + 1e-6 * np.eye(self.n_hidden + 1))\n",
    "        tril_indices = np.tril_indices(self.n_hidden + 1)\n",
    "        initial_params.extend(L[tril_indices])\n",
    "        \n",
    "        initial_params.append(np.log(self.sigma))\n",
    "        \n",
    "        # Optimize with bounds for stability\n",
    "        bounds = []\n",
    "        \n",
    "        # W - no bounds\n",
    "        bounds.extend([(None, None)] * (self.n_hidden * (self.n_input + 1)))\n",
    "        \n",
    "        # M - no bounds\n",
    "        bounds.extend([(None, None)] * ((self.n_hidden + 1) * self.n_features))\n",
    "        \n",
    "        # L - diagonal elements must be positive\n",
    "        for i in range(len(tril_indices[0])):\n",
    "            if tril_indices[0][i] == tril_indices[1][i]:  # diagonal\n",
    "                bounds.append((1e-8, None))\n",
    "            else:\n",
    "                bounds.append((None, None))\n",
    "                \n",
    "        # log_sigma must be > log(1e-8)\n",
    "        bounds.append((np.log(1e-8), None))\n",
    "        \n",
    "        # Optimize\n",
    "        result = minimize(\n",
    "            lambda p: -self.log_likelihood(p, all_stats, task_features_list),\n",
    "            initial_params,\n",
    "            method='L-BFGS-B',\n",
    "            bounds=bounds,\n",
    "            options={'maxiter': max_iter, 'disp': True}\n",
    "        )\n",
    "        \n",
    "        # Store optimized parameters\n",
    "        self._unpack_parameters(result.x)\n",
    "        \n",
    "        # Recompute MAP estimates\n",
    "        _ = self.log_likelihood(result.x, all_stats, task_features_list)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _unpack_parameters(self, params):\n",
    "        \"\"\"Helper to unpack optimized parameters\"\"\"\n",
    "        param_idx = 0\n",
    "        \n",
    "        # W\n",
    "        W_size = self.n_hidden * (self.n_input + 1)\n",
    "        self.W = params[param_idx:param_idx + W_size].reshape(self.n_hidden, self.n_input + 1)\n",
    "        param_idx += W_size\n",
    "        \n",
    "        # M: (n_hidden + 1 x n_features)\n",
    "        M_size = (self.n_hidden + 1) * self.n_features\n",
    "        M = params[param_idx:param_idx + M_size].reshape(self.n_hidden + 1, self.n_features)\n",
    "        param_idx += M_size\n",
    "        \n",
    "        # Sigma (Cholesky)\n",
    "        L = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n",
    "        tril_indices = np.tril_indices(self.n_hidden + 1)\n",
    "        L[tril_indices] = params[param_idx:param_idx + len(tril_indices[0])]\n",
    "        param_idx += len(tril_indices[0])\n",
    "        self.Sigma = np.dot(L, L.T)\n",
    "        \n",
    "        # sigma\n",
    "        self.sigma = np.exp(params[param_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:46:52.533711Z",
     "iopub.status.busy": "2025-05-06T13:46:52.533429Z",
     "iopub.status.idle": "2025-05-06T13:46:52.546456Z",
     "shell.execute_reply": "2025-05-06T13:46:52.545352Z",
     "shell.execute_reply.started": "2025-05-06T13:46:52.533691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate synthetic data with more meaningful relationships\n",
    "np.random.seed(42)\n",
    "n_input = 5\n",
    "n_hidden = 4\n",
    "n_tasks = 10\n",
    "n_features = 3\n",
    "n_samples = 200\n",
    "\n",
    "# True parameters with stronger relationships\n",
    "true_W = np.random.randn(n_hidden, n_input + 1) * 0.7\n",
    "true_M = np.random.randn(n_hidden + 1, n_features) * 0.7\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "task_features_list = []\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    # Generate meaningful task features\n",
    "    task_features = np.random.randn(n_features) * 2\n",
    "    task_features_list.append(task_features)\n",
    "    \n",
    "    X = np.random.randn(n_samples, n_input)\n",
    "    X_bias = np.hstack([X, np.ones((n_samples, 1))])\n",
    "    H = np.tanh(np.dot(X_bias, true_W.T))\n",
    "    H_bias = np.hstack([H, np.ones((n_samples, 1))])\n",
    "    \n",
    "    # Compute task mean from features\n",
    "    task_mean = np.dot(true_M, task_features)\n",
    "    \n",
    "    # Generate task weights\n",
    "    true_A = np.random.multivariate_normal(task_mean, np.eye(n_hidden+1)*0.1)\n",
    "    y = np.dot(H_bias, true_A) + np.random.randn(n_samples)*0.1\n",
    "    \n",
    "    X_list.append(X)\n",
    "    y_list.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:46:55.008795Z",
     "iopub.status.busy": "2025-05-06T13:46:55.008519Z",
     "iopub.status.idle": "2025-05-06T13:46:56.229642Z",
     "shell.execute_reply": "2025-05-06T13:46:56.228684Z",
     "shell.execute_reply.started": "2025-05-06T13:46:55.008775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization success: True\n",
      "Final sigma: 0.7124352023358816\n",
      "Sample predictions: [-0.47234402 -0.32747134  0.19923604  0.87527359 -0.43738474]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n"
     ]
    }
   ],
   "source": [
    "# Create and fit model\n",
    "model = MultiTaskNNDependentMean(n_input=n_input, n_hidden=n_hidden,\n",
    "                                     n_tasks=n_tasks, n_features=n_features,\n",
    "                                     activation='tanh')\n",
    "result = model.fit(X_list, y_list, task_features_list, max_iter=200)\n",
    "\n",
    "print(\"Optimization success:\", result.success)\n",
    "print(\"Final sigma:\", model.sigma)\n",
    "\n",
    "# Evaluate\n",
    "X_test = np.random.randn(10, n_input)\n",
    "y_pred = model.predict(X_test, task_idx=0, task_features=task_features_list[0])\n",
    "print(\"Sample predictions:\", y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering of Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:47:01.133730Z",
     "iopub.status.busy": "2025-05-06T13:47:01.133450Z",
     "iopub.status.idle": "2025-05-06T13:47:01.165495Z",
     "shell.execute_reply": "2025-05-06T13:47:01.164609Z",
     "shell.execute_reply.started": "2025-05-06T13:47:01.133711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiTaskNNClustering:\n",
    "    def __init__(self, n_input, n_hidden, n_tasks, n_clusters, activation='tanh'):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_tasks = n_tasks\n",
    "        self.n_clusters = n_clusters\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize with larger scale and better conditioning\n",
    "        self.W = np.random.randn(n_hidden, n_input + 1) * 0.5\n",
    "        self.q = np.ones(n_clusters) / n_clusters\n",
    "        self.m = np.random.randn(n_clusters, n_hidden + 1) * 0.5\n",
    "        \n",
    "        # Initialize Sigma with larger diagonal for numerical stability\n",
    "        self.Sigma = np.array([np.eye(n_hidden + 1) * 0.5 for _ in range(n_clusters)])\n",
    "        self.sigma = 1.0\n",
    "        \n",
    "        self.A = np.zeros((n_tasks, n_hidden + 1))\n",
    "        self.z = np.zeros((n_tasks, n_clusters))\n",
    "    \n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation == 'linear':\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function\")\n",
    "    \n",
    "    def compute_hidden(self, X):\n",
    "        X_bias = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "        h = self._activate(np.dot(X_bias, self.W.T))\n",
    "        return np.hstack([h, np.ones((h.shape[0], 1))])\n",
    "    \n",
    "    def predict(self, X, task_idx):\n",
    "        h = self.compute_hidden(X)\n",
    "        return np.dot(h, self.A[task_idx])\n",
    "    \n",
    "    def _compute_task_log_likelihood(self, X_i, y_i, cluster_idx):\n",
    "        n_i = len(y_i)\n",
    "        h_i = self.compute_hidden(X_i)\n",
    "        \n",
    "        # Add small constant to avoid division by zero\n",
    "        sigma_sq = max(self.sigma**2, 1e-8)\n",
    "        \n",
    "        try:\n",
    "            # Use Cholesky decomposition for numerical stability\n",
    "            L = cholesky(self.Sigma[cluster_idx], lower=True)\n",
    "            Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n",
    "            Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n",
    "            \n",
    "            Q_i = (1/sigma_sq) * np.dot(h_i.T, h_i) + Sigma_inv\n",
    "            L_Q = cholesky(Q_i, lower=True)\n",
    "            Q_inv = solve_triangular(L_Q, np.eye(self.n_hidden + 1), lower=True)\n",
    "            Q_inv = np.dot(Q_inv.T, Q_inv)\n",
    "            \n",
    "            R_i = (1/sigma_sq) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[cluster_idx])\n",
    "            \n",
    "            # Compute log determinants efficiently\n",
    "            logdet_Sigma = 2 * np.sum(np.log(np.diag(L)))\n",
    "            logdet_Q_i = 2 * np.sum(np.log(np.diag(L_Q)))\n",
    "            \n",
    "            term1 = -0.5 * (logdet_Sigma + n_i * np.log(sigma_sq) + logdet_Q_i)\n",
    "            term2 = 0.5 * (np.dot(R_i.T, np.dot(Q_inv, R_i)) - (1/(2*sigma_sq)) * np.sum(y_i**2) - np.dot(self.m[cluster_idx].T, np.dot(Sigma_inv, self.m[cluster_idx])))\n",
    "            \n",
    "            return term1 + term2\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            # Return -inf if matrix is not positive definite\n",
    "            return -np.inf\n",
    "    \n",
    "    def e_step(self, data):\n",
    "        log_responsibilities = np.zeros((self.n_tasks, self.n_clusters))\n",
    "        \n",
    "        for i, (X_i, y_i) in enumerate(data):\n",
    "            for alpha in range(self.n_clusters):\n",
    "                log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n",
    "                log_responsibilities[i, alpha] = np.log(self.q[alpha] + 1e-8) + log_lik\n",
    "            \n",
    "            # Normalize using logsumexp for numerical stability\n",
    "            log_responsibilities[i] -= logsumexp(log_responsibilities[i])\n",
    "        \n",
    "        self.z = np.exp(log_responsibilities)\n",
    "    \n",
    "    def m_step(self, data):\n",
    "        def objective(params):\n",
    "            W = params[:self.n_hidden * (self.n_input + 1)].reshape(self.n_hidden, self.n_input + 1)\n",
    "            log_sigma = params[-1]\n",
    "            sigma = np.exp(log_sigma)\n",
    "            \n",
    "            self.W = W\n",
    "            self.sigma = max(sigma, 1e-8)  # Prevent sigma from becoming too small\n",
    "            \n",
    "            total_log_lik = 0.0\n",
    "            for i, (X_i, y_i) in enumerate(data):\n",
    "                for alpha in range(self.n_clusters):\n",
    "                    log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n",
    "                    total_log_lik += self.z[i, alpha] * log_lik\n",
    "            \n",
    "            return -total_log_lik if np.isfinite(total_log_lik) else np.inf\n",
    "        \n",
    "        # Initial parameters with bounds\n",
    "        initial_params = np.concatenate([\n",
    "            self.W.flatten(),\n",
    "            [np.log(self.sigma)]\n",
    "        ])\n",
    "        \n",
    "        # Add bounds for sigma (log_sigma > log(1e-8))\n",
    "        bounds = [(None, None)] * len(initial_params)\n",
    "        bounds[-1] = (np.log(1e-8), None)\n",
    "        \n",
    "        result = minimize(\n",
    "            objective,\n",
    "            initial_params,\n",
    "            method='L-BFGS-B',\n",
    "            bounds=bounds,\n",
    "            options={'maxiter': 50, 'disp': True}\n",
    "        )\n",
    "        \n",
    "        opt_params = result.x\n",
    "        W_size = self.n_hidden * (self.n_input + 1)\n",
    "        self.W = opt_params[:W_size].reshape(self.n_hidden, self.n_input + 1)\n",
    "        self.sigma = max(np.exp(opt_params[-1]), 1e-8)\n",
    "        \n",
    "        # Update cluster parameters with regularization\n",
    "        for alpha in range(self.n_clusters):\n",
    "            self.q[alpha] = max(np.sum(self.z[:, alpha]) / self.n_tasks, 1e-8)\n",
    "            \n",
    "            sum_z = np.sum(self.z[:, alpha])\n",
    "            if sum_z > 1e-8:\n",
    "                weighted_R = np.zeros(self.n_hidden + 1)\n",
    "                weighted_Q = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n",
    "                \n",
    "                for i, (X_i, y_i) in enumerate(data):\n",
    "                    h_i = self.compute_hidden(X_i)\n",
    "                    L = cholesky(self.Sigma[alpha] + 1e-6*np.eye(self.n_hidden + 1), lower=True)\n",
    "                    Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n",
    "                    Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n",
    "                    \n",
    "                    Q_i = (1/max(self.sigma**2, 1e-8)) * np.dot(h_i.T, h_i) + Sigma_inv\n",
    "                    R_i = (1/max(self.sigma**2, 1e-8)) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[alpha])\n",
    "                    \n",
    "                    weighted_R += self.z[i, alpha] * R_i\n",
    "                    weighted_Q += self.z[i, alpha] * Q_i\n",
    "                \n",
    "                try:\n",
    "                    self.m[alpha] = np.linalg.solve(weighted_Q + 1e-6*np.eye(self.n_hidden + 1), weighted_R)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Update covariance with regularization\n",
    "                weighted_cov = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n",
    "                for i, (X_i, y_i) in enumerate(data):\n",
    "                    h_i = self.compute_hidden(X_i)\n",
    "                    A_i = self._compute_map_estimate(X_i, y_i, alpha)\n",
    "                    diff = A_i - self.m[alpha]\n",
    "                    weighted_cov += self.z[i, alpha] * np.outer(diff, diff)\n",
    "                \n",
    "                self.Sigma[alpha] = weighted_cov / sum_z + 1e-6 * np.eye(self.n_hidden + 1)\n",
    "    \n",
    "    def _compute_map_estimate(self, X_i, y_i, cluster_idx):\n",
    "        h_i = self.compute_hidden(X_i)\n",
    "        sigma_sq = max(self.sigma**2, 1e-8)\n",
    "        \n",
    "        L = cholesky(self.Sigma[cluster_idx] + 1e-6*np.eye(self.n_hidden + 1), lower=True)\n",
    "        Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n",
    "        Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n",
    "        \n",
    "        Q_i = (1/sigma_sq) * np.dot(h_i.T, h_i) + Sigma_inv\n",
    "        R_i = (1/sigma_sq) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[cluster_idx])\n",
    "        \n",
    "        return np.linalg.solve(Q_i + 1e-6*np.eye(self.n_hidden + 1), R_i)\n",
    "    \n",
    "    def fit(self, data, max_iter=100, tol=1e-4):\n",
    "        prev_log_lik = -np.inf\n",
    "        \n",
    "        for iteration in tqdm((range(max_iter))):\n",
    "            self.e_step(data)\n",
    "            self.m_step(data)\n",
    "            \n",
    "            # Compute current log likelihood\n",
    "            current_log_lik = 0.0\n",
    "            for i, (X_i, y_i) in enumerate(data):\n",
    "                cluster_log_liks = []\n",
    "                for alpha in range(self.n_clusters):\n",
    "                    log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n",
    "                    cluster_log_liks.append(np.log(self.q[alpha] + 1e-8) + log_lik)\n",
    "                current_log_lik += logsumexp(cluster_log_liks)\n",
    "            \n",
    "            if np.isnan(current_log_lik):\n",
    "                print(\"Warning: log likelihood is nan, stopping early\")\n",
    "                break\n",
    "                \n",
    "            if iteration > 0 and np.abs(current_log_lik - prev_log_lik) < tol:\n",
    "                print(f\"Converged at iteration {iteration}\")\n",
    "                break\n",
    "                \n",
    "            prev_log_lik = current_log_lik\n",
    "            #if iteration % 10 == 0:\n",
    "            #    print(f\"Iteration {iteration}, log likelihood: {current_log_lik}\")\n",
    "        \n",
    "        self._compute_final_weights(data)\n",
    "    \n",
    "    def _compute_final_weights(self, data):\n",
    "        for i, (X_i, y_i) in enumerate(data):\n",
    "            most_likely_cluster = np.argmax(self.z[i])\n",
    "            self.A[i] = self._compute_map_estimate(X_i, y_i, most_likely_cluster)\n",
    "    \n",
    "    def get_cluster_assignments(self):\n",
    "        return np.argmax(self.z, axis=1)\n",
    "    \n",
    "    def get_task_similarity(self):\n",
    "        assignments = self.get_cluster_assignments()\n",
    "        return np.array([[1.0 if a == b else 0.0 for b in assignments] for a in assignments])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:18:25.250994Z",
     "iopub.status.busy": "2025-05-06T12:18:25.250306Z",
     "iopub.status.idle": "2025-05-06T12:18:25.260118Z",
     "shell.execute_reply": "2025-05-06T12:18:25.259333Z",
     "shell.execute_reply.started": "2025-05-06T12:18:25.250968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_tasks=20, n_samples_train=10, n_samples_test=300, \n",
    "                           n_input=10, n_hidden=1, n_clusters=2, activation='tanh'):\n",
    "    # True parameters\n",
    "    true_W = np.random.randn(n_hidden, n_input + 1)\n",
    "    \n",
    "    # Two cluster means\n",
    "    true_m = np.array([\n",
    "        [1.5, 0.5],  # Cluster 0 (bias term added)\n",
    "        [-1.5, -0.5]  # Cluster 1 (bias term added)\n",
    "    ])\n",
    "    \n",
    "    # Generate tasks - assign half to each cluster\n",
    "    cluster_assignments = np.zeros(n_tasks)\n",
    "    cluster_assignments[n_tasks//2:] = 1\n",
    "    \n",
    "    # Generate data for each task\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for i in range(n_tasks):\n",
    "        # Generate covariates\n",
    "        X_train = np.random.randn(n_samples_train, n_input)\n",
    "        X_test = np.random.randn(n_samples_test, n_input)\n",
    "        \n",
    "        # Scale per task to zero mean and unit variance\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Compute hidden activations\n",
    "        X_train_bias = np.hstack([X_train, np.ones((n_samples_train, 1))])\n",
    "        X_test_bias = np.hstack([X_test, np.ones((n_samples_test, 1))])\n",
    "        \n",
    "        if activation == 'tanh':\n",
    "            h_train = np.tanh(np.dot(X_train_bias, true_W.T))\n",
    "            h_test = np.tanh(np.dot(X_test_bias, true_W.T))\n",
    "        else:\n",
    "            h_train = np.dot(X_train_bias, true_W.T)\n",
    "            h_test = np.dot(X_test_bias, true_W.T)\n",
    "        \n",
    "        # Add bias term\n",
    "        h_train = np.hstack([h_train, np.ones((n_samples_train, 1))])\n",
    "        h_test = np.hstack([h_test, np.ones((n_samples_test, 1))])\n",
    "        \n",
    "        # Get cluster for this task\n",
    "        cluster = int(cluster_assignments[i])\n",
    "        A_i = np.random.multivariate_normal(true_m[cluster], np.eye(2)*0.1)\n",
    "        \n",
    "        # Generate responses with noise\n",
    "        y_train = np.dot(h_train, A_i) + np.random.randn(n_samples_train)*0.1\n",
    "        y_test = np.dot(h_test, A_i) + np.random.randn(n_samples_test)*0.1\n",
    "        \n",
    "        train_data.append((X_train, y_train))\n",
    "        test_data.append((X_test, y_test))\n",
    "    \n",
    "    return train_data, test_data, cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:18:28.771168Z",
     "iopub.status.busy": "2025-05-06T12:18:28.770833Z",
     "iopub.status.idle": "2025-05-06T12:18:28.777472Z",
     "shell.execute_reply": "2025-05-06T12:18:28.776491Z",
     "shell.execute_reply.started": "2025-05-06T12:18:28.771127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data, true_clusters):\n",
    "    # 1. Cluster assignment accuracy\n",
    "    pred_clusters = model.get_cluster_assignments()\n",
    "    cluster_acc = np.mean(pred_clusters == true_clusters)\n",
    "    \n",
    "    # 2. Test MSE\n",
    "    mse = 0\n",
    "    for i, (X_test, y_test) in enumerate(test_data):\n",
    "        y_pred = model.predict(X_test, i)\n",
    "        mse += np.mean((y_test - y_pred)**2)\n",
    "    mse /= len(test_data)\n",
    "    \n",
    "    # 3. Task similarity matrix accuracy\n",
    "    true_similarity = np.array([[1.0 if a == b else 0.0 for b in true_clusters] \n",
    "                               for a in true_clusters])\n",
    "    pred_similarity = model.get_task_similarity()\n",
    "    similarity_acc = np.mean(true_similarity == pred_similarity)\n",
    "    \n",
    "    return {\n",
    "        'cluster_accuracy': cluster_acc,\n",
    "        'test_mse': mse,\n",
    "        'similarity_accuracy': similarity_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:18:30.986284Z",
     "iopub.status.busy": "2025-05-06T12:18:30.985457Z",
     "iopub.status.idle": "2025-05-06T12:18:30.993177Z",
     "shell.execute_reply": "2025-05-06T12:18:30.992080Z",
     "shell.execute_reply.started": "2025-05-06T12:18:30.986258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_simulations(n_simulations=10, activation='tanh'):\n",
    "    results = []\n",
    "    \n",
    "    for sim in range(n_simulations):\n",
    "        print(f\"\\nSimulation {sim+1}/{n_simulations}\")\n",
    "        \n",
    "        # Generate data\n",
    "        train_data, test_data, true_clusters = generate_synthetic_data(activation=activation)\n",
    "        \n",
    "        # Initialize and fit model\n",
    "        model = MultiTaskNNClustering(\n",
    "            n_input=10,\n",
    "            n_hidden=1,\n",
    "            n_tasks=20,\n",
    "            n_clusters=2,\n",
    "            activation=activation\n",
    "        )\n",
    "        \n",
    "        model.fit(train_data, max_iter=100)\n",
    "        print('yes')\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate_model(model, test_data, true_clusters)\n",
    "        print('yes')\n",
    "        results.append(metrics)\n",
    "        \n",
    "        print(f\"Cluster accuracy: {metrics['cluster_accuracy']:.3f}\")\n",
    "        print(f\"Test MSE: {metrics['test_mse']:.4f}\")\n",
    "        print(f\"Similarity accuracy: {metrics['similarity_accuracy']:.3f}\")\n",
    "    \n",
    "    # Aggregate results\n",
    "    avg_results = {\n",
    "        'avg_cluster_accuracy': np.mean([r['cluster_accuracy'] for r in results]),\n",
    "        'avg_test_mse': np.mean([r['test_mse'] for r in results]),\n",
    "        'avg_similarity_accuracy': np.mean([r['similarity_accuracy'] for r in results])\n",
    "    }\n",
    "    \n",
    "    return avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T12:18:33.659905Z",
     "iopub.status.busy": "2025-05-06T12:18:33.659581Z",
     "iopub.status.idle": "2025-05-06T12:27:03.614078Z",
     "shell.execute_reply": "2025-05-06T12:27:03.613234Z",
     "shell.execute_reply.started": "2025-05-06T12:18:33.659875Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with tanh activation:\n",
      "\n",
      "Simulation 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_31/593227174.py:88: RuntimeWarning: overflow encountered in exp\n",
      "  sigma = np.exp(log_sigma)\n",
      "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.450\n",
      "Test MSE: 2.2244\n",
      "Similarity accuracy: 0.505\n",
      "\n",
      "Simulation 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:31<00:59,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 35\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.950\n",
      "Test MSE: 0.0128\n",
      "Similarity accuracy: 0.905\n",
      "\n",
      "Simulation 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_31/593227174.py:88: RuntimeWarning: overflow encountered in exp\n",
      "  sigma = np.exp(log_sigma)\n",
      "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      " 23%|██▎       | 23/100 [00:41<02:18,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 23\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.000\n",
      "Test MSE: 0.0127\n",
      "Similarity accuracy: 1.000\n",
      "\n",
      "Simulation 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:34<03:53,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 13\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 1.000\n",
      "Test MSE: 0.0136\n",
      "Similarity accuracy: 1.000\n",
      "\n",
      "Simulation 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:34<00:22,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 61\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.550\n",
      "Test MSE: 0.0130\n",
      "Similarity accuracy: 0.505\n",
      "\n",
      "Simulation 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_31/593227174.py:88: RuntimeWarning: overflow encountered in exp\n",
      "  sigma = np.exp(log_sigma)\n",
      "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "100%|██████████| 100/100 [00:44<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.600\n",
      "Test MSE: 1.0080\n",
      "Similarity accuracy: 0.520\n",
      "\n",
      "Simulation 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:39<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "Cluster accuracy: 1.000\n",
      "Test MSE: 0.6215\n",
      "Similarity accuracy: 1.000\n",
      "\n",
      "Simulation 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:20<00:53,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 28\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.950\n",
      "Test MSE: 0.0141\n",
      "Similarity accuracy: 0.905\n",
      "\n",
      "Simulation 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:16<02:12,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 11\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 1.000\n",
      "Test MSE: 0.0150\n",
      "Similarity accuracy: 1.000\n",
      "\n",
      "Simulation 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_31/593227174.py:88: RuntimeWarning: overflow encountered in exp\n",
      "  sigma = np.exp(log_sigma)\n",
      "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      " 34%|███▍      | 34/100 [00:23<00:45,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 34\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.850\n",
      "Test MSE: 0.0143\n",
      "Similarity accuracy: 0.745\n",
      "\n",
      "Testing with linear activation:\n",
      "\n",
      "Simulation 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:13<01:52,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 11\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 1.000\n",
      "Test MSE: 0.0148\n",
      "Similarity accuracy: 1.000\n",
      "\n",
      "Simulation 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_31/593227174.py:88: RuntimeWarning: overflow encountered in exp\n",
      "  sigma = np.exp(log_sigma)\n",
      "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "  8%|▊         | 8/100 [00:07<01:30,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 8\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 1.000\n",
      "Test MSE: 0.0139\n",
      "Similarity accuracy: 1.000\n",
      "\n",
      "Simulation 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_31/593227174.py:88: RuntimeWarning: overflow encountered in exp\n",
      "  sigma = np.exp(log_sigma)\n",
      "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      " 11%|█         | 11/100 [00:07<01:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 11\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 1.000\n",
      "Test MSE: 0.0151\n",
      "Similarity accuracy: 1.000\n",
      "\n",
      "Simulation 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:29<00:36,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 45\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.150\n",
      "Test MSE: 0.0147\n",
      "Similarity accuracy: 0.745\n",
      "\n",
      "Simulation 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:17<00:56,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 24\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.700\n",
      "Test MSE: 0.0128\n",
      "Similarity accuracy: 0.580\n",
      "\n",
      "Simulation 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:09<01:31,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 9\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.000\n",
      "Test MSE: 0.0128\n",
      "Similarity accuracy: 1.000\n",
      "\n",
      "Simulation 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_31/593227174.py:88: RuntimeWarning: overflow encountered in exp\n",
      "  sigma = np.exp(log_sigma)\n",
      "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n",
      "  8%|▊         | 8/100 [00:06<01:18,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 8\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.000\n",
      "Test MSE: 0.0140\n",
      "Similarity accuracy: 1.000\n",
      "\n",
      "Simulation 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:11<01:14,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 13\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.000\n",
      "Test MSE: 0.0145\n",
      "Similarity accuracy: 1.000\n",
      "\n",
      "Simulation 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [00:30<00:48,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 38\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 0.100\n",
      "Test MSE: 0.0129\n",
      "Similarity accuracy: 0.820\n",
      "\n",
      "Simulation 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:18<01:13,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 20\n",
      "yes\n",
      "yes\n",
      "Cluster accuracy: 1.000\n",
      "Test MSE: 0.0123\n",
      "Similarity accuracy: 1.000\n",
      "\n",
      "Final Results:\n",
      "Tanh activation: {'avg_cluster_accuracy': 0.735, 'avg_test_mse': 0.39494072882681475, 'avg_similarity_accuracy': 0.8084999999999999}\n",
      "Linear activation: {'avg_cluster_accuracy': 0.49499999999999994, 'avg_test_mse': 0.013777777418109297, 'avg_similarity_accuracy': 0.9145}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing with tanh activation:\")\n",
    "tanh_results = run_simulations(activation='tanh')\n",
    "\n",
    "print(\"\\nTesting with linear activation:\")\n",
    "linear_results = run_simulations(activation='linear')\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"Tanh activation:\", tanh_results)\n",
    "print(\"Linear activation:\", linear_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gating of Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:47:22.726218Z",
     "iopub.status.busy": "2025-05-06T13:47:22.725836Z",
     "iopub.status.idle": "2025-05-06T13:47:22.765457Z",
     "shell.execute_reply": "2025-05-06T13:47:22.764479Z",
     "shell.execute_reply.started": "2025-05-06T13:47:22.726191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiTaskNNGating:\n",
    "    def __init__(self, n_input, n_hidden, n_tasks, n_clusters, n_features, activation='tanh'):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_tasks = n_tasks\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_features = n_features\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize with larger scale for better convergence\n",
    "        self.W = np.random.randn(n_hidden, n_input + 1) * 0.5\n",
    "        self.U = np.random.randn(n_clusters, n_features) * 0.5\n",
    "        self.m = np.random.randn(n_clusters, n_hidden + 1) * 0.5\n",
    "        \n",
    "        # Initialize covariance matrices with larger diagonal\n",
    "        self.Sigma = np.array([np.eye(n_hidden + 1) * 0.5 for _ in range(n_clusters)])\n",
    "        self.sigma = 1.0\n",
    "        \n",
    "        self.A = np.zeros((n_tasks, n_hidden + 1))\n",
    "        self.z = np.zeros((n_tasks, n_clusters))\n",
    "    \n",
    "    def _activate(self, x):\n",
    "        if self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation == 'linear'\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation function\")\n",
    "    \n",
    "    def compute_hidden(self, X):\n",
    "        X_bias = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "        h = self._activate(np.dot(X_bias, self.W.T))\n",
    "        return np.hstack([h, np.ones((h.shape[0], 1))])\n",
    "    \n",
    "    def compute_gating_probabilities(self, F):\n",
    "        \"\"\"Compute task-cluster assignment probabilities with numerical stability\"\"\"\n",
    "        # Ensure F is 2D array\n",
    "        F = np.atleast_2d(F)\n",
    "        if F.shape[0] == 1 and self.n_tasks > 1:\n",
    "            F = np.repeat(F, self.n_tasks, axis=0)\n",
    "            \n",
    "        logits = np.dot(F, self.U.T)\n",
    "        return softmax(logits, axis=1)\n",
    "    \n",
    "    def predict(self, X, task_idx, task_features=None):\n",
    "        h = self.compute_hidden(X)\n",
    "        if task_features is not None:\n",
    "            # If task features provided, use gating to determine cluster\n",
    "            probs = self.compute_gating_probabilities(task_features)\n",
    "            cluster = np.argmax(probs)\n",
    "            return np.dot(h, self._compute_map_estimate(X, np.zeros(len(X)), cluster))\n",
    "        return np.dot(h, self.A[task_idx])\n",
    "    \n",
    "    def _compute_task_log_likelihood(self, X_i, y_i, cluster_idx):\n",
    "        n_i = len(y_i)\n",
    "        h_i = self.compute_hidden(X_i)\n",
    "        \n",
    "        # Add small constant to avoid division by zero\n",
    "        sigma_sq = max(self.sigma**2, 1e-8)\n",
    "        \n",
    "        try:\n",
    "            # Use Cholesky decomposition for numerical stability\n",
    "            L = cholesky(self.Sigma[cluster_idx] + 1e-6*np.eye(self.n_hidden+1), lower=True)\n",
    "            Sigma_inv = solve_triangular(L, np.eye(self.n_hidden+1), lower=True)\n",
    "            Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n",
    "            \n",
    "            Q_i = (1/sigma_sq) * np.dot(h_i.T, h_i) + Sigma_inv\n",
    "            L_Q = cholesky(Q_i, lower=True)\n",
    "            Q_inv = solve_triangular(L_Q, np.eye(self.n_hidden+1), lower=True)\n",
    "            Q_inv = np.dot(Q_inv.T, Q_inv)\n",
    "            \n",
    "            R_i = (1/sigma_sq) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[cluster_idx])\n",
    "            \n",
    "            # Compute log determinants\n",
    "            logdet_Sigma = 2 * np.sum(np.log(np.diag(L)))\n",
    "            logdet_Q_i = 2 * np.sum(np.log(np.diag(L_Q)))\n",
    "            \n",
    "            term1 = -0.5 * (logdet_Sigma + n_i * np.log(sigma_sq) + logdet_Q_i)\n",
    "            term2 = 0.5 * (np.dot(R_i.T, np.dot(Q_inv, R_i)) - (1/(2*sigma_sq)) * np.sum(y_i**2) - np.dot(self.m[cluster_idx].T, np.dot(Sigma_inv, self.m[cluster_idx])))\n",
    "            \n",
    "            return term1 + term2\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            return -np.inf\n",
    "    \n",
    "    def e_step(self, data, task_features):\n",
    "        \"\"\"Expectation step with improved numerical stability\"\"\"\n",
    "        # Ensure task_features is 2D array\n",
    "        task_features = np.atleast_2d(task_features)\n",
    "        if task_features.shape[0] == 1 and self.n_tasks > 1:\n",
    "            task_features = np.repeat(task_features, self.n_tasks, axis=0)\n",
    "        \n",
    "        q = self.compute_gating_probabilities(task_features)\n",
    "        log_responsibilities = np.zeros((self.n_tasks, self.n_clusters))\n",
    "        \n",
    "        for i, (X_i, y_i) in enumerate(data):\n",
    "            for alpha in range(self.n_clusters):\n",
    "                log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n",
    "                log_responsibilities[i, alpha] = np.log(q[i, alpha] + 1e-8) + log_lik\n",
    "            \n",
    "            # Normalize using logsumexp\n",
    "            log_responsibilities[i] -= logsumexp(log_responsibilities[i])\n",
    "        \n",
    "        self.z = np.exp(log_responsibilities)\n",
    "    \n",
    "    def m_step(self, data, task_features):\n",
    "        \"\"\"Maximization step with regularization\"\"\"\n",
    "        # Optimize W and sigma\n",
    "        def objective(params):\n",
    "            W = params[:self.n_hidden*(self.n_input+1)].reshape(self.n_hidden, self.n_input+1)\n",
    "            log_sigma = params[-1]\n",
    "            sigma = np.exp(log_sigma)\n",
    "            \n",
    "            self.W = W\n",
    "            self.sigma = max(sigma, 1e-8)\n",
    "            \n",
    "            total_log_lik = 0.0\n",
    "            for i, (X_i, y_i) in enumerate(data):\n",
    "                for alpha in range(self.n_clusters):\n",
    "                    log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n",
    "                    total_log_lik += self.z[i, alpha] * log_lik\n",
    "            \n",
    "            return -total_log_lik if np.isfinite(total_log_lik) else np.inf\n",
    "        \n",
    "        # Initial parameters with bounds\n",
    "        initial_params = np.concatenate([\n",
    "            self.W.flatten(),\n",
    "            [np.log(self.sigma)]\n",
    "        ])\n",
    "        \n",
    "        bounds = [(None, None)] * len(initial_params)\n",
    "        bounds[-1] = (np.log(1e-8), None)  # sigma > 1e-8\n",
    "        \n",
    "        result = minimize(\n",
    "            objective,\n",
    "            initial_params,\n",
    "            method='L-BFGS-B',\n",
    "            bounds=bounds,\n",
    "            options={'maxiter': 50, 'disp': True}\n",
    "        )\n",
    "        \n",
    "        # Update parameters\n",
    "        opt_params = result.x\n",
    "        W_size = self.n_hidden * (self.n_input + 1)\n",
    "        self.W = opt_params[:W_size].reshape(self.n_hidden, self.n_input + 1)\n",
    "        self.sigma = max(np.exp(opt_params[-1]), 1e-8)\n",
    "        \n",
    "        # Update cluster parameters with regularization\n",
    "        for alpha in range(self.n_clusters):\n",
    "            sum_z = np.sum(self.z[:, alpha])\n",
    "            if sum_z > 1e-8:\n",
    "                # Update m_α\n",
    "                weighted_R = np.zeros(self.n_hidden + 1)\n",
    "                weighted_Q = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n",
    "                \n",
    "                for i, (X_i, y_i) in enumerate(data):\n",
    "                    h_i = self.compute_hidden(X_i)\n",
    "                    L = cholesky(self.Sigma[alpha] + 1e-6*np.eye(self.n_hidden+1), lower=True)\n",
    "                    Sigma_inv = solve_triangular(L, np.eye(self.n_hidden+1), lower=True)\n",
    "                    Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n",
    "                    \n",
    "                    Q_i = (1/max(self.sigma**2, 1e-8)) * np.dot(h_i.T, h_i) + Sigma_inv\n",
    "                    R_i = (1/max(self.sigma**2, 1e-8)) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[alpha])\n",
    "                    \n",
    "                    weighted_R += self.z[i, alpha] * R_i\n",
    "                    weighted_Q += self.z[i, alpha] * Q_i\n",
    "                \n",
    "                try:\n",
    "                    self.m[alpha] = np.linalg.solve(weighted_Q + 1e-6*np.eye(self.n_hidden+1), weighted_R)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Update Σ_α with regularization\n",
    "                weighted_cov = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n",
    "                for i, (X_i, y_i) in enumerate(data):\n",
    "                    h_i = self.compute_hidden(X_i)\n",
    "                    A_i = self._compute_map_estimate(X_i, y_i, alpha)\n",
    "                    diff = A_i - self.m[alpha]\n",
    "                    weighted_cov += self.z[i, alpha] * np.outer(diff, diff)\n",
    "                \n",
    "                self.Sigma[alpha] = weighted_cov / sum_z + 1e-6 * np.eye(self.n_hidden + 1)\n",
    "        \n",
    "        # Update gating parameters U\n",
    "        if self.n_clusters > 1:\n",
    "            task_features = np.atleast_2d(task_features)\n",
    "            if task_features.shape[0] == 1 and self.n_tasks > 1:\n",
    "                task_features = np.repeat(task_features, self.n_tasks, axis=0)\n",
    "                \n",
    "            lr = LogisticRegression(\n",
    "                multi_class='multinomial',\n",
    "                solver='lbfgs',\n",
    "                fit_intercept=False,\n",
    "                max_iter=100,\n",
    "                penalty='l2',\n",
    "                C=1.0\n",
    "            )\n",
    "            try:\n",
    "                lr.fit(task_features, self.get_cluster_assignments(), sample_weight=np.max(self.z, axis=1))\n",
    "                self.U = lr.coef_\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def _compute_map_estimate(self, X_i, y_i, cluster_idx):\n",
    "        h_i = self.compute_hidden(X_i)\n",
    "        sigma_sq = max(self.sigma**2, 1e-8)\n",
    "        \n",
    "        L = cholesky(self.Sigma[cluster_idx] + 1e-6*np.eye(self.n_hidden + 1), lower=True)\n",
    "        Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n",
    "        Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n",
    "        \n",
    "        Q_i = (1/sigma_sq) * np.dot(h_i.T, h_i) + Sigma_inv\n",
    "        R_i = (1/sigma_sq) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[cluster_idx])\n",
    "        \n",
    "        return np.linalg.solve(Q_i + 1e-6*np.eye(self.n_hidden + 1), R_i)\n",
    "    \n",
    "    def fit(self, data, task_features, max_iter=100, tol=1e-4):\n",
    "        \"\"\"Improved fitting with better initialization and checks\"\"\"\n",
    "        prev_log_lik = -np.inf\n",
    "        \n",
    "        # Normalize task features\n",
    "        task_features = np.atleast_2d(task_features)\n",
    "        if task_features.shape[0] == 1 and self.n_tasks > 1:\n",
    "            task_features = np.repeat(task_features, self.n_tasks, axis=0)\n",
    "        \n",
    "        self.task_feature_mean = np.mean(task_features, axis=0)\n",
    "        self.task_feature_std = np.std(task_features, axis=0) + 1e-8\n",
    "        task_features = (task_features - self.task_feature_mean) / self.task_feature_std\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            try:\n",
    "                self.e_step(data, task_features)\n",
    "                self.m_step(data, task_features)\n",
    "                \n",
    "                # Compute current log likelihood\n",
    "                current_log_lik = 0.0\n",
    "                q = self.compute_gating_probabilities(task_features)\n",
    "                \n",
    "                for i, (X_i, y_i) in enumerate(data):\n",
    "                    cluster_log_liks = []\n",
    "                    for alpha in range(self.n_clusters):\n",
    "                        log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n",
    "                        cluster_log_liks.append(np.log(q[i, alpha] + 1e-8) + log_lik)\n",
    "                    current_log_lik += logsumexp(cluster_log_liks)\n",
    "                \n",
    "                if np.isnan(current_log_lik):\n",
    "                    print(\"Warning: log likelihood is nan, stopping early\")\n",
    "                    break\n",
    "                    \n",
    "                if iteration > 0 and abs(current_log_lik - prev_log_lik) < tol:\n",
    "                    print(f\"Converged at iteration {iteration}\")\n",
    "                    break\n",
    "                    \n",
    "                prev_log_lik = current_log_lik\n",
    "                print(f\"Iteration {iteration}, log likelihood: {current_log_lik}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error at iteration {iteration}: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        self._compute_final_weights(data)\n",
    "        return self\n",
    "    \n",
    "    def _compute_final_weights(self, data):\n",
    "        for i, (X_i, y_i) in enumerate(data):\n",
    "            most_likely_cluster = np.argmax(self.z[i])\n",
    "            self.A[i] = self._compute_map_estimate(X_i, y_i, most_likely_cluster)\n",
    "    \n",
    "    def get_cluster_assignments(self):\n",
    "        return np.argmax(self.z, axis=1)\n",
    "    \n",
    "    def get_task_similarity(self):\n",
    "        assignments = self.get_cluster_assignments()\n",
    "        return np.array([[1.0 if a == b else 0.0 for b in assignments] for a in assignments])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T13:51:43.011291Z",
     "iopub.status.busy": "2025-05-06T13:51:43.010891Z",
     "iopub.status.idle": "2025-05-06T13:51:44.824654Z",
     "shell.execute_reply": "2025-05-06T13:51:44.823851Z",
     "shell.execute_reply.started": "2025-05-06T13:51:43.011269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2993834058.py:112: RuntimeWarning: overflow encountered in exp\n",
      "  sigma = np.exp(log_sigma)\n",
      "/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at iteration 0: index 1 is out of bounds for axis 1 with size 1\n",
      "Cluster assignments: [0 0 0 0 0 1 1 1 1 1]\n",
      "True clusters: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "Task 0 (cluster 0) predictions: [2.14593525 2.08323812 0.43084465]\n",
      "Task 5 (cluster 1) predictions: [-3.72567776  1.19625966  3.69211654]\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data with clear cluster structure\n",
    "n_input = 5\n",
    "n_hidden = 4  # Increased hidden units\n",
    "n_tasks = 10\n",
    "n_clusters = 2\n",
    "n_features = 3\n",
    "n_samples = 200\n",
    "\n",
    "# Create task features with clear separation\n",
    "task_features = np.zeros((n_tasks, n_features))\n",
    "for i in range(n_tasks):\n",
    "    if i < n_tasks // 2:\n",
    "        task_features[i] = [1.0, -0.5, 0.7] + np.random.randn(n_features)*0.1\n",
    "    else:\n",
    "        task_features[i] = [-0.8, 1.2, -0.3] + np.random.randn(n_features)*0.1\n",
    "\n",
    "data = []\n",
    "for i in range(n_tasks):\n",
    "    X = np.random.randn(n_samples, n_input)\n",
    "    if i < n_tasks // 2:  # First cluster\n",
    "        y = 2.5 * X[:, 0] - 1.5 * X[:, 1] + np.random.randn(n_samples) * 0.1\n",
    "    else:  # Second cluster\n",
    "        y = -2.0 * X[:, 0] + 3.0 * X[:, 2] + np.random.randn(n_samples) * 0.1\n",
    "    data.append((X, y))\n",
    "\n",
    "# Initialize and fit model\n",
    "model = MultiTaskNNGating(\n",
    "    n_input=n_input,\n",
    "    n_hidden=n_hidden,\n",
    "    n_tasks=n_tasks,\n",
    "    n_clusters=n_clusters,\n",
    "    n_features=n_features,\n",
    "    activation='tanh'\n",
    ")\n",
    "model.fit(data, task_features, max_iter=100)\n",
    "\n",
    "# Check results\n",
    "assignments = model.get_cluster_assignments()\n",
    "print(\"Cluster assignments:\", assignments)\n",
    "print(\"True clusters:\", [0 if i < n_tasks//2 else 1 for i in range(n_tasks)])\n",
    "\n",
    "# Make predictions\n",
    "X_test = np.random.randn(5, n_input)\n",
    "for task_idx in [0, n_tasks//2]:\n",
    "    preds = model.predict(X_test, task_idx)\n",
    "    print(f\"Task {task_idx} (cluster {assignments[task_idx]}) predictions:\", preds[:3])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
