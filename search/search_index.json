{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"bmm-multitask-learning documentation","text":"<p>Welcome to our <code>bmm-multitask-learning</code> python package main page!</p>"},{"location":"coalescent/coalescent_example/","title":"Example","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nfrom tabulate import tabulate\nfrom dataclasses import dataclass\nfrom scipy import stats\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# inverse wishart class\nfrom bmm_multitask_learning.coalescent.inverse_wishart import InverseWishart\n\n# \u043e\u0431\u0435\u0440\u0442\u043a\u0430 \u043d\u0430\u0434 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043e\u0434\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438\nfrom bmm_multitask_learning.coalescent.task_classes import TaskData\n\n# \u0441\u043e\u043b\u0432\u0435\u0440 \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447\u0438\nfrom bmm_multitask_learning.coalescent.coalescent_learner import MultitaskProblem\n\n# \u043e\u0431\u0435\u0440\u0442\u043a\u0430 \u0434\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\nfrom bmm_multitask_learning.coalescent.utils import MyModel\n\n\ndef print_table(r2, mse, K):\n    print(tabulate([[\"R2\", *r2], [\"MSE\", *mse]], headers=[\"Metric\", *[f\"Value_{i}\" for i in range(K)]], tablefmt=\"grid\"))\n\ndef test_models(X, Y, models, K):\n    r2 = []\n    mse = []\n    for i in range(Y.shape[1]):\n        task_prediction = models[i].predict(X)\n        true_labels = Y[:, i]\n        r2.append(r2_score(true_labels, task_prediction))\n        mse.append(mean_squared_error(true_labels, task_prediction))\n    return np.array(r2), np.array(mse)\n</pre> import matplotlib.pyplot as plt import numpy as np from tabulate import tabulate from dataclasses import dataclass from scipy import stats  from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score, mean_squared_error  # inverse wishart class from bmm_multitask_learning.coalescent.inverse_wishart import InverseWishart  # \u043e\u0431\u0435\u0440\u0442\u043a\u0430 \u043d\u0430\u0434 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043e\u0434\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 from bmm_multitask_learning.coalescent.task_classes import TaskData  # \u0441\u043e\u043b\u0432\u0435\u0440 \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447\u0438 from bmm_multitask_learning.coalescent.coalescent_learner import MultitaskProblem  # \u043e\u0431\u0435\u0440\u0442\u043a\u0430 \u0434\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 from bmm_multitask_learning.coalescent.utils import MyModel   def print_table(r2, mse, K):     print(tabulate([[\"R2\", *r2], [\"MSE\", *mse]], headers=[\"Metric\", *[f\"Value_{i}\" for i in range(K)]], tablefmt=\"grid\"))  def test_models(X, Y, models, K):     r2 = []     mse = []     for i in range(Y.shape[1]):         task_prediction = models[i].predict(X)         true_labels = Y[:, i]         r2.append(r2_score(true_labels, task_prediction))         mse.append(mean_squared_error(true_labels, task_prediction))     return np.array(r2), np.array(mse) In\u00a0[3]: Copied! <pre>@dataclass\nclass Node:\n    mean: float\n    cov: float\n    t: float\n    parent: int\n    children: list[int]\n\ndef generate_coalescent(K):\n    nodes = set(range(K))\n    tree_nodes = {i: {\"parent\":None, \"children\": [], \"delta\": -float(\"inf\")} for i in range(K)}\n    new_node = -1\n    while len(nodes) &gt; 1:\n        l, r = np.random.choice(list(nodes), 2, replace=False,)\n        nodes.remove(l)\n        nodes.remove(r)\n        \n        new_node = max(tree_nodes.keys()) + 1\n        \n        nodes.add(new_node)\n        tree_nodes[new_node] = {\"parent\": None, \"children\": [l,r], \"delta\": -float(\"inf\")}\n        \n        tree_nodes[l][\"parent\"] = new_node\n        tree_nodes[r][\"parent\"] = new_node\n        tree_nodes[l][\"delta\"] = np.random.exponential(1)\n        tree_nodes[r][\"delta\"] = np.random.exponential(1)\n    \n    tree_nodes['root'] = tree_nodes[new_node]\n    return tree_nodes\n\ndef get_weights(tree, d, sigma, R):\n    weights = []\n    s_gt = []\n    def generate_w(node, tree, d, sigma,):\n        if node[\"parent\"] is None:\n            node[\"w\"] = np.random.randn(d)\n        else:\n            node[\"w\"] = np.random.normal(tree[node[\"parent\"]][\"w\"], sigma * (node[\"delta\"]))\n            \n        for child in node[\"children\"]:\n            generate_w(tree[child], tree, d, sigma)\n        if len(node[\"children\"]) == 0:\n            s = node[\"w\"]\n            s_exp = np.diag(np.exp(s))\n            Sigma = s_exp @ R @ s_exp\n            w = np.random.multivariate_normal(np.zeros(d), Sigma)\n            s_gt.append(s)\n            weights.append(w)\n\n    generate_w(tree['root'], tree, d, sigma)\n    return np.stack(weights), np.stack(s_gt)\n</pre> @dataclass class Node:     mean: float     cov: float     t: float     parent: int     children: list[int]  def generate_coalescent(K):     nodes = set(range(K))     tree_nodes = {i: {\"parent\":None, \"children\": [], \"delta\": -float(\"inf\")} for i in range(K)}     new_node = -1     while len(nodes) &gt; 1:         l, r = np.random.choice(list(nodes), 2, replace=False,)         nodes.remove(l)         nodes.remove(r)                  new_node = max(tree_nodes.keys()) + 1                  nodes.add(new_node)         tree_nodes[new_node] = {\"parent\": None, \"children\": [l,r], \"delta\": -float(\"inf\")}                  tree_nodes[l][\"parent\"] = new_node         tree_nodes[r][\"parent\"] = new_node         tree_nodes[l][\"delta\"] = np.random.exponential(1)         tree_nodes[r][\"delta\"] = np.random.exponential(1)          tree_nodes['root'] = tree_nodes[new_node]     return tree_nodes  def get_weights(tree, d, sigma, R):     weights = []     s_gt = []     def generate_w(node, tree, d, sigma,):         if node[\"parent\"] is None:             node[\"w\"] = np.random.randn(d)         else:             node[\"w\"] = np.random.normal(tree[node[\"parent\"]][\"w\"], sigma * (node[\"delta\"]))                      for child in node[\"children\"]:             generate_w(tree[child], tree, d, sigma)         if len(node[\"children\"]) == 0:             s = node[\"w\"]             s_exp = np.diag(np.exp(s))             Sigma = s_exp @ R @ s_exp             w = np.random.multivariate_normal(np.zeros(d), Sigma)             s_gt.append(s)             weights.append(w)      generate_w(tree['root'], tree, d, sigma)     return np.stack(weights), np.stack(s_gt) In\u00a0[4]: Copied! <pre># define parameters\n\nK = 20  # number of tasks\nn = 40  # number of data points per task\ntask_dimention = 10  #dimention of task\nsigma = 0.1  # divergence rate of tasks\n\n# \u0437\u0430\u0434\u0430\u0435\u043c \u043e\u0431\u0449\u0443\u044e \u043a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447\nR_gt = stats.invwishart.rvs(df=task_dimention, scale=np.eye(task_dimention), size=1)\nR_gt = InverseWishart.cov2corr(R_gt)\n\n# R_gt = np.eye(d)  # \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u0440\u0438\u043c\u0435\u0440\n\n# \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u043a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0432\u0435\u0441\u043e\u0432 \u0438 \u0441\u0430\u043c\u0438 \u0432\u0435\u0441\u0430\ntree = generate_coalescent(K)\nweights_gt, s_gt = get_weights(tree, task_dimention, sigma, R_gt)\n\n# \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435. \u0421\u043a\u0435\u0439\u043b\u0435\u0440 \u0434\u043b\u044f \u0448\u0443\u043c\u0430 \u0432 \u0446\u0435\u043b\u044f\u0445 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u0438 \u043f\u043e\u0434\u0431\u0438\u0440\u0430\u0435\u043c \u0430\u0434\u0430\u043f\u0442\u0438\u0432\u043d\u043e\n# \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e, \u0447\u0442\u043e\u0431\u044b ground truth \u0432\u0435\u0441\u0430 \u0445\u043e\u0440\u043e\u0448\u043e \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u043b\u0438\u0441\u044c, \u0430 \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u044b\u0435 -- \u043f\u043b\u043e\u0445\u043e\n\nnoise_scale = weights_gt.max(axis = 1)[None, :] * 1.5\n\n# \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\nX = np.random.randn(n, task_dimention)\nY = np.dot(X, weights_gt.T) + np.random.randn(n, K) * noise_scale\n\n# \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0430\nX_test = np.random.randn(n, task_dimention)\nY_test = np.dot(X_test, weights_gt.T) + np.random.randn(n, K) * noise_scale\n\nprint(Y.shape)\n</pre> # define parameters  K = 20  # number of tasks n = 40  # number of data points per task task_dimention = 10  #dimention of task sigma = 0.1  # divergence rate of tasks  # \u0437\u0430\u0434\u0430\u0435\u043c \u043e\u0431\u0449\u0443\u044e \u043a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0443\u044e \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 R_gt = stats.invwishart.rvs(df=task_dimention, scale=np.eye(task_dimention), size=1) R_gt = InverseWishart.cov2corr(R_gt)  # R_gt = np.eye(d)  # \u0434\u0440\u0443\u0433\u043e\u0439 \u043f\u0440\u0438\u043c\u0435\u0440  # \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u043a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0432\u0435\u0441\u043e\u0432 \u0438 \u0441\u0430\u043c\u0438 \u0432\u0435\u0441\u0430 tree = generate_coalescent(K) weights_gt, s_gt = get_weights(tree, task_dimention, sigma, R_gt)  # \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435. \u0421\u043a\u0435\u0439\u043b\u0435\u0440 \u0434\u043b\u044f \u0448\u0443\u043c\u0430 \u0432 \u0446\u0435\u043b\u044f\u0445 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u0438 \u043f\u043e\u0434\u0431\u0438\u0440\u0430\u0435\u043c \u0430\u0434\u0430\u043f\u0442\u0438\u0432\u043d\u043e # \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e, \u0447\u0442\u043e\u0431\u044b ground truth \u0432\u0435\u0441\u0430 \u0445\u043e\u0440\u043e\u0448\u043e \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u043b\u0438\u0441\u044c, \u0430 \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u044b\u0435 -- \u043f\u043b\u043e\u0445\u043e  noise_scale = weights_gt.max(axis = 1)[None, :] * 1.5  # \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f X = np.random.randn(n, task_dimention) Y = np.dot(X, weights_gt.T) + np.random.randn(n, K) * noise_scale  # \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0430 X_test = np.random.randn(n, task_dimention) Y_test = np.dot(X_test, weights_gt.T) + np.random.randn(n, K) * noise_scale  print(Y.shape) <pre>(40, 20)\n</pre> In\u00a0[5]: Copied! <pre>print(\"Ground truth\")\nmodels_gt = [MyModel(weights_gt[i]) for i in range(K)]\nprint_table(*test_models(X_test, Y_test, models_gt,K), K)\n\nprint(\"LinearRegression\")\nmodels = []\nfor i in range(Y.shape[1]):\n    model = LinearRegression()\n    model.fit(X, Y[:, i])\n    models.append(model)\nprint_table(*test_models(X_test, Y_test, models, K), K )\n</pre> print(\"Ground truth\") models_gt = [MyModel(weights_gt[i]) for i in range(K)] print_table(*test_models(X_test, Y_test, models_gt,K), K)  print(\"LinearRegression\") models = [] for i in range(Y.shape[1]):     model = LinearRegression()     model.fit(X, Y[:, i])     models.append(model) print_table(*test_models(X_test, Y_test, models, K), K ) <pre>Ground truth\n+----------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n| Metric   |    Value_0 |   Value_1 |   Value_2 |   Value_3 |   Value_4 |   Value_5 |   Value_6 |   Value_7 |   Value_8 |   Value_9 |   Value_10 |   Value_11 |   Value_12 |   Value_13 |   Value_14 |   Value_15 |   Value_16 |   Value_17 |   Value_18 |   Value_19 |\n+==========+============+===========+===========+===========+===========+===========+===========+===========+===========+===========+============+============+============+============+============+============+============+============+============+============+\n| R2       |   0.260993 |  0.782468 |  0.356231 |  0.678716 |  0.436923 |  0.486206 |  0.487526 |  0.595509 |  0.690502 |  0.552038 |   0.541004 |   0.402476 |   0.398552 |   0.518333 |   0.550749 |   0.429566 |   0.542901 |    0.18931 |   0.388669 |   0.548935 |\n+----------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n| MSE      | 203.931    |  1.46526  | 15.0134   |  3.10777  | 13.4244   | 62.8011   | 38.0846   |  5.37617  |  5.70298  | 21.5291   |   0.222032 | 147.803    |  23.3517   |  14.031    |   2.85527  |   1.96203  |  11.0228   |   57.7279  |  14.3895   |  18.0196   |\n+----------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\nLinearRegression\n+----------+-------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n| Metric   |     Value_0 |   Value_1 |   Value_2 |   Value_3 |   Value_4 |   Value_5 |   Value_6 |   Value_7 |   Value_8 |   Value_9 |   Value_10 |   Value_11 |   Value_12 |   Value_13 |   Value_14 |   Value_15 |   Value_16 |   Value_17 |   Value_18 |   Value_19 |\n+==========+=============+===========+===========+===========+===========+===========+===========+===========+===========+===========+============+============+============+============+============+============+============+============+============+============+\n| R2       |  -0.0785935 |  0.544011 |  0.268632 |   0.64056 | -0.243195 |  0.343616 |  0.396453 |  0.475481 |  0.675289 |  0.389981 |  0.0210683 |   0.340819 |   0.223101 |   0.503516 |   0.386842 | -0.0861858 |   0.397585 |  -0.159301 |   0.129871 |   0.374665 |\n+----------+-------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n| MSE      | 297.64      |  3.07146  | 17.0564   |   3.47685 | 29.6392   | 80.2298   | 44.8527   |  6.97148  |  5.98331  | 29.3176   |  0.473542  | 163.054    |  30.1637   |  14.4626   |   3.897    |  3.73599   |  14.527    |  82.5519   |  20.4811   |  24.9816   |\n+----------+-------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n</pre> In\u00a0[6]: Copied! <pre>def select_for_task(X, y, task_pos, selection_count):\n    selected = np.random.choice(X.shape[0], selection_count, replace=False)\n    X_task = X[selected]\n    y_task = y[selected][:, task_pos][:, None]\n    return X_task, y_task\n\ntasks = []\nselection_count = n\n\nfor i in range(K):\n    X_task, y_task = select_for_task(X, Y, i, selection_count)\n    X_task = X_task\n    task = TaskData(X_task, y_task)\n    tasks.append(task)\n</pre> def select_for_task(X, y, task_pos, selection_count):     selected = np.random.choice(X.shape[0], selection_count, replace=False)     X_task = X[selected]     y_task = y[selected][:, task_pos][:, None]     return X_task, y_task  tasks = [] selection_count = n  for i in range(K):     X_task, y_task = select_for_task(X, Y, i, selection_count)     X_task = X_task     task = TaskData(X_task, y_task)     tasks.append(task)  In\u00a0[7]: Copied! <pre>rho = 0.05\ncov_sigma=0.1\noptimization_steps = 200\n\nproblem = MultitaskProblem(tasks, \n                           dim=task_dimention, \n                           rho=rho,\n                           cov_sigma=cov_sigma,\n    )\n\nproblem.fit(n_steps=optimization_steps)\nweights_coalescent = problem.weights\n</pre> rho = 0.05 cov_sigma=0.1 optimization_steps = 200  problem = MultitaskProblem(tasks,                             dim=task_dimention,                             rho=rho,                            cov_sigma=cov_sigma,     )  problem.fit(n_steps=optimization_steps) weights_coalescent = problem.weights In\u00a0[8]: Copied! <pre># now we compute r2 and mean squared error\n\nprint(\"Ground truth\")\nmodels_gt = [MyModel(weights_gt[i]) for i in range(K)]\ngt_res = test_models(X_test, Y_test, models_gt,K)\n\nprint(\"Coalescent\")\nmodels_mp = [MyModel(weights_coalescent[i]) for i in range(K)]\nmp_res = test_models(X_test, Y_test, models_mp, K)\n\nprint(\"LinearRegression\")\nmodels = []\nfor i in range(Y.shape[1]):\n    model = LinearRegression()\n    model.fit(X, Y[:, i])\n    models.append(model)\nweight_lr = [m.coef_ for m in models]\nlr_res = test_models(X_test, Y_test, models, K)\n</pre> # now we compute r2 and mean squared error  print(\"Ground truth\") models_gt = [MyModel(weights_gt[i]) for i in range(K)] gt_res = test_models(X_test, Y_test, models_gt,K)  print(\"Coalescent\") models_mp = [MyModel(weights_coalescent[i]) for i in range(K)] mp_res = test_models(X_test, Y_test, models_mp, K)  print(\"LinearRegression\") models = [] for i in range(Y.shape[1]):     model = LinearRegression()     model.fit(X, Y[:, i])     models.append(model) weight_lr = [m.coef_ for m in models] lr_res = test_models(X_test, Y_test, models, K) <pre>Ground truth\nCoalescent\nLinearRegression\n</pre> In\u00a0[9]: Copied! <pre>ind = np.argsort(lr_res[0])\n\nprint(\"Ground truth\")\nprint_table(gt_res[0][ind], gt_res[1][ind], K)\n\nprint(\"Coalescent\")\nprint_table(mp_res[0][ind], mp_res[1][ind], K)\nprint(\"LinearRegression\")\nprint_table(lr_res[0][ind], lr_res[1][ind], K)\n</pre> ind = np.argsort(lr_res[0])  print(\"Ground truth\") print_table(gt_res[0][ind], gt_res[1][ind], K)  print(\"Coalescent\") print_table(mp_res[0][ind], mp_res[1][ind], K) print(\"LinearRegression\") print_table(lr_res[0][ind], lr_res[1][ind], K) <pre>Ground truth\n+----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n| Metric   |   Value_0 |   Value_1 |   Value_2 |    Value_3 |   Value_4 |   Value_5 |   Value_6 |   Value_7 |    Value_8 |   Value_9 |   Value_10 |   Value_11 |   Value_12 |   Value_13 |   Value_14 |   Value_15 |   Value_16 |   Value_17 |   Value_18 |   Value_19 |\n+==========+===========+===========+===========+============+===========+===========+===========+===========+============+===========+============+============+============+============+============+============+============+============+============+============+\n| R2       |  0.436923 |   0.18931 |  0.429566 |   0.260993 |  0.541004 |  0.388669 |  0.398552 |  0.356231 |   0.402476 |  0.486206 |   0.548935 |   0.550749 |   0.552038 |   0.487526 |   0.542901 |   0.595509 |   0.518333 |   0.782468 |   0.678716 |   0.690502 |\n+----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n| MSE      | 13.4244   |  57.7279  |  1.96203  | 203.931    |  0.222032 | 14.3895   | 23.3517   | 15.0134   | 147.803    | 62.8011   |  18.0196   |   2.85527  |  21.5291   |  38.0846   |  11.0228   |   5.37617  |  14.031    |   1.46526  |   3.10777  |   5.70298  |\n+----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\nCoalescent\n+----------+-----------+-----------+-----------+-------------+-----------+-----------+-----------+-----------+------------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n| Metric   |   Value_0 |   Value_1 |   Value_2 |     Value_3 |   Value_4 |   Value_5 |   Value_6 |   Value_7 |    Value_8 |   Value_9 |   Value_10 |   Value_11 |   Value_12 |   Value_13 |   Value_14 |   Value_15 |   Value_16 |   Value_17 |   Value_18 |   Value_19 |\n+==========+===========+===========+===========+=============+===========+===========+===========+===========+============+===========+============+============+============+============+============+============+============+============+============+============+\n| R2       | -0.192893 | -0.106887 |  0.182515 |   0.0912063 |  0.218148 |  0.163108 |   0.22693 |  0.270468 |   0.332619 |   0.31118 |    0.33564 |   0.386137 |   0.392318 |   0.397822 |   0.391347 |   0.478905 |   0.492835 |   0.546429 |    0.62345 |   0.678753 |\n+----------+-----------+-----------+-----------+-------------+-----------+-----------+-----------+-----------+------------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n| MSE      | 28.4399   | 78.8196   |  2.81178  | 250.784     |  0.378208 | 19.6988   |  30.0151  | 17.0135   | 165.083    |  84.1945  |   26.5406  |   3.90148  |  29.2053   |  44.751    |  14.6775   |   6.92598  |  14.7738   |   3.05518  |    3.64236 |   5.91947  |\n+----------+-----------+-----------+-----------+-------------+-----------+-----------+-----------+-----------+------------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\nLinearRegression\n+----------+-----------+-----------+------------+-------------+-----------+-----------+-----------+-----------+------------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n| Metric   |   Value_0 |   Value_1 |    Value_2 |     Value_3 |   Value_4 |   Value_5 |   Value_6 |   Value_7 |    Value_8 |   Value_9 |   Value_10 |   Value_11 |   Value_12 |   Value_13 |   Value_14 |   Value_15 |   Value_16 |   Value_17 |   Value_18 |   Value_19 |\n+==========+===========+===========+============+=============+===========+===========+===========+===========+============+===========+============+============+============+============+============+============+============+============+============+============+\n| R2       | -0.243195 | -0.159301 | -0.0861858 |  -0.0785935 | 0.0210683 |  0.129871 |  0.223101 |  0.268632 |   0.340819 |  0.343616 |   0.374665 |   0.386842 |   0.389981 |   0.396453 |   0.397585 |   0.475481 |   0.503516 |   0.544011 |    0.64056 |   0.675289 |\n+----------+-----------+-----------+------------+-------------+-----------+-----------+-----------+-----------+------------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n| MSE      | 29.6392   | 82.5519   |  3.73599   | 297.64      | 0.473542  | 20.4811   | 30.1637   | 17.0564   | 163.054    | 80.2298   |  24.9816   |   3.897    |  29.3176   |  44.8527   |  14.527    |   6.97148  |  14.4626   |   3.07146  |    3.47685 |   5.98331  |\n+----------+-----------+-----------+------------+-------------+-----------+-----------+-----------+-----------+------------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n</pre> <p>\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u043c \u044d\u0442\u0438 \u0442\u0430\u0431\u043b\u0438\u0446\u044b \u043d\u0430 \u0433\u0440\u0430\u0444\u0438\u043a\u0430\u0445. \u041f\u043e \u043e\u0441\u0438 x \u0442\u0430\u043a\u0436\u0435 \u0437\u0430\u0434\u0430\u0447\u0438 \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u043f\u043e R2 \u043f\u0440\u043e\u0441\u0442\u044b\u0445 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439</p> In\u00a0[10]: Copied! <pre>fig, axs = plt.subplots(2,1, figsize = (5, 10))\n\nind = np.argsort(lr_res[0])\nfor ax, res_index, loss_name in zip(axs, [0,1], [\"R2, \u2191\u2191\", \"MSE, \u2193\u2193\"]):\n    denum = mp_res[res_index][ind]\n    for num, alg_name, color in zip(\n            [lr_res[res_index], gt_res[res_index], mp_res[res_index]], \n            [\"LinearRegression\", \"GroundTruth\", \"Coalescent\"],\n            [\"pink\", \"green\", \"blue\"]\n            ):\n        num = num[ind]\n        ax.plot(num, label = alg_name, color = color)\n\n    ax.set_xlabel(\"Task number\")\n    ax.set_ylabel(f\"{loss_name[:-4]}\")\n    ax.set_title(f\"comparison wrt {loss_name} better\")\n    ax.legend()\n    ax.grid()\n</pre> fig, axs = plt.subplots(2,1, figsize = (5, 10))  ind = np.argsort(lr_res[0]) for ax, res_index, loss_name in zip(axs, [0,1], [\"R2, \u2191\u2191\", \"MSE, \u2193\u2193\"]):     denum = mp_res[res_index][ind]     for num, alg_name, color in zip(             [lr_res[res_index], gt_res[res_index], mp_res[res_index]],              [\"LinearRegression\", \"GroundTruth\", \"Coalescent\"],             [\"pink\", \"green\", \"blue\"]             ):         num = num[ind]         ax.plot(num, label = alg_name, color = color)      ax.set_xlabel(\"Task number\")     ax.set_ylabel(f\"{loss_name[:-4]}\")     ax.set_title(f\"comparison wrt {loss_name} better\")     ax.legend()     ax.grid() <p>\u0438 \u043d\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0432\u0438\u043c \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u043d\u0430\u0440\u0438\u0441\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u0432\u044b\u0448\u0435 \u0434\u043b\u044f MSE \u043b\u043e\u0441\u0441\u0430</p> <p>\u0447\u0435\u043c \u043c\u0435\u043d\u044c\u0448\u0435 \u0442\u0435\u043c \u043b\u0443\u0447\u0448\u0435</p> <p>\u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e \u043d\u0430 \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0434\u043b\u044f linear regression \u0437\u0430\u0434\u0430\u0447 Coalescent \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043b\u0443\u0447\u0448\u0435.</p> In\u00a0[11]: Copied! <pre>fig, ax = plt.subplots(1,1, figsize = (5, 5))\nres_index, loss_name = 1,  \"MSE, \u2193\u2193\"\ndenum = lr_res[res_index][ind]\nfor num, alg_name in zip([mp_res[res_index], gt_res[res_index]], [\"Coalescent\", \"GroundTruth\"]):\n    num = num[ind]\n    ax.plot((num)/denum, label = alg_name)\n\nax.hlines(1, 0, K, colors=[\"black\"], label=\"Linear Regression\")\nax.set_xlabel(\"Task number\")\nax.set_ylabel(f\"{loss_name[:-4]}, relative\")\nax.set_title(f\"comparison wrt relative {loss_name} better\")\nax.legend()\nax.grid()\n</pre>   fig, ax = plt.subplots(1,1, figsize = (5, 5)) res_index, loss_name = 1,  \"MSE, \u2193\u2193\" denum = lr_res[res_index][ind] for num, alg_name in zip([mp_res[res_index], gt_res[res_index]], [\"Coalescent\", \"GroundTruth\"]):     num = num[ind]     ax.plot((num)/denum, label = alg_name)  ax.hlines(1, 0, K, colors=[\"black\"], label=\"Linear Regression\") ax.set_xlabel(\"Task number\") ax.set_ylabel(f\"{loss_name[:-4]}, relative\") ax.set_title(f\"comparison wrt relative {loss_name} better\") ax.legend() ax.grid()  <p>uhf</p> In\u00a0[12]: Copied! <pre>assert task_dimention == 2, \"this demonstration works only for d = 2\"\n\ns = np.array([0.1, -0.2])\ns_exp = np.diag(np.exp(s))\ncov = s_exp @ R_gt @ s_exp\n\nn_iters = 100\nn_samples = 1000\nR_distr = InverseWishart(2, 2+1, corr_mat=True)\n\nfor i in range(n_iters):\n    cov = s_exp @ R_gt @ s_exp\n    samples = np.random.multivariate_normal([0, 0], cov, n_samples)\n    s_leaves = np.repeat(s[None,:], n_samples, axis=0)\n\n    r_samples = optimal_R(s_leaves, samples)\n\n    R_distr.update_posterior(r_samples)\n\nprint(R_gt)\nprint(R_distr.get_most_prob())\n# w = \n</pre> assert task_dimention == 2, \"this demonstration works only for d = 2\"  s = np.array([0.1, -0.2]) s_exp = np.diag(np.exp(s)) cov = s_exp @ R_gt @ s_exp  n_iters = 100 n_samples = 1000 R_distr = InverseWishart(2, 2+1, corr_mat=True)  for i in range(n_iters):     cov = s_exp @ R_gt @ s_exp     samples = np.random.multivariate_normal([0, 0], cov, n_samples)     s_leaves = np.repeat(s[None,:], n_samples, axis=0)      r_samples = optimal_R(s_leaves, samples)      R_distr.update_posterior(r_samples)  print(R_gt) print(R_distr.get_most_prob()) # w =   <pre>\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 assert task_dimention == 2, \"this demonstration works only for d = 2\"\n      3 s = np.array([0.1, -0.2])\n      4 s_exp = np.diag(np.exp(s))\n\nAssertionError: this demonstration works only for d = 2</pre>"},{"location":"coalescent/coalescent_example/#example","title":"Example\u00b6","text":""},{"location":"coalescent/coalescent_example/#preparation-to-experiment","title":"Preparation to experiment\u00b6","text":""},{"location":"coalescent/coalescent_example/#imports-and-help-functions-for-demonstration","title":"Imports and help functions for demonstration\u00b6","text":""},{"location":"coalescent/coalescent_example/#data-generator-via-coalescent-process","title":"data generator via Coalescent process\u00b6","text":""},{"location":"coalescent/coalescent_example/#experiment-setting","title":"Experiment setting\u00b6","text":""},{"location":"coalescent/coalescent_example/","title":"\u0417\u0430\u0434\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0437\u0430\u0434\u0430\u0447\u0438 \u0438 \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\u00b6","text":""},{"location":"coalescent/coalescent_example/","title":"\u0421\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0431\u043e\u0442\u0443 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0432\u0435\u0441\u043e\u0432 \u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438\u00b6","text":"<p>\u0432\u0438\u0434\u043d\u043e, \u0447\u0442\u043e \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u043f\u043b\u043e\u0445\u043e \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f</p>"},{"location":"coalescent/coalescent_example/","title":"\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\u00b6","text":"<p>\u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 TaskData \u0434\u043b\u044f \u0440\u0430\u0437\u043d\u044b\u0445 \u0442\u0430\u0441\u043e\u043a.</p> <p>\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 <code>selection_count</code> \u0437\u0430\u0434\u0430\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u0430\u043d\u043d\u044b\u0445, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043f\u0440\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438</p>"},{"location":"coalescent/coalescent_example/","title":"\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\u00b6","text":"<p>\u0437\u0430\u0434\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435</p> <p><code>rho</code> -- \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 prior \u0432\u0430\u0436\u043d\u0435\u0435 \u0447\u0435\u043c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0435. \u0414\u0430\u043d\u043d\u044b\u0435 \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0441 \u0432\u0435\u0441\u043e\u043c 1/rho.</p> <p><code>cov_sigma</code> -- \u0441\u043a\u0435\u0439\u043b \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u043a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u0438</p>"},{"location":"coalescent/coalescent_example/","title":"\u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\u00b6","text":"<p>\u0441\u0440\u0430\u0432\u043d\u0438\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0442\u0440\u0435\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439:</p> <ul> <li><code>GroundTruth</code> -- ground truth \u0432\u0435\u0441\u0430</li> <li><code>Coalescent</code> -- Coalescent learinig</li> <li><code>Linear regression</code> -- \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e</li> </ul>"},{"location":"coalescent/coalescent_example/","title":"\u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\u00b6","text":"<p>\u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u043e \u0437\u0430\u0434\u0430\u0447\u0430\u043c \u0441\u043e\u0433\u043b\u0430\u0441\u043d\u043e R2 \u043f\u0440\u043e\u0441\u0442\u044b\u0445 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439.</p> <p>\u0422\u043e\u0433\u0434\u0430 \u0443\u0432\u0438\u0434\u0438\u043c, \u0447\u0442\u043e <code>Coalescent</code> \u0441 \u043d\u0438\u043c\u0438 \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043b\u0443\u0447\u0448\u0435.</p> <p>\u041d\u043e \u043f\u0440\u0438 \u044d\u0442\u043e\u043c \u0441 \u043f\u0440\u043e\u0441\u0442\u044b\u043c\u0438 \u0437\u0430\u0434\u0430\u0447\u0430\u043c\u0438 -- \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0445\u043e\u0440\u043e\u0448\u043e \u043d\u0430\u0441\u0442\u0440\u0430\u0438\u0432\u0430\u0435\u0442\u0441\u044f -- <code>Coalescent</code> \u0447\u0430\u0441\u0442\u043e \u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0445\u0443\u0436\u0435</p>"},{"location":"coalescent/coalescent_example/#validate-that-r-updates-truly","title":"validate that R updates truly\u00b6","text":""},{"location":"coalescent/hooks/","title":"Hooks","text":"In\u00a0[\u00a0]: Copied! <pre>import shutil\n</pre> import shutil In\u00a0[\u00a0]: Copied! <pre>def on_pre_build(*args, **kwargs):\n    shutil.copy2(\n        \"examples/coalescent/coalescent_example.ipynb\", \n        \"docs/coalescent/coalescent_example.ipynb\"\n    )\n</pre> def on_pre_build(*args, **kwargs):     shutil.copy2(         \"examples/coalescent/coalescent_example.ipynb\",          \"docs/coalescent/coalescent_example.ipynb\"     )"},{"location":"coalescent/reference/","title":"Reference for coalescent approach in multitask learning","text":""},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent","title":"<code>bmm_multitask_learning.coalescent</code>","text":""},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_inference","title":"<code>coalescent_inference</code>","text":""},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_inference.CoalescentTree","title":"<code>CoalescentTree</code>","text":"Source code in <code>bmm_multitask_learning/coalescent/coalescent_inference.py</code> <pre><code>class CoalescentTree:\n    def __init__(self, leaves: list[Item], cov, dim):\n        self.leaves = leaves\n        self.items, _ = self._greedy_rate_brownian(leaves,  cov, dim)\n\n    def iterate_over(self) -&gt; Iterator[Item]:\n        \"\"\"\n        iterates over all elements in tree\n        \"\"\"\n        for level in self.items:\n            for node in level:\n                yield node\n\n    @staticmethod\n    def select_candidates(x, cov, t_m1, n, dim):\n        \"\"\"\n        Realise greedy selection of candidates to coalesce.\n        For this consider all pairs of items and select minimal delta for coalesce\n\n        More details in [2]\n        \"\"\"\n        n_cur = len(x)\n        i_items = n - n_cur\n\n        l, r = None, None\n        min_time = float(\"inf\")\n\n        for i in range(n_cur):\n            for j in range(i + 1, n_cur):\n                t_new = Item.optimal_t(x[i], x[j], cov, i_items, t_m1, n, dim)\n                if t_new &lt; min_time:\n                    min_time, l, r = t_new, i, j\n        return l, r, min_time\n\n    def _greedy_rate_brownian(self, x: list[Item], cov, dim)\\\n            -&gt; tuple[list[list[Item]], list[tuple[int, int]]]:\n        \"\"\"\n        Realises greedy coalescent three construction\n        More details in [2]\n\n        :return: first argument is a constructed coalesce structure, \n            the second argument is a pointer to child elements\n        \"\"\"\n\n        n = len(x)\n        y = [x]\n        coalesced_items = []\n        t = 0\n        for _ in range(n-1):\n            coalecse_candidates = copy(y[-1])\n            l, r, delta = self.select_candidates(coalecse_candidates,\n                                                cov,\n                                                t,\n                                                n,\n                                                dim)\n            coalesced_items.append((l, r))\n            t = t - delta\n            new_item = Item.coalesce(coalecse_candidates[l],\n                                    coalecse_candidates[r],\n                                    t)\n\n            coalecse_candidates[l].parent = new_item\n            coalecse_candidates[r].parent = new_item\n\n            coalecse_candidates[l] = new_item\n            del coalecse_candidates[r]\n            y.append(coalecse_candidates)\n        return y, coalesced_items\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_inference.CoalescentTree.iterate_over","title":"<code>iterate_over()</code>","text":"<p>iterates over all elements in tree</p> Source code in <code>bmm_multitask_learning/coalescent/coalescent_inference.py</code> <pre><code>def iterate_over(self) -&gt; Iterator[Item]:\n    \"\"\"\n    iterates over all elements in tree\n    \"\"\"\n    for level in self.items:\n        for node in level:\n            yield node\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_inference.CoalescentTree.select_candidates","title":"<code>select_candidates(x, cov, t_m1, n, dim)</code>  <code>staticmethod</code>","text":"<p>Realise greedy selection of candidates to coalesce. For this consider all pairs of items and select minimal delta for coalesce</p> <p>More details in [2]</p> Source code in <code>bmm_multitask_learning/coalescent/coalescent_inference.py</code> <pre><code>@staticmethod\ndef select_candidates(x, cov, t_m1, n, dim):\n    \"\"\"\n    Realise greedy selection of candidates to coalesce.\n    For this consider all pairs of items and select minimal delta for coalesce\n\n    More details in [2]\n    \"\"\"\n    n_cur = len(x)\n    i_items = n - n_cur\n\n    l, r = None, None\n    min_time = float(\"inf\")\n\n    for i in range(n_cur):\n        for j in range(i + 1, n_cur):\n            t_new = Item.optimal_t(x[i], x[j], cov, i_items, t_m1, n, dim)\n            if t_new &lt; min_time:\n                min_time, l, r = t_new, i, j\n    return l, r, min_time\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_inference.Item","title":"<code>Item</code>  <code>dataclass</code>","text":"<p>class to handle the items of coalescent_tree</p> Source code in <code>bmm_multitask_learning/coalescent/coalescent_inference.py</code> <pre><code>@dataclass\nclass Item:\n    \"\"\"\n    class to handle the items of coalescent_tree\n    \"\"\"\n    mean: list\n    cov: float = 0.\n    t: float = 0.\n    parent = None\n\n    @staticmethod\n    def coalesce(first, other, t,):\n        \"\"\"\n        This method coalesces two Item objects. Works for brownian diffusion.\n        For more see [2]\n\n        [2] Y.W. Teh, H. Daum\u0301e III, and D. Roy. \n            Bayesian agglomerative clustering with coalescents. NIPS, 2007.\n        \"\"\"\n        # messages coalesce\n        # assert (self.cov + self.t - t) &gt; 0\n        # assert (other.cov + other.t - t) &gt; 0\n\n        l_inv = 1 / (first.cov + (first.t - t))\n        r_inv = 1 / (other.cov + (other.t - t))\n        new_cov = 1/(l_inv + r_inv)\n        new_mean = (first.mean * l_inv + other.mean * r_inv) * new_cov\n\n        new_t = t\n        return Item(new_mean, new_cov, new_t)\n\n    @staticmethod\n    def optimal_t(first, other, cov, i, t_m1, n, dim):\n        \"\"\"\n        computes optimal delta to coalesce for proposed pair of items\n        \"\"\"\n        c_ni = ((n - i + 1) * (n-i)/2)\n\n        a = 1/4 * 1/c_ni * \\\n            ((4 * c_ni * m_norm(first.mean - other.mean, cov) +\n                dim*dim) ** 0.5 - dim)\n\n        b = 1/2 * (first.cov + other.cov + first.t + other.t - 2 * t_m1)\n\n        assert a - b &gt; 0, f\"t_m1: {t_m1}, a: {a}, b: {b}\"\n        return a - b\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_inference.Item.coalesce","title":"<code>coalesce(first, other, t)</code>  <code>staticmethod</code>","text":"<p>This method coalesces two Item objects. Works for brownian diffusion. For more see [2]</p> <p>[2] Y.W. Teh, H. Daum\u0301e III, and D. Roy.      Bayesian agglomerative clustering with coalescents. NIPS, 2007.</p> Source code in <code>bmm_multitask_learning/coalescent/coalescent_inference.py</code> <pre><code>@staticmethod\ndef coalesce(first, other, t,):\n    \"\"\"\n    This method coalesces two Item objects. Works for brownian diffusion.\n    For more see [2]\n\n    [2] Y.W. Teh, H. Daum\u0301e III, and D. Roy. \n        Bayesian agglomerative clustering with coalescents. NIPS, 2007.\n    \"\"\"\n    # messages coalesce\n    # assert (self.cov + self.t - t) &gt; 0\n    # assert (other.cov + other.t - t) &gt; 0\n\n    l_inv = 1 / (first.cov + (first.t - t))\n    r_inv = 1 / (other.cov + (other.t - t))\n    new_cov = 1/(l_inv + r_inv)\n    new_mean = (first.mean * l_inv + other.mean * r_inv) * new_cov\n\n    new_t = t\n    return Item(new_mean, new_cov, new_t)\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_inference.Item.optimal_t","title":"<code>optimal_t(first, other, cov, i, t_m1, n, dim)</code>  <code>staticmethod</code>","text":"<p>computes optimal delta to coalesce for proposed pair of items</p> Source code in <code>bmm_multitask_learning/coalescent/coalescent_inference.py</code> <pre><code>@staticmethod\ndef optimal_t(first, other, cov, i, t_m1, n, dim):\n    \"\"\"\n    computes optimal delta to coalesce for proposed pair of items\n    \"\"\"\n    c_ni = ((n - i + 1) * (n-i)/2)\n\n    a = 1/4 * 1/c_ni * \\\n        ((4 * c_ni * m_norm(first.mean - other.mean, cov) +\n            dim*dim) ** 0.5 - dim)\n\n    b = 1/2 * (first.cov + other.cov + first.t + other.t - 2 * t_m1)\n\n    assert a - b &gt; 0, f\"t_m1: {t_m1}, a: {a}, b: {b}\"\n    return a - b\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_learner","title":"<code>coalescent_learner</code>","text":""},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_learner.MultitaskProblem","title":"<code>MultitaskProblem</code>","text":"<p>bayessian optimizer for Multitask Learning based on Coalescent [1]</p> <p>To use initialize with list of TaskData and call run. Then trained weights  will be available by method get_weights()</p> <p>[1] @article{daume2009bayesian,         title={Bayesian multitask learning with latent hierarchies},         author={Daum{'e} III, Hal},         journal={arXiv preprint arXiv:0907.0783},         year={2009}     }</p> Source code in <code>bmm_multitask_learning/coalescent/coalescent_learner.py</code> <pre><code>class MultitaskProblem:\n    \"\"\"\n    bayessian optimizer for Multitask Learning based on Coalescent [1]\n\n    To use initialize with list of TaskData and call run. Then trained weights \n    will be available by method get_weights()\n\n\n\n\n    [1] @article{daume2009bayesian,\n            title={Bayesian multitask learning with latent hierarchies},\n            author={Daum{\\'e} III, Hal},\n            journal={arXiv preprint arXiv:0907.0783},\n            year={2009}\n        }\n\n    \"\"\"\n    def __init__(self,\n            tasks: list[TaskData],\n            dim,\n            rho=0.05,\n            cov_sigma=0.1,\n            s_init=None\n        ):\n        \"\"\"\n\n        :param tasks: list of TaskData, which will be learned\n        :param dim: dimention of problems\n        :param rho: parameter that scales noise level in labels\n        :param cov_sigma: covariance matrix scaler for Coalescent evolution variation\n        :param s_init: initial values for S_i: variance scales of data\n        \"\"\"\n\n        self.tasks = tasks\n        self.K = len(tasks)\n        self.dim = dim\n        self.rho = 0.05\n        self.R_distr = InverseWishart(dim, dim+1, corr_mat=True)\n\n        self.cov_sigma = cov_sigma\n        self.L_distr = InverseWishart(dim, dim+1,  cov_sigma * np.eye(dim), corr_mat=False)\n\n        if s_init is None:\n            s_init = np.zeros((self.K, dim), dtype=float)\n            for i in range(self.K):\n                S = np.random.randn(dim)/5\n                s_init[i] = S\n        else:\n            assert s_init.shape == (self.K, dim), f\"provided s_init have\\\n                incorrect shape: {s_init.shape=} instead of {(self.K, dim)}\"\n\n        self.S_leaves: S_handler = S_handler(s_init)\n        self.weights = np.zeros((self.K, dim), dtype=float)\n\n        self._trained = False\n\n    def get_weights(self):\n        if not self._trained:\n            raise Warning(\"first call fit() method of trainer.\")\n        return self.weights\n\n    def fit(self, n_steps=100):\n        assert n_steps &gt; 0 and isinstance(n_steps, int), \"number of steps should be positive integer\"\n\n        # first setup the weights as in simple linear regression\n        self._update_weights(\n            tasks=self.tasks,\n            weights_mp=self.weights,\n            S_leaves=None,\n            R=None,\n            K=self.K,\n            rho=self.rho,\n            cov=np.eye(self.dim)\n        )\n\n        # method iteration\n        for _ in range(n_steps):\n            # get the most probable parameters\n            R = self.R_distr.get_most_prob()\n            L = self.L_distr.get_most_prob()\n\n\n        # inference on coalescent tree.\n        # Integrate out the evoulution of covariance\n            leaves = [Item(elem, cov=0) for elem in self.S_leaves.s_list]\n            coalescent_tree = CoalescentTree(leaves, L, self.dim)\n\n        # gradient optimization of S_leaves\n        # based on generated tree and parameters\n            self.S_leaves.update_param(R, L, self.weights, coalescent_tree)\n\n        # update the posteriors based on new tree and weights\n            # update covariance matrix posterior\n            L_samples = optimal_cov(coalescent_tree, self.dim)\n            self.L_distr.update_posterior(L_samples)\n\n\n            # update correlation matrix_posterior\n            R_samples = optimal_R(self.S_leaves.s_list, self.weights,)\n            self.R_distr.update_posterior(R_samples)\n\n            # update weights based on new posterior\n            self._update_weights(\n                tasks=self.tasks,\n                weights_mp=self.weights,\n                S_leaves=self.S_leaves.s_list,\n                R=R,\n                K=self.K,\n                rho=self.rho,\n                cov=None\n            )\n\n        self._trained = True\n\n    def _update_weights(self, tasks, weights_mp, S_leaves, R, K, rho, cov=None):\n        for i in range(K):\n            if cov is None:\n                S = np.diag(S_leaves[i])\n                cov_i = np.exp(S) @ R @ np.exp(S)\n            else:\n                cov_i = cov\n\n            w = tasks[i].most_prob_w(cov_i, rho)\n            weights_mp[i] = w\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_learner.MultitaskProblem.__init__","title":"<code>__init__(tasks, dim, rho=0.05, cov_sigma=0.1, s_init=None)</code>","text":"<p>:param tasks: list of TaskData, which will be learned :param dim: dimention of problems :param rho: parameter that scales noise level in labels :param cov_sigma: covariance matrix scaler for Coalescent evolution variation :param s_init: initial values for S_i: variance scales of data</p> Source code in <code>bmm_multitask_learning/coalescent/coalescent_learner.py</code> <pre><code>def __init__(self,\n        tasks: list[TaskData],\n        dim,\n        rho=0.05,\n        cov_sigma=0.1,\n        s_init=None\n    ):\n    \"\"\"\n\n    :param tasks: list of TaskData, which will be learned\n    :param dim: dimention of problems\n    :param rho: parameter that scales noise level in labels\n    :param cov_sigma: covariance matrix scaler for Coalescent evolution variation\n    :param s_init: initial values for S_i: variance scales of data\n    \"\"\"\n\n    self.tasks = tasks\n    self.K = len(tasks)\n    self.dim = dim\n    self.rho = 0.05\n    self.R_distr = InverseWishart(dim, dim+1, corr_mat=True)\n\n    self.cov_sigma = cov_sigma\n    self.L_distr = InverseWishart(dim, dim+1,  cov_sigma * np.eye(dim), corr_mat=False)\n\n    if s_init is None:\n        s_init = np.zeros((self.K, dim), dtype=float)\n        for i in range(self.K):\n            S = np.random.randn(dim)/5\n            s_init[i] = S\n    else:\n        assert s_init.shape == (self.K, dim), f\"provided s_init have\\\n            incorrect shape: {s_init.shape=} instead of {(self.K, dim)}\"\n\n    self.S_leaves: S_handler = S_handler(s_init)\n    self.weights = np.zeros((self.K, dim), dtype=float)\n\n    self._trained = False\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_learner.S_handler","title":"<code>S_handler</code>","text":"<p>class to incapsulate the methods for updating covariance scalers S</p> Source code in <code>bmm_multitask_learning/coalescent/coalescent_learner.py</code> <pre><code>class S_handler:\n    \"\"\"\n    class to incapsulate the methods for updating covariance scalers S\n    \"\"\"\n    def __init__(self, s_list):\n        self.s_list = s_list\n        self.K = len(self.s_list)\n\n    def log_prob_S(self, s, R_inv, L_inv, P, W):\n        \"\"\"\n        returns the log probability of proposed parameters for S\n        \"\"\"\n        s_neg = -s\n        s_max = np.max(s_neg)\n        s_neg = s_neg - s_max\n        s_exp = np.exp(s_neg) * np.exp(s_max)\n        if len(s_exp.shape) &gt; 1:\n            s_exp = s_exp.squeeze()\n        S_exp = np.diag(s_exp)\n\n        S = np.diag(s)\n\n        cov_m = S_exp @ R_inv @ S_exp\n        return - np.trace(S) - 1/2 * np.trace((S - P) @ L_inv @ (S - P)) - 1/2 * np.trace(W @ cov_m @ W)\n\n    def _optimize_s(self, R, L, w, p, s_0, verbose=False):\n        \"\"\"\n        This method is to optimize for given parameters\n        \"\"\"\n        L_inv = np.linalg.inv(L)\n        R_inv = np.linalg.inv(R)\n        W = np.diag(w)\n        P = np.diag(p)\n        d = R.shape[0]\n\n        def grad_S(s):\n            s_neg = -s\n            s_max = np.max(s_neg)\n            s_neg = s_neg - s_max\n            s_exp = np.exp(s_neg) * np.exp(s_max)\n            if len(s_exp.shape) &gt; 1:\n                s_exp = s_exp.squeeze()\n            S_exp = np.diag(s_exp)\n\n            S = np.diag(s)\n\n            cov_m = S_exp @ R_inv @ S_exp\n            return (- np.eye(d) - (S - P) @ L_inv + W @ cov_m @ W).diagonal()\n\n        if verbose:\n            print(f\"[INFO] log prob start: {self.log_prob_S(s_0, R_inv, L_inv, P, W)}\")\n\n        optimizer = grad_optimizer(100, 0.001, grad_S)\n        s_opt = optimizer.run(s_0)\n\n        if verbose:\n            print(f\"[INFO] log prob trained: {self.log_prob_S(s_opt)}\")\n\n        return s_opt\n\n    def update_param(self, R, L, weights,\n                    coalescent_tree: CoalescentTree,\n                    verbose=False):\n        for i in range(self.K):\n            parent_s = coalescent_tree.leaves[i].parent.mean\n            w = weights[i]\n            s_0 = self.s_list[i]\n            self.s_list[i] = self._optimize_s(R, L, w, parent_s, s_0, verbose)\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.coalescent_learner.S_handler.log_prob_S","title":"<code>log_prob_S(s, R_inv, L_inv, P, W)</code>","text":"<p>returns the log probability of proposed parameters for S</p> Source code in <code>bmm_multitask_learning/coalescent/coalescent_learner.py</code> <pre><code>def log_prob_S(self, s, R_inv, L_inv, P, W):\n    \"\"\"\n    returns the log probability of proposed parameters for S\n    \"\"\"\n    s_neg = -s\n    s_max = np.max(s_neg)\n    s_neg = s_neg - s_max\n    s_exp = np.exp(s_neg) * np.exp(s_max)\n    if len(s_exp.shape) &gt; 1:\n        s_exp = s_exp.squeeze()\n    S_exp = np.diag(s_exp)\n\n    S = np.diag(s)\n\n    cov_m = S_exp @ R_inv @ S_exp\n    return - np.trace(S) - 1/2 * np.trace((S - P) @ L_inv @ (S - P)) - 1/2 * np.trace(W @ cov_m @ W)\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.inverse_wishart","title":"<code>inverse_wishart</code>","text":""},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.inverse_wishart.InverseWishart","title":"<code>InverseWishart</code>","text":"<p>class to handle methods of InverseWishart distribution</p> Source code in <code>bmm_multitask_learning/coalescent/inverse_wishart.py</code> <pre><code>class InverseWishart:\n    \"\"\"\n    class to handle methods of InverseWishart distribution\n    \"\"\"\n    def __init__(self, d, degrees_of_freedom, init_cov=None, corr_mat=False):\n        \"\"\"\n        :param d: dimension of the covariance matrix\n        :param degrees_of_freedom: degrees of freedom of the\n            inverse Wishart distribution\n        :param init_cov: initial covariance matrix\n        :param corr_mat: if True, the covariance matrix is a correlation\n            matrix (i.e., it has ones on the diagonal)\n        \"\"\"\n        self.d = d\n        if init_cov is None:\n            init_cov = np.eye(d)\n        else:\n            assert init_cov.shape == (d, d)\n\n        self.cov = init_cov\n        self.degrees_of_freedom = degrees_of_freedom\n        self.corr_mat = corr_mat\n\n    @staticmethod\n    def cov2corr(cov):\n        \"\"\"\n        Conver covariance matrox to correlation matrix\n        \"\"\"\n        if len(cov.shape) == 0:\n            cov = cov[None]\n\n        std_devs = 1/np.sqrt(np.diag(cov))\n        D = np.diag(std_devs)\n\n        d = len(cov.shape)\n\n        if d == 1:\n            D = D[None]\n            cov = cov[None]\n        R = D @ cov @ D\n        return R\n\n    def update_posterior(self, samples):\n        \"\"\"\n        :param samples: shape=(n x d)\n        \"\"\"\n        assert len(samples.shape) == 2 and samples.shape[1] == self.d\n        K = samples.shape[0]\n        self.degrees_of_freedom += K\n\n        S = samples.T @ samples  # d x d\n        self.cov += S\n        return\n\n    def get_most_prob(self):\n        \"\"\"\n        returns the mode of the inverse Wishart distribution\n        \"\"\"\n        cov = self.cov\n        cov = cov/(self.degrees_of_freedom + self.d + 1)\n\n        if self.corr_mat:\n            cov = InverseWishart.cov2corr(cov)\n        return cov\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.inverse_wishart.InverseWishart.__init__","title":"<code>__init__(d, degrees_of_freedom, init_cov=None, corr_mat=False)</code>","text":"<p>:param d: dimension of the covariance matrix :param degrees_of_freedom: degrees of freedom of the     inverse Wishart distribution :param init_cov: initial covariance matrix :param corr_mat: if True, the covariance matrix is a correlation     matrix (i.e., it has ones on the diagonal)</p> Source code in <code>bmm_multitask_learning/coalescent/inverse_wishart.py</code> <pre><code>def __init__(self, d, degrees_of_freedom, init_cov=None, corr_mat=False):\n    \"\"\"\n    :param d: dimension of the covariance matrix\n    :param degrees_of_freedom: degrees of freedom of the\n        inverse Wishart distribution\n    :param init_cov: initial covariance matrix\n    :param corr_mat: if True, the covariance matrix is a correlation\n        matrix (i.e., it has ones on the diagonal)\n    \"\"\"\n    self.d = d\n    if init_cov is None:\n        init_cov = np.eye(d)\n    else:\n        assert init_cov.shape == (d, d)\n\n    self.cov = init_cov\n    self.degrees_of_freedom = degrees_of_freedom\n    self.corr_mat = corr_mat\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.inverse_wishart.InverseWishart.cov2corr","title":"<code>cov2corr(cov)</code>  <code>staticmethod</code>","text":"<p>Conver covariance matrox to correlation matrix</p> Source code in <code>bmm_multitask_learning/coalescent/inverse_wishart.py</code> <pre><code>@staticmethod\ndef cov2corr(cov):\n    \"\"\"\n    Conver covariance matrox to correlation matrix\n    \"\"\"\n    if len(cov.shape) == 0:\n        cov = cov[None]\n\n    std_devs = 1/np.sqrt(np.diag(cov))\n    D = np.diag(std_devs)\n\n    d = len(cov.shape)\n\n    if d == 1:\n        D = D[None]\n        cov = cov[None]\n    R = D @ cov @ D\n    return R\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.inverse_wishart.InverseWishart.get_most_prob","title":"<code>get_most_prob()</code>","text":"<p>returns the mode of the inverse Wishart distribution</p> Source code in <code>bmm_multitask_learning/coalescent/inverse_wishart.py</code> <pre><code>def get_most_prob(self):\n    \"\"\"\n    returns the mode of the inverse Wishart distribution\n    \"\"\"\n    cov = self.cov\n    cov = cov/(self.degrees_of_freedom + self.d + 1)\n\n    if self.corr_mat:\n        cov = InverseWishart.cov2corr(cov)\n    return cov\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.inverse_wishart.InverseWishart.update_posterior","title":"<code>update_posterior(samples)</code>","text":"<p>:param samples: shape=(n x d)</p> Source code in <code>bmm_multitask_learning/coalescent/inverse_wishart.py</code> <pre><code>def update_posterior(self, samples):\n    \"\"\"\n    :param samples: shape=(n x d)\n    \"\"\"\n    assert len(samples.shape) == 2 and samples.shape[1] == self.d\n    K = samples.shape[0]\n    self.degrees_of_freedom += K\n\n    S = samples.T @ samples  # d x d\n    self.cov += S\n    return\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.parameters_cov","title":"<code>parameters_cov</code>","text":""},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.parameters_cov.optimal_cov","title":"<code>optimal_cov(coalesce_tree, dim)</code>","text":"<p>retunrns samples from covariance matrix for InverseWishart distribution parameters update</p> Source code in <code>bmm_multitask_learning/coalescent/parameters_cov.py</code> <pre><code>def optimal_cov(coalesce_tree: CoalescentTree, dim):\n    \"\"\"\n    retunrns samples from covariance matrix\n    for InverseWishart distribution parameters update\n    \"\"\"\n    sample_mean = np.zeros((dim, ), dtype=float)\n    overall_items = 0\n\n    # compute sample mean\n    for item in coalesce_tree.iterate_over():\n        parent = item.parent\n        if parent is not None:\n            overall_items += 1\n            dt = item.t - parent.t\n            D_i = item.mean - parent.mean\n            x_i = D_i / np.sqrt(dt)\n            sample_mean += x_i\n    sample_mean /= overall_items\n\n\n    # compute samples\n    samples = []\n\n    for item in coalesce_tree.iterate_over():\n        parent = item.parent\n        if parent is not None:\n            dt = item.t - parent.t\n            D_i = item.mean - parent.mean\n            x_i = D_i / np.sqrt(dt)\n            samples.append(x_i - sample_mean)\n\n    return np.array(samples)\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.plot_coalescent","title":"<code>plot_coalescent</code>","text":""},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.task_classes","title":"<code>task_classes</code>","text":""},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.task_classes.TaskData","title":"<code>TaskData</code>  <code>dataclass</code>","text":"Source code in <code>bmm_multitask_learning/coalescent/task_classes.py</code> <pre><code>@dataclass\nclass TaskData:\n    X: np.ndarray  # (n_task, d)\n    y: np.ndarray  # (n_task, 1)\n\n    def most_prob_w(self, sigma, rho):\n        \"\"\"\n        :param sigma: cov matrix of prior of w\n        :param rho: scaling coefficient for data\n        \"\"\"\n        coeff = (1/rho**2)\n        mean_est_uncent = coeff * (self.X * self.y).sum(0)\n        cov_fixed = np.linalg.inv(np.linalg.inv(sigma) +\n                            coeff * (self.X.T @ self.X))\n\n        mean_est = cov_fixed @ (mean_est_uncent)\n        return mean_est\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.task_classes.TaskData.most_prob_w","title":"<code>most_prob_w(sigma, rho)</code>","text":"<p>:param sigma: cov matrix of prior of w :param rho: scaling coefficient for data</p> Source code in <code>bmm_multitask_learning/coalescent/task_classes.py</code> <pre><code>def most_prob_w(self, sigma, rho):\n    \"\"\"\n    :param sigma: cov matrix of prior of w\n    :param rho: scaling coefficient for data\n    \"\"\"\n    coeff = (1/rho**2)\n    mean_est_uncent = coeff * (self.X * self.y).sum(0)\n    cov_fixed = np.linalg.inv(np.linalg.inv(sigma) +\n                        coeff * (self.X.T @ self.X))\n\n    mean_est = cov_fixed @ (mean_est_uncent)\n    return mean_est\n</code></pre>"},{"location":"coalescent/reference/#bmm_multitask_learning.coalescent.utils","title":"<code>utils</code>","text":""},{"location":"javascripts/hooks/","title":"Hooks","text":"In\u00a0[\u00a0]: Copied! <pre>import shutil\n</pre> import shutil In\u00a0[\u00a0]: Copied! <pre>def on_pre_build(*args, **kwargs):\n    shutil.copy2(\n        \"examples/sbmtl/experiment_testing_method.ipynb\", \n        \"docs/sbmtl/experiment_testing_method.ipynb\"\n    )\n</pre> def on_pre_build(*args, **kwargs):     shutil.copy2(         \"examples/sbmtl/experiment_testing_method.ipynb\",          \"docs/sbmtl/experiment_testing_method.ipynb\"     )"},{"location":"sbmtl/experiment_testing_method/","title":"Example","text":"In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom bmm_multitask_learning.sbmtl.sparse_bayesian_regression import SparseBayesianRegression\n</pre> import torch import torch.nn as nn from bmm_multitask_learning.sbmtl.sparse_bayesian_regression import SparseBayesianRegression In\u00a0[3]: Copied! <pre># \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nn_samples = 200\nn_features = 10\nn_tasks = 5\nX = torch.randn(n_samples, n_features)\ntrue_w = torch.zeros(n_tasks,n_features, )\ntrue_w[:, :3] = torch.randn(n_tasks, 3)  # \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0432\u044b\u0435 3 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0437\u043d\u0430\u0447\u0438\u043c\u044b\nY = true_w @ X.T+ 0.5 * torch.randn(n_tasks, n_samples)  # \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0448\u0443\u043c\u0430\nprint(Y.shape)\nP = Y.shape[0]\n</pre> # \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 n_samples = 200 n_features = 10 n_tasks = 5 X = torch.randn(n_samples, n_features) true_w = torch.zeros(n_tasks,n_features, ) true_w[:, :3] = torch.randn(n_tasks, 3)  # \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0432\u044b\u0435 3 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0437\u043d\u0430\u0447\u0438\u043c\u044b Y = true_w @ X.T+ 0.5 * torch.randn(n_tasks, n_samples)  # \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0448\u0443\u043c\u0430 print(Y.shape) P = Y.shape[0] <pre>torch.Size([5, 200])\n</pre> In\u00a0[4]: Copied! <pre># \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0441\u0442\u043e\u0439 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\ntorch.manual_seed(0)\nmodel = nn.Linear(n_features, P, bias=False)\n</pre> # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0441\u0442\u043e\u0439 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 torch.manual_seed(0) model = nn.Linear(n_features, P, bias=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[5]: Copied! <pre># \u0413\u0440\u0443\u043f\u043f\u044b \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043f\u043e 2 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0432 \u0433\u0440\u0443\u043f\u043f\u0435)\ngroup_indices = [list(range(i, i+2)) for i in range(0, n_features, 2)]\n</pre> # \u0413\u0440\u0443\u043f\u043f\u044b \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043f\u043e 2 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 \u0432 \u0433\u0440\u0443\u043f\u043f\u0435) group_indices = [list(range(i, i+2)) for i in range(0, n_features, 2)] In\u00a0[6]: Copied! <pre># \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 Sparse Bayesian Regression\nsbr = SparseBayesianRegression(model, group_indices, device='cpu')\nsbr.fit(X, Y, num_iter=20)\n</pre> # \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 Sparse Bayesian Regression sbr = SparseBayesianRegression(model, group_indices, device='cpu') sbr.fit(X, Y, num_iter=20) <pre>/home/sasha/bmm-multitask-learning/bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py:194: RuntimeWarning: invalid value encountered in sqrt\n  z = np.sqrt(chi * phi)\n/home/sasha/bmm-multitask-learning/bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py:198: RuntimeWarning: invalid value encountered in sqrt\n  eq1 = Q * np.log(np.sqrt(phi / chi)) - Q * d_logK * sum_log_gamma\n/home/sasha/bmm-multitask-learning/bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py:199: RuntimeWarning: invalid value encountered in sqrt\n  eq2 = (Q * omega) / chi - (Q / 2) * np.sqrt(phi / chi) * R_omega + 0.5 * sum_inv_gamma\n/home/sasha/bmm-multitask-learning/bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py:200: RuntimeWarning: invalid value encountered in sqrt\n  eq3 = Q * np.sqrt(chi/ phi) * R_omega - sum_gamma\n</pre> In\u00a0[7]: Copied! <pre># \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0441 \u043e\u0431\u044b\u0447\u043d\u043e\u0439 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0435\u0439\ny_pred_sbr = sbr.predict(X).squeeze()\n# \u041e\u0431\u044b\u0447\u043d\u0430\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0434\u043b\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f\nols = nn.Linear(n_features, P, bias=False)\noptimizer = torch.optim.SGD(ols.parameters(), lr=0.1)\nfor _ in range(200):\n    optimizer.zero_grad()\n    loss = torch.mean(((ols(X).T - Y) ** 2))\n    loss.backward()\n    optimizer.step()\ny_pred_ols = ols(X).squeeze()\n</pre> # \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0438 \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0441 \u043e\u0431\u044b\u0447\u043d\u043e\u0439 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0435\u0439 y_pred_sbr = sbr.predict(X).squeeze() # \u041e\u0431\u044b\u0447\u043d\u0430\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0434\u043b\u044f \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f ols = nn.Linear(n_features, P, bias=False) optimizer = torch.optim.SGD(ols.parameters(), lr=0.1) for _ in range(200):     optimizer.zero_grad()     loss = torch.mean(((ols(X).T - Y) ** 2))     loss.backward()     optimizer.step() y_pred_ols = ols(X).squeeze() In\u00a0[8]: Copied! <pre>from matplotlib import pyplot as plt\n# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432\nplt.figure(figsize=(10,4))\nplt.stem(true_w.reshape(-1).numpy(), linefmt='g-', markerfmt='go', basefmt=' ', label='\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0432\u0435\u0441\u0430')\nplt.stem(sbr._get_flat_params().cpu().detach().numpy(), linefmt='b-', markerfmt='bo', basefmt=' ', label='Sparse Bayesian')\nplt.stem(ols.weight.detach().cpu().numpy().flatten(), linefmt='r--', markerfmt='ro', basefmt=' ', label='OLS')\nplt.legend()\nplt.title('\u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0432\u0435\u0441\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0435\u0439')\nplt.xlabel('\u0418\u043d\u0434\u0435\u043a\u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430')\nplt.ylabel('\u0412\u0435\u0441')\nplt.show()\n</pre> from matplotlib import pyplot as plt # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 plt.figure(figsize=(10,4)) plt.stem(true_w.reshape(-1).numpy(), linefmt='g-', markerfmt='go', basefmt=' ', label='\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0432\u0435\u0441\u0430') plt.stem(sbr._get_flat_params().cpu().detach().numpy(), linefmt='b-', markerfmt='bo', basefmt=' ', label='Sparse Bayesian') plt.stem(ols.weight.detach().cpu().numpy().flatten(), linefmt='r--', markerfmt='ro', basefmt=' ', label='OLS') plt.legend() plt.title('\u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0432\u0435\u0441\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0435\u0439') plt.xlabel('\u0418\u043d\u0434\u0435\u043a\u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430') plt.ylabel('\u0412\u0435\u0441') plt.show() In\u00a0[9]: Copied! <pre>from matplotlib import pyplot as plt\n# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432\nplt.figure(figsize=(10,4))\nplt.stem(true_w.reshape(-1).numpy(), linefmt='g-', markerfmt='go', basefmt=' ', label='\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0432\u0435\u0441\u0430')\nplt.stem(ols.weight.detach().cpu().numpy().flatten(), linefmt='r--', markerfmt='ro', basefmt=' ', label='OLS')\nplt.legend()\nplt.title('\u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0432\u0435\u0441\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0435\u0439')\nplt.xlabel('\u0418\u043d\u0434\u0435\u043a\u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430')\nplt.ylabel('\u0412\u0435\u0441')\nplt.show()\n</pre> from matplotlib import pyplot as plt # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 plt.figure(figsize=(10,4)) plt.stem(true_w.reshape(-1).numpy(), linefmt='g-', markerfmt='go', basefmt=' ', label='\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0432\u0435\u0441\u0430') plt.stem(ols.weight.detach().cpu().numpy().flatten(), linefmt='r--', markerfmt='ro', basefmt=' ', label='OLS') plt.legend() plt.title('\u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0432\u0435\u0441\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0435\u0439') plt.xlabel('\u0418\u043d\u0434\u0435\u043a\u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430') plt.ylabel('\u0412\u0435\u0441') plt.show() In\u00a0[10]: Copied! <pre>from matplotlib import pyplot as plt\n# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432\nplt.figure(figsize=(10,4))\nplt.stem(true_w.reshape(-1).numpy(), linefmt='g-', markerfmt='go', basefmt=' ', label='\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0432\u0435\u0441\u0430')\nplt.stem(sbr._get_flat_params().cpu().detach().numpy(), linefmt='b-', markerfmt='bo', basefmt=' ', label='Sparse Bayesian')\nplt.legend()\nplt.title('\u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0432\u0435\u0441\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0435\u0439')\nplt.xlabel('\u0418\u043d\u0434\u0435\u043a\u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430')\nplt.ylabel('\u0412\u0435\u0441')\nplt.show()\n</pre> from matplotlib import pyplot as plt # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 plt.figure(figsize=(10,4)) plt.stem(true_w.reshape(-1).numpy(), linefmt='g-', markerfmt='go', basefmt=' ', label='\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0432\u0435\u0441\u0430') plt.stem(sbr._get_flat_params().cpu().detach().numpy(), linefmt='b-', markerfmt='bo', basefmt=' ', label='Sparse Bayesian') plt.legend() plt.title('\u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0432\u0435\u0441\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0435\u0439') plt.xlabel('\u0418\u043d\u0434\u0435\u043a\u0441 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430') plt.ylabel('\u0412\u0435\u0441') plt.show() In\u00a0[12]: Copied! <pre># \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439\nplt.figure(figsize=(6,6))\nplt.scatter(Y.detach().numpy(), y_pred_sbr.detach().numpy(), alpha=0.7, label='Sparse Bayesian')\nplt.scatter(Y.detach().numpy(), y_pred_ols.detach().numpy(), alpha=0.7, label='OLS', marker='x')\nplt.plot(Y.detach().numpy(), Y.detach().numpy(), 'k--', label='y = y')\nplt.xlabel('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f')\nplt.ylabel('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f')\nplt.legend()\nplt.title('\u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\u0439')\nplt.show()\n</pre> # \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 plt.figure(figsize=(6,6)) plt.scatter(Y.detach().numpy(), y_pred_sbr.detach().numpy(), alpha=0.7, label='Sparse Bayesian') plt.scatter(Y.detach().numpy(), y_pred_ols.detach().numpy(), alpha=0.7, label='OLS', marker='x') plt.plot(Y.detach().numpy(), Y.detach().numpy(), 'k--', label='y = y') plt.xlabel('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f') plt.ylabel('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f') plt.legend() plt.title('\u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\u0439') plt.show()"},{"location":"sbmtl/experiment_testing_method/#sparse-bayesian-regression","title":"\u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 Sparse Bayesian Regression\u00b6","text":""},{"location":"sbmtl/hooks/","title":"Hooks","text":"In\u00a0[\u00a0]: Copied! <pre>import shutil\n</pre> import shutil In\u00a0[\u00a0]: Copied! <pre>def on_pre_build(*args, **kwargs):\n    shutil.copy2(\n        \"examples/sbmtl/experiment_testing_method.ipynb\", \n        \"docs/sbmtl/experiment_testing_method.ipynb\"\n    )\n</pre> def on_pre_build(*args, **kwargs):     shutil.copy2(         \"examples/sbmtl/experiment_testing_method.ipynb\",          \"docs/sbmtl/experiment_testing_method.ipynb\"     )"},{"location":"sbmtl/reference/","title":"Reference for sparse multitask approach in multitask learning","text":""},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression","title":"<code>bmm_multitask_learning.sbmtl.sparse_bayesian_regression</code>","text":""},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression","title":"<code>SparseBayesianRegression</code>","text":"Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>class SparseBayesianRegression:\n    def __init__(self, model: nn.Module, group_indices: List[List[int]],\n                 device: Optional[str] = None):\n        \"\"\"\n        model: torch.nn.Module (\u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0438\u043b\u0438 \u043b\u044e\u0431\u0430\u044f torch-\u043c\u043e\u0434\u0435\u043b\u044c)\n        group_indices: \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u043f\u0438\u0441\u043a\u043e\u0432 \u0438\u043d\u0434\u0435\u043a\u0441\u043e\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0433\u0440\u0443\u043f\u043f\u0430\u043c\n        device: cpu/cuda\n        \"\"\"\n        self.model = model\n        self.group_indices = group_indices\n        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n        self._init_hyperparams()\n\n    def _init_hyperparams(self) -&gt; None:\n        \"\"\"\n        \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u0442 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438, \u0432\u043a\u043b\u044e\u0447\u0430\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u0440\u0430\u0439\u043e\u0440\u0430 \u0438 \u043f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u0430.\n        \"\"\"\n        G = len(self.group_indices)\n        # \u041e\u0431\u0449\u0438\u0435 (prior) \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f\n        self.omega_prior = torch.tensor(1.0, device=self.device, requires_grad=False)\n        self.chi_prior = torch.tensor(1.0, device=self.device, requires_grad=False)\n        self.phi_prior = torch.tensor(1.0, device=self.device, requires_grad=False)\n        self.nu_prior = torch.tensor(1.0, device=self.device, requires_grad=False) # \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0434\u043b\u044f v_i\n        # \u041f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b\n        self.omega_post = [torch.tensor(1.0, device=self.device, requires_grad=False) for _ in range(G)]\n        self.chi_post = [torch.tensor(1.0, device=self.device, requires_grad=False) for _ in range(G)]\n        self.phi_post = [torch.tensor(1.0, device=self.device, requires_grad=False) for _ in range(G)]\n        self.nu_post = [torch.tensor(1.0, device=self.device, requires_grad=False) for _ in range(G)]\n        self.tau = torch.tensor(1.0, device=self.device, requires_grad=False)  # \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u044f \u0448\u0443\u043c\u0430\n        self.sigma2 = torch.tensor(1.0, device=self.device, requires_grad=False)  # \u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u044f \u0448\u0443\u043c\u0430\n        self.K = 2  # \u0440\u0430\u043d\u0433 \u043b\u0430\u0442\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0430, \u043c\u043e\u0436\u043d\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0438\u0437\u043e\u0432\u0430\u0442\u044c\n        self.P = self.model(torch.zeros(1, self.model.in_features, device=self.device)).shape[-1] # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u044b\u0445\u043e\u0434\u043e\u0432\n        self.D = self.model.in_features # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0432\u0445\u043e\u0434\u043e\u0432\n        self.Omega_inv_g = [torch.eye(len(idxs), device=self.device) for idxs in self.group_indices]  # [D_g]\n        self.gammas = [1.0 for _ in range(G)]\n        # \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0434\u043b\u044f W, Z, V\n        # W: \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e-\u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435 (M_W, Omega_W, S_W)\n        self.M_W = torch.randn(self.P, self.D, device=self.device) / np.sqrt(self.P * self.D)\n        self.Omega_W = torch.eye(self.D, device=self.device)\n        self.S_W = torch.eye(self.P, device=self.device)\n        # Z: \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u043e \u0433\u0440\u0443\u043f\u043f\u0430\u043c (M_Z[g], Omega_Z[g], S_Z[g])\n        self.M_Z = [torch.randn(self.K, len(idxs), device=self.device) / np.sqrt(self.K *  len(idxs)) for idxs in self.group_indices]\n        self.Omega_Z = [torch.eye(len(idxs), device=self.device) for idxs in self.group_indices]\n        self.S_Z = [torch.eye(self.K, device=self.device) for _ in self.group_indices]\n        # V: \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e-\u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435 (M_V, Omega_V, S_V)\n        self.M_V = torch.zeros(self.P, self.K, device=self.device)\n        self.Omega_V = torch.eye(self.K, device=self.device)\n        self.S_V = torch.eye(self.P, device=self.device)\n\n    def _get_flat_params(self) -&gt; Tensor:\n        \"\"\"\n        \u0412\u044b\u0442\u044f\u0433\u0438\u0432\u0430\u0435\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u0432 \u043e\u0434\u0438\u043d \u0432\u0435\u043a\u0442\u043e\u0440.\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n            Tensor: \u0412\u0435\u043a\u0442\u043e\u0440 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438.\n        \"\"\"\n        # \u0412\u044b\u0442\u044f\u0433\u0438\u0432\u0430\u0435\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u0432 \u043e\u0434\u0438\u043d \u0432\u0435\u043a\u0442\u043e\u0440\n        return torch.cat([p.view(-1) for p in self.model.parameters()])\n\n    def _set_flat_params(self, flat_params: Tensor) -&gt; None:\n        \"\"\"\n        \u0423\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437 \u0432\u0435\u043a\u0442\u043e\u0440\u0430.\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            flat_params (Tensor): \u0412\u0435\u043a\u0442\u043e\u0440 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043c\u043e\u0434\u0435\u043b\u0438.\n        \"\"\"\n        # \u0423\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0437 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\n        pointer = 0\n        for p in self.model.parameters():\n            numel = p.numel()\n            p.data.copy_(flat_params[pointer:pointer+numel].view_as(p))\n            pointer += numel\n    @staticmethod\n    def mean_gig(omega: float, chi: float, phi: float) -&gt; float:\n        \"\"\"\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8x\u27e9 \u0434\u043b\u044f GIG(omega, chi, phi).\n        \u27e8x\u27e9 = sqrt(chi/phi) * R_omega(sqrt(chi*phi))\n        \u0433\u0434\u0435 R_omega(z) = K_{omega+1}(z) / K_{omega}(z)\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n            chi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 chi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n            phi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 phi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n            float: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8x\u27e9.\n        \"\"\"\n        z = (chi * phi) ** 0.5\n        K_omega = kv(omega, z)\n        K_omega_p1 = kv(omega + 1, z)\n        R_omega = K_omega_p1 / K_omega if K_omega != 0 else 0.0\n        return (chi / phi) ** 0.5 * R_omega\n\n    @staticmethod\n    def mean_inv_gig(omega: float, chi: float, phi: float) -&gt; float:\n        \"\"\"\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0439 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u044b \u27e81/x\u27e9 \u0434\u043b\u044f GIG(omega, chi, phi).\n        \u27e81/x\u27e9 = sqrt(chi/phi) * R_{omega-1}(sqrt(chi*phi))\n        \u0433\u0434\u0435 R_{omega-1}(z) = K_{omega}(z) / K_{omega-1}(z)\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n            chi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 chi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n            phi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 phi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n            float: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e81/x\u27e9.\n        \"\"\"\n        z = (chi * phi) ** 0.5\n        K_omega = kv(omega, z)\n        K_omega_m1 = kv(omega - 1, z)\n        R_omega = K_omega_m1 / K_omega if K_omega != 0 else 0.0\n        return (phi / chi) ** 0.5 * R_omega\n\n    @staticmethod\n    def mean_log_gig(omega: float, chi: float, phi: float) -&gt; float:\n        \"\"\"\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0430 \u27e8log(x)\u27e9 \u0434\u043b\u044f GIG(omega, chi, phi).\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n            chi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 chi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n            phi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 phi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n            float: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8log(x)\u27e9.\n        \"\"\"\n        z = (chi * phi) ** 0.5\n        return 0.5 * np.log(chi / phi) + (SparseBayesianRegression.d_log_bessel_k(omega, z))\n\n    @staticmethod\n    def d_log_bessel_k(omega: float, z: float) -&gt; float:\n        \"\"\"\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0443\u044e \u043f\u043e omega \u043e\u0442 log K_omega(z).\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega.\n            z (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 z.\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n            float: \u0417\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u043e\u0439.\n        \"\"\"\n        # \u041f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0430\u044f \u043f\u043e omega \u043e\u0442 log K_omega(z)\n        eps = 1e-5\n        return (np.log(kv(omega + eps, z)) - np.log(kv(omega - eps, z))) / (2 * eps)\n\n    def update_gig_hyperparams(self, group_idx: int, mean_gamma: float, mean_inv_gamma: float, mean_log_gamma: float) -&gt; None:\n        \"\"\"\n        \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b GIG (omega, chi, phi) \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b.\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            group_idx (int): \u0418\u043d\u0434\u0435\u043a\u0441 \u0433\u0440\u0443\u043f\u043f\u044b.\n            mean_gamma (float): \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8gamma\u27e9.\n            mean_inv_gamma (float): \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e81/gamma\u27e9.\n            mean_log_gamma (float): \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8log(gamma)\u27e9.\n        \"\"\"\n        # \u0427\u0438\u0441\u043b\u0435\u043d\u043d\u043e \u0440\u0435\u0448\u0430\u0435\u0442 \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u0434\u043b\u044f omega, chi, phi \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b\n        Q = 1  # \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b, \u0435\u0441\u043b\u0438 \u0433\u0440\u0443\u043f\u043f \u0431\u043e\u043b\u044c\u0448\u0435, \u043c\u043e\u0436\u043d\u043e \u043e\u0431\u043e\u0431\u0449\u0438\u0442\u044c\n        def equations(params):\n            omega, chi, phi = params\n            z = np.sqrt(chi * phi)\n            K_omega = kv(omega, z)\n            d_logK = self.d_log_bessel_k(omega, z)\n            R_omega = kv(omega + 1, z) / K_omega if K_omega != 0 else 0.0\n            eq1 = Q * np.log(np.sqrt(phi / chi)) - Q * d_logK - Q * mean_log_gamma\n            eq2 = (Q * omega) / chi - (Q / 2) * np.sqrt(phi / chi) * R_omega + 0.5 * mean_inv_gamma\n            eq3 = (Q / np.sqrt(chi * phi)) * R_omega - mean_gamma\n            return [eq1, eq2, eq3]\n        # \u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n        omega0 = float(self.omega_post[group_idx].cpu().numpy())\n        chi0 = float(self.chi_post[group_idx].cpu().numpy())\n        phi0 = float(self.phi_post[group_idx].cpu().numpy())\n        sol = root(equations, [omega0, chi0, phi0], method='hybr')\n        if sol.success:\n            self.omega_post[group_idx] = torch.tensor(sol.x[0], device=self.device)\n            self.chi_post[group_idx] = torch.tensor(sol.x[1], device=self.device)\n            self.phi_post[group_idx] = torch.tensor(sol.x[2], device=self.device)\n\n    def update_gig_prior(self, mean_gammas: List[float], mean_inv_gammas: List[float], mean_log_gammas: List[float]) -&gt; None:\n        \"\"\"\n        \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b GIG (omega, chi, phi) \u0434\u043b\u044f \u043e\u0431\u0449\u0435\u0433\u043e \u043f\u0440\u0430\u0439\u043e\u0440\u0430.\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            mean_gammas (List[float]): \u0421\u043f\u0438\u0441\u043e\u043a \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0439 \u27e8gamma\u27e9 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f.\n            mean_inv_gammas (List[float]): \u0421\u043f\u0438\u0441\u043e\u043a \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0439 \u27e81/gamma\u27e9 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f.\n            mean_log_gammas (List[float]): \u0421\u043f\u0438\u0441\u043e\u043a \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0439 \u27e8log(gamma)\u27e9 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f.\n        \"\"\"\n        # \u0427\u0438\u0441\u043b\u0435\u043d\u043d\u043e \u0440\u0435\u0448\u0430\u0435\u0442 \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u0434\u043b\u044f omega, chi, phi \u0434\u043b\u044f \u043e\u0431\u0449\u0435\u0433\u043e \u043f\u0440\u0430\u0439\u043e\u0440\u0430 (\u043f\u043e \u0441\u0440\u0435\u0434\u043d\u0438\u043c \u043f\u043e \u0433\u0440\u0443\u043f\u043f\u0430\u043c)\n        Q = len(mean_gammas)\n        sum_gamma = torch.sum(torch.tensor(mean_gammas))\n        sum_inv_gamma = torch.sum(torch.tensor(mean_inv_gammas))\n        sum_log_gamma = torch.sum(torch.tensor(mean_log_gammas))\n        def equations(params):\n            omega, chi, phi = params\n            z = np.sqrt(chi * phi)\n            K_omega = kv(omega, z)\n            d_logK = self.d_log_bessel_k(omega, z)\n            R_omega = kv(omega + 1, z) / K_omega if K_omega != 0 else 0.0\n            eq1 = Q * np.log(np.sqrt(phi / chi)) - Q * d_logK * sum_log_gamma\n            eq2 = (Q * omega) / chi - (Q / 2) * np.sqrt(phi / chi) * R_omega + 0.5 * sum_inv_gamma\n            eq3 = Q * np.sqrt(chi/ phi) * R_omega - sum_gamma\n            return [eq1, eq2, eq3]\n        omega0 = float(self.omega_prior.cpu().numpy())\n        chi0 = float(self.chi_prior.cpu().numpy())\n        phi0 = float(self.phi_prior.cpu().numpy())\n        sol = root(equations, [omega0, chi0, phi0], method='hybr')\n        if sol.success:\n            self.omega_prior = torch.tensor(sol.x[0], device=self.device)\n            self.chi_prior = torch.tensor(sol.x[1], device=self.device)\n            self.phi_prior = torch.tensor(sol.x[2], device=self.device)\n\n    def compute_moments_W(self, M_W: Tensor, Omega_W: Tensor, S_W: Tensor) -&gt; Tensor:\n        \"\"\"\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8W W^T\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b W.\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            M_W (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 W.\n            Omega_W (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c W.\n            S_W (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c W.\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n            Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8W W^T\u27e9.\n        \"\"\"\n        # \u041c\u043e\u043c\u0435\u043d\u0442: E[W W^T] = M_W M_W^T + tr(S_W) * Omega_W\n        return M_W @ M_W.t() + torch.trace(S_W) * Omega_W\n\n    def compute_moments_Z(self, M_Z: Tensor, Omega_Z: Tensor, S_Z: Tensor) -&gt; Tensor:\n        \"\"\"\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8Z Z^T\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b Z.\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            M_Z (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 Z.\n            Omega_Z (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c Z.\n            S_Z (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c Z.\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n            Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8Z Z^T\u27e9.\n        \"\"\"\n        # \u041c\u043e\u043c\u0435\u043d\u0442: E[Z Z^T] = M_Z M_Z^T + tr(S_Z) * Omega_Z\n        return M_Z @ M_Z.t() + torch.trace(S_Z) * Omega_Z\n\n    def compute_moments_VVT(self, M_V: Tensor, Omega_V: Tensor, S_V: Tensor) -&gt; Tensor:\n        \"\"\"\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8V V^T\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b V.\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            M_V (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 V.\n            Omega_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c V.\n            S_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c V.\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n            Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8V V^T\u27e9.\n        \"\"\"\n        # \u041c\u043e\u043c\u0435\u043d\u0442: E[V V^T] = M_V M_V^T + tr(S_V) * Omega_V\n        return M_V @ M_V.t() + torch.trace(Omega_V) * S_V\n    def compute_moments_VTV(self, M_V: Tensor, Omega_V: Tensor, S_V: Tensor) -&gt; Tensor:\n        \"\"\"\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8V^T V\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b V.\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            M_V (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 V.\n            Omega_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c V.\n            S_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c V.\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n            Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8V^T V\u27e9.\n        \"\"\"\n        # \u041c\u043e\u043c\u0435\u043d\u0442: E[V V^T] = M_V M_V^T + tr(S_V) * Omega_V\n        return M_V.T @ M_V + torch.trace(S_V) * Omega_V\n\n    def e_step(self, X: Tensor, Y: Tensor) -&gt; Dict[str, Any]:\n        \"\"\"\n        E-\u0448\u0430\u0433: \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0438\u0440\u0443\u0435\u0442 \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u043d\u044b\u0445 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0438 \u043c\u043e\u043c\u0435\u043d\u0442\u043e\u0432.\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            X (Tensor): \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (D, N)\n            Y (Tensor): \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0442\u043a\u043b\u0438\u043a\u043e\u0432 (P, N)\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442: dict[str, Any] \u2014 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u043c\u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430\u043c\u0438 \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u0434\u043b\u044f M-\u0448\u0430\u0433a.\n        \"\"\"\n        group_moments = self._compute_group_moments()\n        self._update_posterior_matrices(X, Y, group_moments)\n        self._update_posterior_wishart(group_moments)\n        self._update_posterior_gig(group_moments)\n        return {\n            'mean_gammas': group_moments['mean_gammas'],\n            'mean_inv_gammas': group_moments['mean_inv_gammas'],\n            'mean_log_gammas': group_moments['mean_log_gammas'],\n            \"M_W\": self.M_W, \"Omega_W\": self.Omega_W, \"S_W\": self.S_W,\n            \"M_Z\": self.M_Z, \"Omega_Z\": self.Omega_Z, \"S_Z\": self.S_Z,\n            \"M_V\": self.M_V, \"Omega_V\": self.Omega_V, \"S_V\": self.S_V\n        }\n\n    def _compute_group_moments(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442\u044b (\u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f) \u043f\u043e \u0432\u0441\u0435\u043c \u0433\u0440\u0443\u043f\u043f\u0430\u043c, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0431\u043b\u043e\u0447\u043d\u044b\u0435 \u043c\u0430\u0442\u0440\u0438\u0446\u044b Gamma \u0438 Omega_inv.\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442: dict[str, Any] \u2014 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441 \u044d\u0442\u0438\u043c\u0438 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u0430\u043c\u0438.\n        \"\"\"\n        G = len(self.group_indices)\n        D = self.D\n        mean_gammas, mean_inv_gammas, mean_log_gammas = [], [], []\n        Gamma = torch.zeros(D, D, device=self.device)\n        Omega_inv = torch.zeros(D, D, device=self.device)\n        for g, idxs in enumerate(self.group_indices):\n            omega = float(self.omega_post[g].cpu().numpy())\n            chi = float(self.chi_post[g].cpu().numpy())\n            phi = float(self.phi_post[g].cpu().numpy())\n            mg = self.mean_gig(omega, chi, phi)\n            mig = self.mean_inv_gig(omega, chi, phi)\n            mlg = self.mean_log_gig(omega, chi, phi)\n            mean_gammas.append(mg)\n            mean_inv_gammas.append(mig)\n            mean_log_gammas.append(mlg)\n            Gamma[idxs, :][:, idxs] = mg * torch.eye(len(idxs), device=self.device)\n            Omega_inv[idxs, :][:, idxs] = self.Omega_inv_g[g]\n        return {\n            'mean_gammas': mean_gammas,\n            'mean_inv_gammas': mean_inv_gammas,\n            'mean_log_gammas': mean_log_gammas,\n            'Gamma': Gamma,\n            'Omega_inv': Omega_inv\n        }\n\n    def _update_posterior_matrices(self, X: Tensor, Y: Tensor, group_moments: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u043d\u044b\u0445 \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e-\u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u044b\u0445 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 W, Z, V \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0442\u0435\u043a\u0443\u0449\u0438\u0445 \u043c\u043e\u043c\u0435\u043d\u0442\u043e\u0432 \u0438 \u0434\u0430\u043d\u043d\u044b\u0445.\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            X (Tensor): \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n            Y (Tensor): \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0442\u043a\u043b\u0438\u043a\u043e\u0432\n            group_moments (dict): \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441 \u043c\u043e\u043c\u0435\u043d\u0442\u0430\u043c\u0438 \u0438 \u0431\u043b\u043e\u0447\u043d\u044b\u043c\u0438 \u043c\u0430\u0442\u0440\u0438\u0446\u0430\u043c\u0438\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442: None\n        \"\"\"\n        Gamma = group_moments['Gamma']\n        Omega_inv = group_moments['Omega_inv']\n        tau = self.tau\n        sigma2 = self.sigma2\n        # \u041e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 W\n        Omega_W_inv = (1.0 / tau) * Omega_inv @ Gamma + (1.0 / sigma2) * (X @ X.t())\n        self.Omega_W = torch.linalg.inv(Omega_W_inv)\n        Z = torch.cat(self.M_Z, dim=1)  # [K, D]\n        self.M_W = ((1.0 / tau) * self.M_V @ Z @ Omega_inv @ Gamma + (1.0 / sigma2) * Y @ X.t()) @ self.Omega_W\n        self.S_W = torch.eye(self.P, device=self.device)\n        # \u041e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 Z\n        self.M_Z, self.Omega_Z, self.S_Z = [], [], []\n        for g, idxs in enumerate(self.group_indices):\n            Dg = len(idxs)\n            Wg = self.M_W[:, idxs]  # [P, Dg]\n            moment_V = self.compute_moments_VTV(self.M_V, self.Omega_V, self.S_V)\n            S_Zi = torch.linalg.inv((1.0 / tau) * moment_V + torch.eye(self.K, device=self.device))\n            M_Zi = (1.0 / tau) * S_Zi @ self.M_V.t() @ Wg\n            Omega_Zi = (1.0 / group_moments['mean_gammas'][g]) * torch.linalg.inv(self.Omega_inv_g[g])\n            self.M_Z.append(M_Zi)\n            self.Omega_Z.append(Omega_Zi)\n            self.S_Z.append(S_Zi)\n        # \u041e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 V\n        Omega_V_inv = torch.zeros(self.K, self.K, device=self.device)\n        for g, idxs in enumerate(self.group_indices):\n            Omega_i_inv = self.Omega_inv_g[g]\n            M_Zi = self.M_Z[g]\n            S_Zi = self.S_Z[g]\n            moment_Z = Omega_i_inv * torch.trace(S_Zi) + M_Zi @ Omega_i_inv @ M_Zi.t()\n            Omega_V_inv += group_moments['mean_gammas'][g] * moment_Z\n        Omega_V_inv += torch.eye(self.K, device=self.device)\n        self.Omega_V = torch.linalg.inv(Omega_V_inv)\n        self.M_V = self.M_W @ Omega_inv @ Gamma @ Z.t() @ self.Omega_V\n        self.S_V = tau * torch.eye(self.P, device=self.device)\n\n    def _update_posterior_wishart(self, group_moments: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f Wishart (Lambda, nu, Omega_inv_g) \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b.\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            group_moments (dict): \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441 \u043c\u043e\u043c\u0435\u043d\u0442\u0430\u043c\u0438 \u0438 \u0431\u043b\u043e\u0447\u043d\u044b\u043c\u0438 \u043c\u0430\u0442\u0440\u0438\u0446\u0430\u043c\u0438\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442: None\n        \"\"\"\n        self.Lambda, self.nu_post = [], []\n        for g, idxs in enumerate(self.group_indices):\n            Dg = len(idxs)\n            Wg = self.M_W[:, idxs]  # [P, Dg]\n            Zg = self.M_Z[g]  # [K, Dg]\n            resid = Wg - self.M_V @ Zg.T  # [P, Dg]\n            Omega_W_g = self.Omega_W[idxs, :][:, idxs]\n            moment_resid_mean_part = resid.T @ resid\n            Omega_i_inv = self.Omega_inv_g[g]\n            D_W = torch.trace(self.S_W) * Omega_W_g\n            moment_resid_disp_part = D_W + self.Omega_Z[g] * torch.sum(self.S_Z[g] * self.Omega_V) * torch.trace(self.S_V) + self.M_Z[g].T @ self.Omega_V @self.M_Z[g] * torch.trace(self.S_V)+\\\n            self.Omega_Z[g] * torch.sum(self.S_Z[g] * (self.M_Z[g].T @ self.M_Z[g]))\n            moment_resid = moment_resid_mean_part + moment_resid_disp_part\n            moment_Z =  self.Omega_Z[g] * torch.sum(self.S_Z[g] @ Omega_i_inv) + self.M_Z[g] @ Omega_i_inv @ self.M_Z[g].T\n            Lambda_i = (1.0 / self.tau) * group_moments['mean_gammas'][g] * moment_resid + group_moments['mean_gammas'][g] * moment_Z + torch.eye(Dg, device=self.device)\n            self.Lambda.append(Lambda_i)\n            self.nu_post.append(float(self.nu_prior) + self.P + self.K)\n        self.Omega_inv_g = [(Dg + self.nu_post[g] - 1) * torch.linalg.inv(self.Lambda[g]) for g, idxs in enumerate(self.group_indices)]\n\n    def _update_posterior_gig(self, group_moments: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u043d\u043e\u0433\u043e GIG (omega_post, chi_post, phi_post) \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b.\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            group_moments (dict): \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441 \u043c\u043e\u043c\u0435\u043d\u0442\u0430\u043c\u0438 \u0438 \u0431\u043b\u043e\u0447\u043d\u044b\u043c\u0438 \u043c\u0430\u0442\u0440\u0438\u0446\u0430\u043c\u0438\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442: None\n        \"\"\"\n        for g, idxs in enumerate(self.group_indices):\n            Dg = len(idxs)\n            # omega_post\n            self.omega_post[g] = self.omega_prior + 0.5 * (self.P + self.K) * Dg\n            # chi_post\n            self.chi_post[g] = self.chi_prior\n            # phi_post\n            #\u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435\n            Omega_i_inv = self.Omega_inv_g[g]\n            Wg = self.M_W[:, idxs]  # [P, Dg]\n            Zg = self.M_Z[g]  # [K, Dg]\n            resid = Wg - self.M_V @ Zg\n            Omega_W_g = self.Omega_W[idxs, :][:, idxs]\n            Omega_inv_i = self.Omega_inv_g[g]\n            #\u041f\u043e\u0434\u0441\u0447\u0435\u0442\u044b \u043c\u043e\u043c\u0435\u043d\u0442\u043e\u0432\n            moment_resid_mean_part = resid @ Omega_i_inv @ resid.t()\n            moment_resid_W_disp_part = self.S_W * torch.sum(Omega_W_g * Omega_inv_i)\n            moment_resid_VZ_disp_part = self.S_V * torch.sum(self.Omega_V * self.S_Z[g]) * torch.sum(self.Omega_Z[g]* Omega_i_inv) + self.M_V @ self.S_Z[g] @self.M_V.T * torch.sum(self.Omega_Z[g]* Omega_i_inv)+\\\n            self.S_V * torch.sum(self.Omega_V * (self.M_Z[g] @ Omega_inv_i @self.M_Z[g].T))\n            moment_resid = moment_resid_mean_part + moment_resid_W_disp_part + moment_resid_VZ_disp_part\n            tr_resid = torch.trace(moment_resid)\n            # \u041c\u043e\u043c\u0435\u043d\u0442: &lt;Z_i Omega_i^{-1} Z_i^T&gt;\n            moment_Z = Zg @ Omega_i_inv @ Zg.t() + torch.trace(self.S_Z[g]) * Omega_i_inv\n            tr_Z = torch.trace(moment_Z)\n            self.phi_post[g] = self.phi_prior + (1.0 / self.tau) * tr_resid + tr_Z\n    def m_step(self, post: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        M-\u0448\u0430\u0433: \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 E-\u0448\u0430\u0433a.\n\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            post (Dict[str, Any]): \u0421\u043b\u043e\u0432\u0430\u0440\u044c \u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438 E-\u0448\u0430\u0433a, \u0432\u043a\u043b\u044e\u0447\u0430\u044f \u043c\u043e\u043c\u0435\u043d\u0442\u044b \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f.\n        \"\"\"\n        # \u0422\u0435\u043f\u0435\u0440\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0431\u0449\u0435\u0433\u043e \u043f\u0440\u0430\u0439\u043e\u0440\u0430\n        self.update_gig_prior(post['mean_gammas'], post['mean_inv_gammas'], post['mean_log_gammas'])\n        # \u041e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 tau (\u0434\u0438\u0441\u043f\u0435\u0440\u0441\u0438\u044f \u0448\u0443\u043c\u0430)\n        # ... (\u0441\u043c. 4.2, \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0437\u0430\u0434\u0430\u0447\u0438)\n    def fit(self, X: Tensor, Y: Tensor, num_iter: int = 10) -&gt; None:\n        \"\"\"\n        \u041e\u0431\u0443\u0447\u0430\u0435\u0442 \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430.\n\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            X (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (N, D).\n            Y (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0442\u043a\u043b\u0438\u043a\u043e\u0432 (N, P).\n            num_iter (int): \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439 EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430.\n        \"\"\"\n        X = X.T\n        for _ in range(num_iter):\n            post = self.e_step(X, Y)\n            self.m_step(post)\n            self._set_flat_params(post[\"M_W\"])\n\n    def predict(self, X: Tensor) -&gt; Tensor:\n        \"\"\"\n        \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438.\n\n        \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n            X (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (N, D).\n\n        \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n            Tensor: \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (N, P).\n        \"\"\"\n        self.model.eval()\n        with torch.no_grad():\n            return self.model(X).T\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.__init__","title":"<code>__init__(model, group_indices, device=None)</code>","text":"<p>model: torch.nn.Module (\u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0438\u043b\u0438 \u043b\u044e\u0431\u0430\u044f torch-\u043c\u043e\u0434\u0435\u043b\u044c) group_indices: \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u043f\u0438\u0441\u043a\u043e\u0432 \u0438\u043d\u0434\u0435\u043a\u0441\u043e\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0433\u0440\u0443\u043f\u043f\u0430\u043c device: cpu/cuda</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>def __init__(self, model: nn.Module, group_indices: List[List[int]],\n             device: Optional[str] = None):\n    \"\"\"\n    model: torch.nn.Module (\u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u0438\u043b\u0438 \u043b\u044e\u0431\u0430\u044f torch-\u043c\u043e\u0434\u0435\u043b\u044c)\n    group_indices: \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u043f\u0438\u0441\u043a\u043e\u0432 \u0438\u043d\u0434\u0435\u043a\u0441\u043e\u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0433\u0440\u0443\u043f\u043f\u0430\u043c\n    device: cpu/cuda\n    \"\"\"\n    self.model = model\n    self.group_indices = group_indices\n    self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.model.to(self.device)\n    self._init_hyperparams()\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.compute_moments_VTV","title":"<code>compute_moments_VTV(M_V, Omega_V, S_V)</code>","text":"<p>\u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8V^T V\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b V. \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:     M_V (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 V.     Omega_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c V.     S_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c V. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:     Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8V^T V\u27e9.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>def compute_moments_VTV(self, M_V: Tensor, Omega_V: Tensor, S_V: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8V^T V\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b V.\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        M_V (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 V.\n        Omega_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c V.\n        S_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c V.\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n        Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8V^T V\u27e9.\n    \"\"\"\n    # \u041c\u043e\u043c\u0435\u043d\u0442: E[V V^T] = M_V M_V^T + tr(S_V) * Omega_V\n    return M_V.T @ M_V + torch.trace(S_V) * Omega_V\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.compute_moments_VVT","title":"<code>compute_moments_VVT(M_V, Omega_V, S_V)</code>","text":"<p>\u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8V V^T\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b V. \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:     M_V (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 V.     Omega_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c V.     S_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c V. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:     Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8V V^T\u27e9.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>def compute_moments_VVT(self, M_V: Tensor, Omega_V: Tensor, S_V: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8V V^T\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b V.\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        M_V (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 V.\n        Omega_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c V.\n        S_V (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c V.\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n        Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8V V^T\u27e9.\n    \"\"\"\n    # \u041c\u043e\u043c\u0435\u043d\u0442: E[V V^T] = M_V M_V^T + tr(S_V) * Omega_V\n    return M_V @ M_V.t() + torch.trace(Omega_V) * S_V\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.compute_moments_W","title":"<code>compute_moments_W(M_W, Omega_W, S_W)</code>","text":"<p>\u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8W W^T\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b W. \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:     M_W (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 W.     Omega_W (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c W.     S_W (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c W. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:     Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8W W^T\u27e9.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>def compute_moments_W(self, M_W: Tensor, Omega_W: Tensor, S_W: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8W W^T\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b W.\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        M_W (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 W.\n        Omega_W (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c W.\n        S_W (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c W.\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n        Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8W W^T\u27e9.\n    \"\"\"\n    # \u041c\u043e\u043c\u0435\u043d\u0442: E[W W^T] = M_W M_W^T + tr(S_W) * Omega_W\n    return M_W @ M_W.t() + torch.trace(S_W) * Omega_W\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.compute_moments_Z","title":"<code>compute_moments_Z(M_Z, Omega_Z, S_Z)</code>","text":"<p>\u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8Z Z^T\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b Z. \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:     M_Z (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 Z.     Omega_Z (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c Z.     S_Z (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c Z. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:     Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8Z Z^T\u27e9.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>def compute_moments_Z(self, M_Z: Tensor, Omega_Z: Tensor, S_Z: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u043e\u043c\u0435\u043d\u0442 \u27e8Z Z^T\u27e9 \u0434\u043b\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b Z.\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        M_Z (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 Z.\n        Omega_Z (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u0440\u043e\u043a\u0430\u043c Z.\n        S_Z (Tensor): \u041a\u043e\u0432\u0430\u0440\u0438\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c Z.\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n        Tensor: \u041c\u043e\u043c\u0435\u043d\u0442 \u27e8Z Z^T\u27e9.\n    \"\"\"\n    # \u041c\u043e\u043c\u0435\u043d\u0442: E[Z Z^T] = M_Z M_Z^T + tr(S_Z) * Omega_Z\n    return M_Z @ M_Z.t() + torch.trace(S_Z) * Omega_Z\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.d_log_bessel_k","title":"<code>d_log_bessel_k(omega, z)</code>  <code>staticmethod</code>","text":"<p>\u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0443\u044e \u043f\u043e omega \u043e\u0442 log K_omega(z). \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:     omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega.     z (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 z. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:     float: \u0417\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u043e\u0439.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>@staticmethod\ndef d_log_bessel_k(omega: float, z: float) -&gt; float:\n    \"\"\"\n    \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0443\u044e \u043f\u043e omega \u043e\u0442 log K_omega(z).\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega.\n        z (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 z.\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n        float: \u0417\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u043e\u0439.\n    \"\"\"\n    # \u041f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0430\u044f \u043f\u043e omega \u043e\u0442 log K_omega(z)\n    eps = 1e-5\n    return (np.log(kv(omega + eps, z)) - np.log(kv(omega - eps, z))) / (2 * eps)\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.e_step","title":"<code>e_step(X, Y)</code>","text":"<p>E-\u0448\u0430\u0433: \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0438\u0440\u0443\u0435\u0442 \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u043d\u044b\u0445 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0438 \u043c\u043e\u043c\u0435\u043d\u0442\u043e\u0432. \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:     X (Tensor): \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (D, N)     Y (Tensor): \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0442\u043a\u043b\u0438\u043a\u043e\u0432 (P, N) \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442: dict[str, Any] \u2014 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u043c\u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430\u043c\u0438 \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u0434\u043b\u044f M-\u0448\u0430\u0433a.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>def e_step(self, X: Tensor, Y: Tensor) -&gt; Dict[str, Any]:\n    \"\"\"\n    E-\u0448\u0430\u0433: \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0438\u0440\u0443\u0435\u0442 \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u043d\u044b\u0445 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0438 \u043c\u043e\u043c\u0435\u043d\u0442\u043e\u0432.\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        X (Tensor): \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (D, N)\n        Y (Tensor): \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0442\u043a\u043b\u0438\u043a\u043e\u0432 (P, N)\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442: dict[str, Any] \u2014 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u043c\u0438 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430\u043c\u0438 \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u0434\u043b\u044f M-\u0448\u0430\u0433a.\n    \"\"\"\n    group_moments = self._compute_group_moments()\n    self._update_posterior_matrices(X, Y, group_moments)\n    self._update_posterior_wishart(group_moments)\n    self._update_posterior_gig(group_moments)\n    return {\n        'mean_gammas': group_moments['mean_gammas'],\n        'mean_inv_gammas': group_moments['mean_inv_gammas'],\n        'mean_log_gammas': group_moments['mean_log_gammas'],\n        \"M_W\": self.M_W, \"Omega_W\": self.Omega_W, \"S_W\": self.S_W,\n        \"M_Z\": self.M_Z, \"Omega_Z\": self.Omega_Z, \"S_Z\": self.S_Z,\n        \"M_V\": self.M_V, \"Omega_V\": self.Omega_V, \"S_V\": self.S_V\n    }\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.fit","title":"<code>fit(X, Y, num_iter=10)</code>","text":"<p>\u041e\u0431\u0443\u0447\u0430\u0435\u0442 \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430.</p> \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b <p>X (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (N, D). Y (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0442\u043a\u043b\u0438\u043a\u043e\u0432 (N, P). num_iter (int): \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439 EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>def fit(self, X: Tensor, Y: Tensor, num_iter: int = 10) -&gt; None:\n    \"\"\"\n    \u041e\u0431\u0443\u0447\u0430\u0435\u0442 \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430.\n\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        X (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (N, D).\n        Y (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0442\u043a\u043b\u0438\u043a\u043e\u0432 (N, P).\n        num_iter (int): \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439 EM-\u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430.\n    \"\"\"\n    X = X.T\n    for _ in range(num_iter):\n        post = self.e_step(X, Y)\n        self.m_step(post)\n        self._set_flat_params(post[\"M_W\"])\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.m_step","title":"<code>m_step(post)</code>","text":"<p>M-\u0448\u0430\u0433: \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 E-\u0448\u0430\u0433a.</p> \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b <p>post (Dict[str, Any]): \u0421\u043b\u043e\u0432\u0430\u0440\u044c \u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438 E-\u0448\u0430\u0433a, \u0432\u043a\u043b\u044e\u0447\u0430\u044f \u043c\u043e\u043c\u0435\u043d\u0442\u044b \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>def m_step(self, post: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    M-\u0448\u0430\u0433: \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 E-\u0448\u0430\u0433a.\n\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        post (Dict[str, Any]): \u0421\u043b\u043e\u0432\u0430\u0440\u044c \u0441 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c\u0438 E-\u0448\u0430\u0433a, \u0432\u043a\u043b\u044e\u0447\u0430\u044f \u043c\u043e\u043c\u0435\u043d\u0442\u044b \u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043f\u043e\u0441\u0442\u0435\u0440\u0438\u043e\u0440\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f.\n    \"\"\"\n    # \u0422\u0435\u043f\u0435\u0440\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0431\u0449\u0435\u0433\u043e \u043f\u0440\u0430\u0439\u043e\u0440\u0430\n    self.update_gig_prior(post['mean_gammas'], post['mean_inv_gammas'], post['mean_log_gammas'])\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.mean_gig","title":"<code>mean_gig(omega, chi, phi)</code>  <code>staticmethod</code>","text":"<p>\u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8x\u27e9 \u0434\u043b\u044f GIG(omega, chi, phi). \u27e8x\u27e9 = sqrt(chi/phi) * R_omega(sqrt(chi*phi)) \u0433\u0434\u0435 R_omega(z) = K_{omega+1}(z) / K_{omega}(z) \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:     omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.     chi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 chi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.     phi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 phi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:     float: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8x\u27e9.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>@staticmethod\ndef mean_gig(omega: float, chi: float, phi: float) -&gt; float:\n    \"\"\"\n    \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8x\u27e9 \u0434\u043b\u044f GIG(omega, chi, phi).\n    \u27e8x\u27e9 = sqrt(chi/phi) * R_omega(sqrt(chi*phi))\n    \u0433\u0434\u0435 R_omega(z) = K_{omega+1}(z) / K_{omega}(z)\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n        chi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 chi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n        phi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 phi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n        float: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8x\u27e9.\n    \"\"\"\n    z = (chi * phi) ** 0.5\n    K_omega = kv(omega, z)\n    K_omega_p1 = kv(omega + 1, z)\n    R_omega = K_omega_p1 / K_omega if K_omega != 0 else 0.0\n    return (chi / phi) ** 0.5 * R_omega\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.mean_inv_gig","title":"<code>mean_inv_gig(omega, chi, phi)</code>  <code>staticmethod</code>","text":"<p>\u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0439 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u044b \u27e81/x\u27e9 \u0434\u043b\u044f GIG(omega, chi, phi). \u27e81/x\u27e9 = sqrt(chi/phi) * R_{omega-1}(sqrt(chi*phi)) \u0433\u0434\u0435 R_{omega-1}(z) = K_{omega}(z) / K_{omega-1}(z) \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:     omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.     chi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 chi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.     phi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 phi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:     float: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e81/x\u27e9.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>@staticmethod\ndef mean_inv_gig(omega: float, chi: float, phi: float) -&gt; float:\n    \"\"\"\n    \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0439 \u0432\u0435\u043b\u0438\u0447\u0438\u043d\u044b \u27e81/x\u27e9 \u0434\u043b\u044f GIG(omega, chi, phi).\n    \u27e81/x\u27e9 = sqrt(chi/phi) * R_{omega-1}(sqrt(chi*phi))\n    \u0433\u0434\u0435 R_{omega-1}(z) = K_{omega}(z) / K_{omega-1}(z)\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n        chi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 chi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n        phi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 phi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n        float: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e81/x\u27e9.\n    \"\"\"\n    z = (chi * phi) ** 0.5\n    K_omega = kv(omega, z)\n    K_omega_m1 = kv(omega - 1, z)\n    R_omega = K_omega_m1 / K_omega if K_omega != 0 else 0.0\n    return (phi / chi) ** 0.5 * R_omega\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.mean_log_gig","title":"<code>mean_log_gig(omega, chi, phi)</code>  <code>staticmethod</code>","text":"<p>\u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0430 \u27e8log(x)\u27e9 \u0434\u043b\u044f GIG(omega, chi, phi). \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:     omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.     chi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 chi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.     phi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 phi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:     float: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8log(x)\u27e9.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>@staticmethod\ndef mean_log_gig(omega: float, chi: float, phi: float) -&gt; float:\n    \"\"\"\n    \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0430 \u27e8log(x)\u27e9 \u0434\u043b\u044f GIG(omega, chi, phi).\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        omega (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 omega \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n        chi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 chi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n        phi (float): \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 phi \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f GIG.\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n        float: \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8log(x)\u27e9.\n    \"\"\"\n    z = (chi * phi) ** 0.5\n    return 0.5 * np.log(chi / phi) + (SparseBayesianRegression.d_log_bessel_k(omega, z))\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.predict","title":"<code>predict(X)</code>","text":"<p>\u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438.</p> \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b <p>X (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (N, D).</p> \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 <p>Tensor: \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (N, P).</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>def predict(self, X: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u0412\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438.\n\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        X (Tensor): \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (N, D).\n\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442:\n        Tensor: \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 (N, P).\n    \"\"\"\n    self.model.eval()\n    with torch.no_grad():\n        return self.model(X).T\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.update_gig_hyperparams","title":"<code>update_gig_hyperparams(group_idx, mean_gamma, mean_inv_gamma, mean_log_gamma)</code>","text":"<p>\u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b GIG (omega, chi, phi) \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b. \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:     group_idx (int): \u0418\u043d\u0434\u0435\u043a\u0441 \u0433\u0440\u0443\u043f\u043f\u044b.     mean_gamma (float): \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8gamma\u27e9.     mean_inv_gamma (float): \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e81/gamma\u27e9.     mean_log_gamma (float): \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8log(gamma)\u27e9.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>def update_gig_hyperparams(self, group_idx: int, mean_gamma: float, mean_inv_gamma: float, mean_log_gamma: float) -&gt; None:\n    \"\"\"\n    \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b GIG (omega, chi, phi) \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b.\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        group_idx (int): \u0418\u043d\u0434\u0435\u043a\u0441 \u0433\u0440\u0443\u043f\u043f\u044b.\n        mean_gamma (float): \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8gamma\u27e9.\n        mean_inv_gamma (float): \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e81/gamma\u27e9.\n        mean_log_gamma (float): \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0435 \u27e8log(gamma)\u27e9.\n    \"\"\"\n    # \u0427\u0438\u0441\u043b\u0435\u043d\u043d\u043e \u0440\u0435\u0448\u0430\u0435\u0442 \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u0434\u043b\u044f omega, chi, phi \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b\n    Q = 1  # \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u044b, \u0435\u0441\u043b\u0438 \u0433\u0440\u0443\u043f\u043f \u0431\u043e\u043b\u044c\u0448\u0435, \u043c\u043e\u0436\u043d\u043e \u043e\u0431\u043e\u0431\u0449\u0438\u0442\u044c\n    def equations(params):\n        omega, chi, phi = params\n        z = np.sqrt(chi * phi)\n        K_omega = kv(omega, z)\n        d_logK = self.d_log_bessel_k(omega, z)\n        R_omega = kv(omega + 1, z) / K_omega if K_omega != 0 else 0.0\n        eq1 = Q * np.log(np.sqrt(phi / chi)) - Q * d_logK - Q * mean_log_gamma\n        eq2 = (Q * omega) / chi - (Q / 2) * np.sqrt(phi / chi) * R_omega + 0.5 * mean_inv_gamma\n        eq3 = (Q / np.sqrt(chi * phi)) * R_omega - mean_gamma\n        return [eq1, eq2, eq3]\n    # \u041d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n    omega0 = float(self.omega_post[group_idx].cpu().numpy())\n    chi0 = float(self.chi_post[group_idx].cpu().numpy())\n    phi0 = float(self.phi_post[group_idx].cpu().numpy())\n    sol = root(equations, [omega0, chi0, phi0], method='hybr')\n    if sol.success:\n        self.omega_post[group_idx] = torch.tensor(sol.x[0], device=self.device)\n        self.chi_post[group_idx] = torch.tensor(sol.x[1], device=self.device)\n        self.phi_post[group_idx] = torch.tensor(sol.x[2], device=self.device)\n</code></pre>"},{"location":"sbmtl/reference/#bmm_multitask_learning.sbmtl.sparse_bayesian_regression.SparseBayesianRegression.update_gig_prior","title":"<code>update_gig_prior(mean_gammas, mean_inv_gammas, mean_log_gammas)</code>","text":"<p>\u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b GIG (omega, chi, phi) \u0434\u043b\u044f \u043e\u0431\u0449\u0435\u0433\u043e \u043f\u0440\u0430\u0439\u043e\u0440\u0430. \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:     mean_gammas (List[float]): \u0421\u043f\u0438\u0441\u043e\u043a \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0439 \u27e8gamma\u27e9 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f.     mean_inv_gammas (List[float]): \u0421\u043f\u0438\u0441\u043e\u043a \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0439 \u27e81/gamma\u27e9 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f.     mean_log_gammas (List[float]): \u0421\u043f\u0438\u0441\u043e\u043a \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0439 \u27e8log(gamma)\u27e9 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f.</p> Source code in <code>bmm_multitask_learning/sbmtl/sparse_bayesian_regression.py</code> <pre><code>def update_gig_prior(self, mean_gammas: List[float], mean_inv_gammas: List[float], mean_log_gammas: List[float]) -&gt; None:\n    \"\"\"\n    \u041e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u0442 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b GIG (omega, chi, phi) \u0434\u043b\u044f \u043e\u0431\u0449\u0435\u0433\u043e \u043f\u0440\u0430\u0439\u043e\u0440\u0430.\n    \u0410\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u044b:\n        mean_gammas (List[float]): \u0421\u043f\u0438\u0441\u043e\u043a \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0439 \u27e8gamma\u27e9 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f.\n        mean_inv_gammas (List[float]): \u0421\u043f\u0438\u0441\u043e\u043a \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0439 \u27e81/gamma\u27e9 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f.\n        mean_log_gammas (List[float]): \u0421\u043f\u0438\u0441\u043e\u043a \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0439 \u27e8log(gamma)\u27e9 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0433\u0440\u0443\u043f\u043f.\n    \"\"\"\n    # \u0427\u0438\u0441\u043b\u0435\u043d\u043d\u043e \u0440\u0435\u0448\u0430\u0435\u0442 \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u0434\u043b\u044f omega, chi, phi \u0434\u043b\u044f \u043e\u0431\u0449\u0435\u0433\u043e \u043f\u0440\u0430\u0439\u043e\u0440\u0430 (\u043f\u043e \u0441\u0440\u0435\u0434\u043d\u0438\u043c \u043f\u043e \u0433\u0440\u0443\u043f\u043f\u0430\u043c)\n    Q = len(mean_gammas)\n    sum_gamma = torch.sum(torch.tensor(mean_gammas))\n    sum_inv_gamma = torch.sum(torch.tensor(mean_inv_gammas))\n    sum_log_gamma = torch.sum(torch.tensor(mean_log_gammas))\n    def equations(params):\n        omega, chi, phi = params\n        z = np.sqrt(chi * phi)\n        K_omega = kv(omega, z)\n        d_logK = self.d_log_bessel_k(omega, z)\n        R_omega = kv(omega + 1, z) / K_omega if K_omega != 0 else 0.0\n        eq1 = Q * np.log(np.sqrt(phi / chi)) - Q * d_logK * sum_log_gamma\n        eq2 = (Q * omega) / chi - (Q / 2) * np.sqrt(phi / chi) * R_omega + 0.5 * sum_inv_gamma\n        eq3 = Q * np.sqrt(chi/ phi) * R_omega - sum_gamma\n        return [eq1, eq2, eq3]\n    omega0 = float(self.omega_prior.cpu().numpy())\n    chi0 = float(self.chi_prior.cpu().numpy())\n    phi0 = float(self.phi_prior.cpu().numpy())\n    sol = root(equations, [omega0, chi0, phi0], method='hybr')\n    if sol.success:\n        self.omega_prior = torch.tensor(sol.x[0], device=self.device)\n        self.chi_prior = torch.tensor(sol.x[1], device=self.device)\n        self.phi_prior = torch.tensor(sol.x[2], device=self.device)\n</code></pre>"},{"location":"task_clustering/hooks/","title":"Hooks","text":"In\u00a0[\u00a0]: Copied! <pre>import shutil\n</pre> import shutil In\u00a0[\u00a0]: Copied! <pre>def on_pre_build(*args, **kwargs):\n    shutil.copy2(\n        \"examples/task_clustering/multi-task-learning.ipynb\", \n        \"docs/task_clustering/multi-task-learning.ipynb\"\n    )\n</pre> def on_pre_build(*args, **kwargs):     shutil.copy2(         \"examples/task_clustering/multi-task-learning.ipynb\",          \"docs/task_clustering/multi-task-learning.ipynb\"     )"},{"location":"task_clustering/multi-task-learning/","title":"Neural Network Model","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nnp.random.seed(42)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.special import softmax\n</pre> import numpy as np np.random.seed(42)  import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error, r2_score from sklearn.preprocessing import StandardScaler from scipy.special import softmax In\u00a0[2]: Copied! <pre>import os, sys\n#sys.path.append(\"..\")\nsys.path.insert(1, \"/kaggle/input/multitask\")\n</pre> import os, sys #sys.path.append(\"..\") sys.path.insert(1, \"/kaggle/input/multitask\") In\u00a0[3]: Copied! <pre>n_input = 5\nn_hidden = 4  \nn_tasks = 10\nn_clusters = 2\nn_features = 3\nn_samples = 200\n</pre> n_input = 5 n_hidden = 4   n_tasks = 10 n_clusters = 2 n_features = 3 n_samples = 200 In\u00a0[4]: Copied! <pre>from MultiTask_Algo import MultiTaskNN\n</pre> from MultiTask_Algo import MultiTaskNN In\u00a0[5]: Copied! <pre>def generate_synthetic_data(n_tasks=10, n_samples_train=50, n_samples_test=300,\n                            n_input=5, n_hidden=4, activation='tanh'):\n    # True parameters\n    true_W = np.random.randn(n_hidden, n_input + 1)\n    true_Sigma = np.eye(n_hidden + 1) * 0.5\n    true_m = np.array([0.1, 1.5, -1.5, 2, 4])\n\n    # Generate data for each task\n    train_data = []\n    test_data = []\n\n    for i in range(n_tasks):\n        # Generate covariates\n        X_train = np.random.randn(n_samples_train, n_input)\n        X_test = np.random.randn(n_samples_test, n_input)\n\n        # Scale per task to zero mean and unit variance\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n\n        # Compute hidden activations\n        X_train_bias = np.hstack([X_train, np.ones((n_samples_train, 1))])\n        X_test_bias = np.hstack([X_test, np.ones((n_samples_test, 1))])\n\n        if activation == 'tanh':\n            h_train = np.tanh(np.dot(X_train_bias, true_W.T))\n            h_test = np.tanh(np.dot(X_test_bias, true_W.T))\n        else:\n            h_train = np.dot(X_train_bias, true_W.T)\n            h_test = np.dot(X_test_bias, true_W.T)\n\n        # Add bias term\n        h_train = np.hstack([h_train, np.ones((n_samples_train, 1))])\n        h_test = np.hstack([h_test, np.ones((n_samples_test, 1))])\n\n        # Generate task-specific weights from true distribution\n        A = np.random.multivariate_normal(true_m, true_Sigma)\n\n        # Generate responses with noise\n        y_train = np.dot(h_train, A) + np.random.randn(n_samples_train) * 0.1\n        y_test = np.dot(h_test, A) + np.random.randn(n_samples_test) * 0.1\n\n        train_data.append((X_train, y_train))\n        test_data.append((X_test, y_test))\n\n    return train_data, test_data\n</pre> def generate_synthetic_data(n_tasks=10, n_samples_train=50, n_samples_test=300,                             n_input=5, n_hidden=4, activation='tanh'):     # True parameters     true_W = np.random.randn(n_hidden, n_input + 1)     true_Sigma = np.eye(n_hidden + 1) * 0.5     true_m = np.array([0.1, 1.5, -1.5, 2, 4])      # Generate data for each task     train_data = []     test_data = []      for i in range(n_tasks):         # Generate covariates         X_train = np.random.randn(n_samples_train, n_input)         X_test = np.random.randn(n_samples_test, n_input)          # Scale per task to zero mean and unit variance         scaler = StandardScaler()         X_train = scaler.fit_transform(X_train)         X_test = scaler.transform(X_test)          # Compute hidden activations         X_train_bias = np.hstack([X_train, np.ones((n_samples_train, 1))])         X_test_bias = np.hstack([X_test, np.ones((n_samples_test, 1))])          if activation == 'tanh':             h_train = np.tanh(np.dot(X_train_bias, true_W.T))             h_test = np.tanh(np.dot(X_test_bias, true_W.T))         else:             h_train = np.dot(X_train_bias, true_W.T)             h_test = np.dot(X_test_bias, true_W.T)          # Add bias term         h_train = np.hstack([h_train, np.ones((n_samples_train, 1))])         h_test = np.hstack([h_test, np.ones((n_samples_test, 1))])          # Generate task-specific weights from true distribution         A = np.random.multivariate_normal(true_m, true_Sigma)          # Generate responses with noise         y_train = np.dot(h_train, A) + np.random.randn(n_samples_train) * 0.1         y_test = np.dot(h_test, A) + np.random.randn(n_samples_test) * 0.1          train_data.append((X_train, y_train))         test_data.append((X_test, y_test))      return train_data, test_data In\u00a0[6]: Copied! <pre>def evaluate_model(model, test_data):\n    mse = 0\n    for i, (X_test, y_test) in enumerate(test_data):\n        y_pred = model.predict(X_test, i)\n        mse += np.mean((y_test - y_pred) ** 2)\n    mse /= len(test_data)\n\n    return {'test_mse': mse}\n</pre> def evaluate_model(model, test_data):     mse = 0     for i, (X_test, y_test) in enumerate(test_data):         y_pred = model.predict(X_test, i)         mse += np.mean((y_test - y_pred) ** 2)     mse /= len(test_data)      return {'test_mse': mse} In\u00a0[7]: Copied! <pre>def plot_training_results(model, X_list, y_list):\n    plt.figure(figsize=(18, 12))\n    \n    # 1. \u0413\u0440\u0430\u0444\u0438\u043a \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 vs \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 (\u043f\u0435\u0440\u0432\u044b\u0435 3 \u0437\u0430\u0434\u0430\u0447\u0438)\n    plt.subplot(2, 3, 1)\n    \n    global_min = min(np.min(y) for y in y_list)\n    global_max = max(np.max(y) for y in y_list)\n\n    for i in range(min(3, len(X_list))):\n        y_pred = model.predict(X_list[i], i)\n        plt.scatter(y_list[i], y_pred, alpha=0.6, label=f'Task {i+1}')\n    plt.plot([global_min, global_max], [global_min, global_max], 'k--', label='Ideal')\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.title('Predictions vs True Values (Train)')\n    plt.legend()\n\n    # 2. MSE \u043f\u043e \u0437\u0430\u0434\u0430\u0447\u0430\u043c\n    #plt.subplot(2, 3, 2)\n    #train_mses = []\n    #for i, (X, y) in enumerate(zip(X_list, y_list)):\n    #    y_pred = model.predict(X, i)\n    #    train_mses.append(mean_squared_error(y, y_pred))\n    #plt.bar(range(1, len(train_mses)+1), train_mses)\n    #plt.xlabel('Task')\n    #plt.ylabel('MSE')\n    #plt.title('Train MSE per Task')\n    \n    # 3. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 W\n    #plt.subplot(2, 3, 3)\n    #plt.imshow(model.W, aspect='auto', cmap='coolwarm')\n    #plt.colorbar()\n    #plt.title('Shared Weights W')\n    \n    # 4. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 A (\u043f\u0435\u0440\u0432\u044b\u0435 10 \u0437\u0430\u0434\u0430\u0447)\n    #plt.subplot(2, 3, 4)\n    #A_matrix = np.array(model.A_map[:10])  # \u041f\u043e\u043a\u0430\u0436\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0432\u044b\u0435 10 \u0437\u0430\u0434\u0430\u0447\n    #plt.imshow(A_matrix, aspect='auto', cmap='coolwarm')\n    #plt.colorbar()\n    #plt.title('Task-specific Weights A (first 10 tasks)')\n    \n    plt.tight_layout()\n    plt.show()\n</pre> def plot_training_results(model, X_list, y_list):     plt.figure(figsize=(18, 12))          # 1. \u0413\u0440\u0430\u0444\u0438\u043a \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 vs \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 (\u043f\u0435\u0440\u0432\u044b\u0435 3 \u0437\u0430\u0434\u0430\u0447\u0438)     plt.subplot(2, 3, 1)          global_min = min(np.min(y) for y in y_list)     global_max = max(np.max(y) for y in y_list)      for i in range(min(3, len(X_list))):         y_pred = model.predict(X_list[i], i)         plt.scatter(y_list[i], y_pred, alpha=0.6, label=f'Task {i+1}')     plt.plot([global_min, global_max], [global_min, global_max], 'k--', label='Ideal')     plt.xlabel('True Values')     plt.ylabel('Predictions')     plt.title('Predictions vs True Values (Train)')     plt.legend()      # 2. MSE \u043f\u043e \u0437\u0430\u0434\u0430\u0447\u0430\u043c     #plt.subplot(2, 3, 2)     #train_mses = []     #for i, (X, y) in enumerate(zip(X_list, y_list)):     #    y_pred = model.predict(X, i)     #    train_mses.append(mean_squared_error(y, y_pred))     #plt.bar(range(1, len(train_mses)+1), train_mses)     #plt.xlabel('Task')     #plt.ylabel('MSE')     #plt.title('Train MSE per Task')          # 3. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 W     #plt.subplot(2, 3, 3)     #plt.imshow(model.W, aspect='auto', cmap='coolwarm')     #plt.colorbar()     #plt.title('Shared Weights W')          # 4. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 A (\u043f\u0435\u0440\u0432\u044b\u0435 10 \u0437\u0430\u0434\u0430\u0447)     #plt.subplot(2, 3, 4)     #A_matrix = np.array(model.A_map[:10])  # \u041f\u043e\u043a\u0430\u0436\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0432\u044b\u0435 10 \u0437\u0430\u0434\u0430\u0447     #plt.imshow(A_matrix, aspect='auto', cmap='coolwarm')     #plt.colorbar()     #plt.title('Task-specific Weights A (first 10 tasks)')          plt.tight_layout()     plt.show() In\u00a0[8]: Copied! <pre>def run_simulations(n_simulations=1, activation='tanh', with_plot=False,\n                    n_tasks=10, n_input=5, n_hidden=4):\n    results = []\n    successful_simulations = 0\n\n    for sim in range(n_simulations):\n        print(f\"\\nSimulation {sim + 1}/{n_simulations}\")\n\n        try:\n            # Generate data\n            train_data, test_data = generate_synthetic_data(\n                n_tasks=n_tasks, n_input=n_input,\n                n_hidden=n_hidden, activation=activation\n            )\n\n            # Initialize and fit model\n            model = MultiTaskNN(\n                n_input=n_input,\n                n_hidden=n_hidden,\n                n_tasks=n_tasks,\n                activation=activation\n            )\n\n            X_list = [X for X, y in train_data]\n            y_list = [y for X, y in train_data]\n\n            # Fit model with error handling\n            fit_result = model.fit(X_list, y_list, max_iter=100)\n            if fit_result is None:\n                print(\"Skipping simulation due to fitting error\")\n                continue\n\n            # Evaluate\n            metrics = evaluate_model(model, test_data)\n            results.append(metrics)\n            successful_simulations += 1\n\n            print(f\"Test MSE: {metrics['test_mse']:.4f}\")\n\n            if with_plot:\n                plot_training_results(model, X_list, y_list)\n\n        except Exception as e:\n            print(f\"Error in simulation {sim + 1}: {str(e)}\")\n            continue\n\n    if successful_simulations == 0:\n        print(\"Warning: All simulations failed\")\n        return None\n\n    # Aggregate results\n    avg_results = {\n        'avg_test_mse': np.mean([r['test_mse'] for r in results]),\n        'success_rate': successful_simulations / n_simulations\n    }\n\n    return avg_results\n</pre> def run_simulations(n_simulations=1, activation='tanh', with_plot=False,                     n_tasks=10, n_input=5, n_hidden=4):     results = []     successful_simulations = 0      for sim in range(n_simulations):         print(f\"\\nSimulation {sim + 1}/{n_simulations}\")          try:             # Generate data             train_data, test_data = generate_synthetic_data(                 n_tasks=n_tasks, n_input=n_input,                 n_hidden=n_hidden, activation=activation             )              # Initialize and fit model             model = MultiTaskNN(                 n_input=n_input,                 n_hidden=n_hidden,                 n_tasks=n_tasks,                 activation=activation             )              X_list = [X for X, y in train_data]             y_list = [y for X, y in train_data]              # Fit model with error handling             fit_result = model.fit(X_list, y_list, max_iter=100)             if fit_result is None:                 print(\"Skipping simulation due to fitting error\")                 continue              # Evaluate             metrics = evaluate_model(model, test_data)             results.append(metrics)             successful_simulations += 1              print(f\"Test MSE: {metrics['test_mse']:.4f}\")              if with_plot:                 plot_training_results(model, X_list, y_list)          except Exception as e:             print(f\"Error in simulation {sim + 1}: {str(e)}\")             continue      if successful_simulations == 0:         print(\"Warning: All simulations failed\")         return None      # Aggregate results     avg_results = {         'avg_test_mse': np.mean([r['test_mse'] for r in results]),         'success_rate': successful_simulations / n_simulations     }      return avg_results In\u00a0[9]: Copied! <pre>tanh_results = run_simulations(n_simulations=1, activation='tanh', with_plot=True)\nprint(f\"Tanh results: {tanh_results}\")\n</pre> tanh_results = run_simulations(n_simulations=1, activation='tanh', with_plot=True) print(f\"Tanh results: {tanh_results}\") <pre>\nSimulation 1/1\nTest MSE: 9.5748\n</pre> <pre>Tanh results: {'avg_test_mse': 9.574803376357206, 'success_rate': 1.0}\n</pre> In\u00a0[10]: Copied! <pre>linear_results = run_simulations(n_simulations=1, activation='linear', with_plot=True)\nprint(f\"linear results: {linear_results}\")\n</pre> linear_results = run_simulations(n_simulations=1, activation='linear', with_plot=True) print(f\"linear results: {linear_results}\") <pre>\nSimulation 1/1\nTest MSE: 104.7930\n</pre> <pre>linear results: {'avg_test_mse': 104.79300970318968, 'success_rate': 1.0}\n</pre> In\u00a0[11]: Copied! <pre>from MultiTask_Algo import MultiTaskNNDependentMean\n</pre> from MultiTask_Algo import MultiTaskNNDependentMean In\u00a0[12]: Copied! <pre>def generate_synthetic_data(n_tasks=10, n_samples_train=50, n_samples_test=300,\n                            n_input=5, n_hidden=4, n_features=3, activation='tanh'):\n    # True parameters\n    true_W = np.random.randn(n_hidden, n_input + 1)\n    true_Sigma = np.eye(n_hidden + 1) * 0.5\n    true_M = np.random.randn(n_hidden + 1, n_features) * 0.5\n\n    # Generate random features for each task\n    task_features = np.random.randn(n_tasks, n_features)\n\n    # Generate data for each task\n    train_data = []\n    test_data = []\n\n    for i in range(n_tasks):\n        # Generate covariates\n        X_train = np.random.randn(n_samples_train, n_input)\n        X_test = np.random.randn(n_samples_test, n_input)\n\n        # Scale per task to zero mean and unit variance\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n\n        # Compute hidden activations\n        X_train_bias = np.hstack([X_train, np.ones((n_samples_train, 1))])\n        X_test_bias = np.hstack([X_test, np.ones((n_samples_test, 1))])\n\n        if activation == 'tanh':\n            h_train = np.tanh(np.dot(X_train_bias, true_W.T))\n            h_test = np.tanh(np.dot(X_test_bias, true_W.T))\n        else:\n            h_train = np.dot(X_train_bias, true_W.T)\n            h_test = np.dot(X_test_bias, true_W.T)\n\n        # Add bias term\n        h_train = np.hstack([h_train, np.ones((n_samples_train, 1))])\n        h_test = np.hstack([h_test, np.ones((n_samples_test, 1))])\n\n        # Compute task-specific mean\n        m_i = np.dot(true_M, task_features[i])\n\n        # Generate task-specific weights from distribution with task-dependent mean\n        A = np.random.multivariate_normal(m_i, true_Sigma)\n\n        # Generate responses with noise\n        y_train = np.dot(h_train, A) + np.random.randn(n_samples_train) * 0.1\n        y_test = np.dot(h_test, A) + np.random.randn(n_samples_test) * 0.1\n\n        train_data.append((X_train, y_train))\n        test_data.append((X_test, y_test))\n\n    return train_data, test_data, task_features\n</pre> def generate_synthetic_data(n_tasks=10, n_samples_train=50, n_samples_test=300,                             n_input=5, n_hidden=4, n_features=3, activation='tanh'):     # True parameters     true_W = np.random.randn(n_hidden, n_input + 1)     true_Sigma = np.eye(n_hidden + 1) * 0.5     true_M = np.random.randn(n_hidden + 1, n_features) * 0.5      # Generate random features for each task     task_features = np.random.randn(n_tasks, n_features)      # Generate data for each task     train_data = []     test_data = []      for i in range(n_tasks):         # Generate covariates         X_train = np.random.randn(n_samples_train, n_input)         X_test = np.random.randn(n_samples_test, n_input)          # Scale per task to zero mean and unit variance         scaler = StandardScaler()         X_train = scaler.fit_transform(X_train)         X_test = scaler.transform(X_test)          # Compute hidden activations         X_train_bias = np.hstack([X_train, np.ones((n_samples_train, 1))])         X_test_bias = np.hstack([X_test, np.ones((n_samples_test, 1))])          if activation == 'tanh':             h_train = np.tanh(np.dot(X_train_bias, true_W.T))             h_test = np.tanh(np.dot(X_test_bias, true_W.T))         else:             h_train = np.dot(X_train_bias, true_W.T)             h_test = np.dot(X_test_bias, true_W.T)          # Add bias term         h_train = np.hstack([h_train, np.ones((n_samples_train, 1))])         h_test = np.hstack([h_test, np.ones((n_samples_test, 1))])          # Compute task-specific mean         m_i = np.dot(true_M, task_features[i])          # Generate task-specific weights from distribution with task-dependent mean         A = np.random.multivariate_normal(m_i, true_Sigma)          # Generate responses with noise         y_train = np.dot(h_train, A) + np.random.randn(n_samples_train) * 0.1         y_test = np.dot(h_test, A) + np.random.randn(n_samples_test) * 0.1          train_data.append((X_train, y_train))         test_data.append((X_test, y_test))      return train_data, test_data, task_features In\u00a0[13]: Copied! <pre>def evaluate_model(model, test_data, task_features):\n    mse = 0\n    for i, (X_test, y_test) in enumerate(test_data):\n        y_pred = model.predict(X_test, i)\n        mse += np.mean((y_test - y_pred) ** 2)\n    mse /= len(test_data)\n\n    return {\n        'test_mse': mse,\n        'task_features': task_features\n    }\n</pre> def evaluate_model(model, test_data, task_features):     mse = 0     for i, (X_test, y_test) in enumerate(test_data):         y_pred = model.predict(X_test, i)         mse += np.mean((y_test - y_pred) ** 2)     mse /= len(test_data)      return {         'test_mse': mse,         'task_features': task_features     } In\u00a0[14]: Copied! <pre>def plot_training_results(model, X_list, y_list, task_features):\n    plt.figure(figsize=(20, 12))\n    \n    # 1. \u0413\u0440\u0430\u0444\u0438\u043a \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 vs \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 (\u043f\u0435\u0440\u0432\u044b\u0435 3 \u0437\u0430\u0434\u0430\u0447\u0438)\n    plt.subplot(2, 3, 1)\n    \n    global_min = min(np.min(y) for y in y_list)\n    global_max = max(np.max(y) for y in y_list)\n\n    for i in range(min(3, len(X_list))):\n        y_pred = model.predict(X_list[i], i)\n        plt.scatter(y_list[i], y_pred, alpha=0.6, label=f'Task {i+1}')\n    plt.plot([global_min, global_max], [global_min, global_max], 'k--', label='Ideal')\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.title('Predictions vs True Values (Train)')\n    plt.legend()\n\n    # 2. MSE \u043f\u043e \u0437\u0430\u0434\u0430\u0447\u0430\u043c\n    #plt.subplot(2, 3, 2)\n    #train_mses = []\n    #for i, (X, y) in enumerate(zip(X_list, y_list)):\n    #    y_pred = model.predict(X, i)\n    #    train_mses.append(mean_squared_error(y, y_pred))\n    #plt.bar(range(1, len(train_mses)+1), train_mses)\n    #plt.xlabel('Task')\n    #plt.ylabel('MSE')\n    #plt.title('Train MSE per Task')\n    \n    # 3. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 W\n    #plt.subplot(2, 3, 3)\n    #plt.imshow(model.W, aspect='auto', cmap='coolwarm')\n    #plt.colorbar()\n    #plt.title('Shared Weights W')\n\n    # 4. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 A (\u043f\u0435\u0440\u0432\u044b\u0435 10 \u0437\u0430\u0434\u0430\u0447)\n    #plt.subplot(2, 3, 4)\n    #A_matrix = np.array(model.A_map[:10])  # \u041f\u043e\u043a\u0430\u0436\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0432\u044b\u0435 10 \u0437\u0430\u0434\u0430\u0447\n    #plt.imshow(A_matrix, aspect='auto', cmap='coolwarm')\n    #plt.colorbar()\n    #plt.title('Task-specific Weights A (first 10 tasks)')\n    \n    # 5. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b M\n    #plt.subplot(2, 3, 5)\n    #plt.imshow(model.M, aspect='auto', cmap='coolwarm')\n    #plt.colorbar()\n    #plt.title('Feature Weight Matrix M')\n\n    # 6. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u0432\u044f\u0437\u0435\u0439 \u043c\u0435\u0436\u0434\u0443 features \u0438 \u0432\u0435\u0441\u0430\u043c\u0438\n    #plt.subplot(2, 3, 6)\n    #for i in range(min(10, len(model.A_map))):\n    #    # \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0441\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0435\u0439\n    #    if len(task_features[i]) == len(model.A_map[i]):\n    #        plt.scatter(task_features[i], model.A_map[i], alpha=0.6, label=f'Task {i+1}')\n    #    else:\n    #        # \u0415\u0441\u043b\u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u043d\u0435 \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u044e\u0442, \u0441\u0442\u0440\u043e\u0438\u043c \u0434\u043b\u044f \u043f\u0435\u0440\u0432\u044b\u0445 n_features \u0432\u0435\u0441\u043e\u0432\n    #        n = min(len(task_features[i]), len(model.A_map[i]))\n    #        plt.scatter(task_features[i][:n], model.A_map[i][:n], alpha=0.6, label=f'Task {i+1}')\n    #plt.xlabel('Task Features')\n    #plt.ylabel('A weights')\n    #plt.title('Task Features vs A Weights')\n    #plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n</pre> def plot_training_results(model, X_list, y_list, task_features):     plt.figure(figsize=(20, 12))          # 1. \u0413\u0440\u0430\u0444\u0438\u043a \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 vs \u0438\u0441\u0442\u0438\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 (\u043f\u0435\u0440\u0432\u044b\u0435 3 \u0437\u0430\u0434\u0430\u0447\u0438)     plt.subplot(2, 3, 1)          global_min = min(np.min(y) for y in y_list)     global_max = max(np.max(y) for y in y_list)      for i in range(min(3, len(X_list))):         y_pred = model.predict(X_list[i], i)         plt.scatter(y_list[i], y_pred, alpha=0.6, label=f'Task {i+1}')     plt.plot([global_min, global_max], [global_min, global_max], 'k--', label='Ideal')     plt.xlabel('True Values')     plt.ylabel('Predictions')     plt.title('Predictions vs True Values (Train)')     plt.legend()      # 2. MSE \u043f\u043e \u0437\u0430\u0434\u0430\u0447\u0430\u043c     #plt.subplot(2, 3, 2)     #train_mses = []     #for i, (X, y) in enumerate(zip(X_list, y_list)):     #    y_pred = model.predict(X, i)     #    train_mses.append(mean_squared_error(y, y_pred))     #plt.bar(range(1, len(train_mses)+1), train_mses)     #plt.xlabel('Task')     #plt.ylabel('MSE')     #plt.title('Train MSE per Task')          # 3. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 W     #plt.subplot(2, 3, 3)     #plt.imshow(model.W, aspect='auto', cmap='coolwarm')     #plt.colorbar()     #plt.title('Shared Weights W')      # 4. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 A (\u043f\u0435\u0440\u0432\u044b\u0435 10 \u0437\u0430\u0434\u0430\u0447)     #plt.subplot(2, 3, 4)     #A_matrix = np.array(model.A_map[:10])  # \u041f\u043e\u043a\u0430\u0436\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0432\u044b\u0435 10 \u0437\u0430\u0434\u0430\u0447     #plt.imshow(A_matrix, aspect='auto', cmap='coolwarm')     #plt.colorbar()     #plt.title('Task-specific Weights A (first 10 tasks)')          # 5. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u044b M     #plt.subplot(2, 3, 5)     #plt.imshow(model.M, aspect='auto', cmap='coolwarm')     #plt.colorbar()     #plt.title('Feature Weight Matrix M')      # 6. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0441\u0432\u044f\u0437\u0435\u0439 \u043c\u0435\u0436\u0434\u0443 features \u0438 \u0432\u0435\u0441\u0430\u043c\u0438     #plt.subplot(2, 3, 6)     #for i in range(min(10, len(model.A_map))):     #    # \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0441\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0435\u0439     #    if len(task_features[i]) == len(model.A_map[i]):     #        plt.scatter(task_features[i], model.A_map[i], alpha=0.6, label=f'Task {i+1}')     #    else:     #        # \u0415\u0441\u043b\u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u043d\u0435 \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u044e\u0442, \u0441\u0442\u0440\u043e\u0438\u043c \u0434\u043b\u044f \u043f\u0435\u0440\u0432\u044b\u0445 n_features \u0432\u0435\u0441\u043e\u0432     #        n = min(len(task_features[i]), len(model.A_map[i]))     #        plt.scatter(task_features[i][:n], model.A_map[i][:n], alpha=0.6, label=f'Task {i+1}')     #plt.xlabel('Task Features')     #plt.ylabel('A weights')     #plt.title('Task Features vs A Weights')     #plt.legend()          plt.tight_layout()     plt.show() In\u00a0[15]: Copied! <pre>def run_simulations(n_simulations=1, activation='tanh', with_plot=False, \n                   n_tasks=10, n_input=5, n_hidden=4, n_features=3):\n    results = []\n    successful_simulations = 0\n    \n    for sim in range(n_simulations):\n        print(f\"\\nSimulation {sim+1}/{n_simulations}\")\n        \n        try:\n            # Generate data with task features\n            train_data, test_data, task_features = generate_synthetic_data(\n                n_tasks=n_tasks, n_input=n_input, n_hidden=n_hidden,\n                n_features=n_features, activation=activation\n            )\n\n            # Initialize and fit model\n            model = MultiTaskNNDependentMean(\n                n_input=n_input,\n                n_hidden=n_hidden,\n                n_tasks=n_tasks,\n                n_features=n_features,\n                activation=activation\n            )\n\n            X_list = [X for X, y in train_data]\n            y_list = [y for X, y in train_data]\n            \n            # Fit model with error handling\n            fit_result = model.fit(X_list, y_list, task_features, max_iter=100)\n            if fit_result is None:\n                print(\"Skipping simulation due to fitting error\")\n                continue\n\n            # Evaluate\n            metrics = evaluate_model(model, test_data, task_features)\n            results.append(metrics)\n            successful_simulations += 1\n            \n            print(f\"Test MSE: {metrics['test_mse']:.4f}\")\n\n            if with_plot:\n                try:\n                    plot_training_results(model, X_list, y_list, task_features)\n                except Exception as e:\n                    print(f\"Plotting error: {str(e)}\")\n                    continue\n            \n        except Exception as e:\n            print(f\"Error in simulation {sim+1}: {str(e)}\")\n            continue\n    \n    if successful_simulations == 0:\n        print(\"Warning: All simulations failed\")\n        return None\n\n    # Aggregate results\n    avg_results = {\n        'avg_test_mse': np.mean([r['test_mse'] for r in results]),\n        'success_rate': successful_simulations / n_simulations\n    }\n    \n    return avg_results\n</pre> def run_simulations(n_simulations=1, activation='tanh', with_plot=False,                     n_tasks=10, n_input=5, n_hidden=4, n_features=3):     results = []     successful_simulations = 0          for sim in range(n_simulations):         print(f\"\\nSimulation {sim+1}/{n_simulations}\")                  try:             # Generate data with task features             train_data, test_data, task_features = generate_synthetic_data(                 n_tasks=n_tasks, n_input=n_input, n_hidden=n_hidden,                 n_features=n_features, activation=activation             )              # Initialize and fit model             model = MultiTaskNNDependentMean(                 n_input=n_input,                 n_hidden=n_hidden,                 n_tasks=n_tasks,                 n_features=n_features,                 activation=activation             )              X_list = [X for X, y in train_data]             y_list = [y for X, y in train_data]                          # Fit model with error handling             fit_result = model.fit(X_list, y_list, task_features, max_iter=100)             if fit_result is None:                 print(\"Skipping simulation due to fitting error\")                 continue              # Evaluate             metrics = evaluate_model(model, test_data, task_features)             results.append(metrics)             successful_simulations += 1                          print(f\"Test MSE: {metrics['test_mse']:.4f}\")              if with_plot:                 try:                     plot_training_results(model, X_list, y_list, task_features)                 except Exception as e:                     print(f\"Plotting error: {str(e)}\")                     continue                      except Exception as e:             print(f\"Error in simulation {sim+1}: {str(e)}\")             continue          if successful_simulations == 0:         print(\"Warning: All simulations failed\")         return None      # Aggregate results     avg_results = {         'avg_test_mse': np.mean([r['test_mse'] for r in results]),         'success_rate': successful_simulations / n_simulations     }          return avg_results In\u00a0[16]: Copied! <pre># \u0417\u0430\u043f\u0443\u0441\u043a \u0441\u0438\u043c\u0443\u043b\u044f\u0446\u0438\u0438 \u0441 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0445 \u043e\u0448\u0438\u0431\u043e\u043a\ntry:\n    dependent_mean_results = run_simulations(\n        n_simulations=1, \n        activation='tanh', \n        with_plot=True,\n        n_tasks=10,\n        n_input=5,\n        n_hidden=4,\n        n_features=3\n    )\n    print(f\"Dependent Mean tanh results: {dependent_mean_results}\")\nexcept Exception as e:\n    print(f\"Simulation failed with error: {str(e)}\")\n</pre> # \u0417\u0430\u043f\u0443\u0441\u043a \u0441\u0438\u043c\u0443\u043b\u044f\u0446\u0438\u0438 \u0441 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0445 \u043e\u0448\u0438\u0431\u043e\u043a try:     dependent_mean_results = run_simulations(         n_simulations=1,          activation='tanh',          with_plot=True,         n_tasks=10,         n_input=5,         n_hidden=4,         n_features=3     )     print(f\"Dependent Mean tanh results: {dependent_mean_results}\") except Exception as e:     print(f\"Simulation failed with error: {str(e)}\") <pre>\nSimulation 1/1\nTest MSE: 4.4064\n</pre> <pre>Dependent Mean tanh results: {'avg_test_mse': 4.406377505021862, 'success_rate': 1.0}\n</pre> In\u00a0[17]: Copied! <pre># \u0417\u0430\u043f\u0443\u0441\u043a \u0441\u0438\u043c\u0443\u043b\u044f\u0446\u0438\u0438 \u0441 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0445 \u043e\u0448\u0438\u0431\u043e\u043a\ntry:\n    dependent_mean_results = run_simulations(\n        n_simulations=1, \n        activation='linear', \n        with_plot=True,\n        n_tasks=10,\n        n_input=5,\n        n_hidden=4,\n        n_features=3\n    )\n    print(f\"Dependent Mean linear results: {dependent_mean_results}\")\nexcept Exception as e:\n    print(f\"Simulation failed with error: {str(e)}\")\n</pre> # \u0417\u0430\u043f\u0443\u0441\u043a \u0441\u0438\u043c\u0443\u043b\u044f\u0446\u0438\u0438 \u0441 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0445 \u043e\u0448\u0438\u0431\u043e\u043a try:     dependent_mean_results = run_simulations(         n_simulations=1,          activation='linear',          with_plot=True,         n_tasks=10,         n_input=5,         n_hidden=4,         n_features=3     )     print(f\"Dependent Mean linear results: {dependent_mean_results}\") except Exception as e:     print(f\"Simulation failed with error: {str(e)}\") <pre>\nSimulation 1/1\n</pre> <pre>/kaggle/input/multitask/MultiTask_Algo.py:228: RuntimeWarning: overflow encountered in exp\n  sigma = np.exp(log_sigma)\n/usr/local/lib/python3.11/dist-packages/scipy/optimize/_numdiff.py:596: RuntimeWarning: invalid value encountered in subtract\n  df = fun(x1) - f0\n</pre> <pre>Test MSE: 28.4530\n</pre> <pre>Dependent Mean linear results: {'avg_test_mse': 28.45303892302573, 'success_rate': 1.0}\n</pre> In\u00a0[18]: Copied! <pre>from MultiTask_Algo import MultiTaskNNClustering\n</pre> from MultiTask_Algo import MultiTaskNNClustering In\u00a0[19]: Copied! <pre>def generate_clustered_data(n_tasks=10, n_samples_train=50, n_samples_test=300,\n                            n_input=5, n_hidden=4, n_clusters=2, activation='tanh'):\n    # True parameters with clear cluster separation\n    true_W = np.random.randn(n_hidden, n_input + 1) * 0.5\n\n    # Create distinct cluster centers\n    cluster_centers = np.random.randn(n_clusters, n_hidden + 1) * 2\n    for i in range(n_clusters):\n        cluster_centers[i] += i * 3  # Separate clusters\n\n    # Assign tasks to clusters\n    true_z = np.zeros((n_tasks, n_clusters))\n    for i in range(n_tasks):\n        true_z[i, i % n_clusters] = 1\n\n    # Generate data for each task\n    train_data = []\n    test_data = []\n\n    for i in range(n_tasks):\n        # Generate covariates\n        X_train = np.random.randn(n_samples_train, n_input)\n        X_test = np.random.randn(n_samples_test, n_input)\n\n        # Scale per task\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n\n        # Compute hidden activations\n        X_train_bias = np.hstack([X_train, np.ones((n_samples_train, 1))])\n        X_test_bias = np.hstack([X_test, np.ones((n_samples_test, 1))])\n\n        if activation == 'tanh':\n            h_train = np.tanh(np.dot(X_train_bias, true_W.T))\n            h_test = np.tanh(np.dot(X_test_bias, true_W.T))\n        else:\n            h_train = np.dot(X_train_bias, true_W.T)\n            h_test = np.dot(X_test_bias, true_W.T)\n\n        # Add bias term\n        h_train = np.hstack([h_train, np.ones((n_samples_train, 1))])\n        h_test = np.hstack([h_test, np.ones((n_samples_test, 1))])\n\n        # Get cluster for this task\n        cluster_idx = np.argmax(true_z[i])\n\n        # Generate task weights from cluster distribution\n        A = np.random.multivariate_normal(\n            cluster_centers[cluster_idx],\n            np.eye(n_hidden + 1) * 0.1\n        )\n\n        # Generate responses\n        y_train = np.dot(h_train, A) + np.random.randn(n_samples_train) * 0.1\n        y_test = np.dot(h_test, A) + np.random.randn(n_samples_test) * 0.1\n\n        train_data.append((X_train, y_train))\n        test_data.append((X_test, y_test))\n\n    return train_data, test_data, true_z\n</pre> def generate_clustered_data(n_tasks=10, n_samples_train=50, n_samples_test=300,                             n_input=5, n_hidden=4, n_clusters=2, activation='tanh'):     # True parameters with clear cluster separation     true_W = np.random.randn(n_hidden, n_input + 1) * 0.5      # Create distinct cluster centers     cluster_centers = np.random.randn(n_clusters, n_hidden + 1) * 2     for i in range(n_clusters):         cluster_centers[i] += i * 3  # Separate clusters      # Assign tasks to clusters     true_z = np.zeros((n_tasks, n_clusters))     for i in range(n_tasks):         true_z[i, i % n_clusters] = 1      # Generate data for each task     train_data = []     test_data = []      for i in range(n_tasks):         # Generate covariates         X_train = np.random.randn(n_samples_train, n_input)         X_test = np.random.randn(n_samples_test, n_input)          # Scale per task         scaler = StandardScaler()         X_train = scaler.fit_transform(X_train)         X_test = scaler.transform(X_test)          # Compute hidden activations         X_train_bias = np.hstack([X_train, np.ones((n_samples_train, 1))])         X_test_bias = np.hstack([X_test, np.ones((n_samples_test, 1))])          if activation == 'tanh':             h_train = np.tanh(np.dot(X_train_bias, true_W.T))             h_test = np.tanh(np.dot(X_test_bias, true_W.T))         else:             h_train = np.dot(X_train_bias, true_W.T)             h_test = np.dot(X_test_bias, true_W.T)          # Add bias term         h_train = np.hstack([h_train, np.ones((n_samples_train, 1))])         h_test = np.hstack([h_test, np.ones((n_samples_test, 1))])          # Get cluster for this task         cluster_idx = np.argmax(true_z[i])          # Generate task weights from cluster distribution         A = np.random.multivariate_normal(             cluster_centers[cluster_idx],             np.eye(n_hidden + 1) * 0.1         )          # Generate responses         y_train = np.dot(h_train, A) + np.random.randn(n_samples_train) * 0.1         y_test = np.dot(h_test, A) + np.random.randn(n_samples_test) * 0.1          train_data.append((X_train, y_train))         test_data.append((X_test, y_test))      return train_data, test_data, true_z In\u00a0[20]: Copied! <pre>def evaluate_clustering_model(model, test_data):\n    mse = 0\n    for i, (X_test, y_test) in enumerate(test_data):\n        y_pred = model.predict(X_test, i)\n        mse += np.mean((y_test - y_pred) ** 2)\n    mse /= len(test_data)\n\n    return {\n        'test_mse': mse,\n        'cluster_assignments': model.get_cluster_assignments(),\n        'task_similarity': model.get_task_similarity()\n    }\n</pre> def evaluate_clustering_model(model, test_data):     mse = 0     for i, (X_test, y_test) in enumerate(test_data):         y_pred = model.predict(X_test, i)         mse += np.mean((y_test - y_pred) ** 2)     mse /= len(test_data)      return {         'test_mse': mse,         'cluster_assignments': model.get_cluster_assignments(),         'task_similarity': model.get_task_similarity()     } In\u00a0[21]: Copied! <pre>def plot_clustering_results(model, X_list, y_list):\n    plt.figure(figsize=(20, 15))\n\n    # 1. Predictions vs True (first 3 tasks)\n    plt.subplot(3, 3, 1)\n    global_min = min(np.min(y) for y in y_list)\n    global_max = max(np.max(y) for y in y_list)\n\n    for i in range(min(3, len(X_list))):\n        y_pred = model.predict(X_list[i], i)\n        plt.scatter(y_list[i], y_pred, alpha=0.6, label=f'Task {i + 1}')\n    plt.plot([global_min, global_max], [global_min, global_max], 'k--')\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.title('Predictions vs True Values')\n    plt.legend()\n\n    # 2. Cluster assignments\n    #plt.subplot(3, 3, 2)\n    #assignments = model.get_cluster_assignments()\n    #plt.hist(assignments, bins=model.n_clusters, rwidth=0.8)\n    #plt.xlabel('Cluster')\n    #plt.ylabel('Number of Tasks')\n    #plt.title('Task Cluster Assignments')\n\n    # 3. Task similarity matrix\n    #plt.subplot(3, 3, 3)\n    #similarity = model.get_task_similarity()\n    #plt.imshow(similarity, cmap='Blues', interpolation='nearest')\n    #plt.colorbar()\n    #plt.title('Task Similarity Matrix')\n\n    # 4. Cluster centers visualization\n    #plt.subplot(3, 3, 4)\n    #for cluster in range(model.n_clusters):\n    #    plt.plot(model.m[cluster], label=f'Cluster {cluster + 1}')\n    #plt.xlabel('Weight Index')\n    #plt.ylabel('Value')\n    #plt.title('Cluster Centers (m)')\n    #plt.legend()\n\n    # 5. Cluster probabilities (q)\n    #plt.subplot(3, 3, 5)\n    #plt.bar(range(model.n_clusters), model.q)\n    #plt.xlabel('Cluster')\n    #plt.ylabel('Probability')\n    #plt.title('Cluster Probabilities (q)')\n\n    # 6. First two dimensions of A weights colored by cluster\n    #plt.subplot(3, 3, 6)\n    #colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k'][:model.n_clusters]\n    #for i, A in enumerate(model.A_map):\n    #    cluster = assignments[i]\n    #    plt.scatter(A[0], A[1], color=colors[cluster], alpha=0.6)\n    #plt.xlabel('A[0]')\n    #plt.ylabel('A[1]')\n    #plt.title('A Weights Colored by Cluster')\n\n    # 7. Shared weights W\n    #plt.subplot(3, 3, 7)\n    #plt.imshow(model.W, aspect='auto', cmap='coolwarm')\n    #plt.colorbar()\n    #plt.title('Shared Weights W')\n\n    # 8. Cluster responsibilities (z)\n    #plt.subplot(3, 3, 8)\n    #plt.imshow(model.z.T, aspect='auto', cmap='viridis')\n    #plt.colorbar()\n    #plt.xlabel('Task')\n    #plt.ylabel('Cluster')\n    #plt.title('Cluster Responsibilities')\n\n    plt.tight_layout()\n    plt.show()\n</pre> def plot_clustering_results(model, X_list, y_list):     plt.figure(figsize=(20, 15))      # 1. Predictions vs True (first 3 tasks)     plt.subplot(3, 3, 1)     global_min = min(np.min(y) for y in y_list)     global_max = max(np.max(y) for y in y_list)      for i in range(min(3, len(X_list))):         y_pred = model.predict(X_list[i], i)         plt.scatter(y_list[i], y_pred, alpha=0.6, label=f'Task {i + 1}')     plt.plot([global_min, global_max], [global_min, global_max], 'k--')     plt.xlabel('True Values')     plt.ylabel('Predictions')     plt.title('Predictions vs True Values')     plt.legend()      # 2. Cluster assignments     #plt.subplot(3, 3, 2)     #assignments = model.get_cluster_assignments()     #plt.hist(assignments, bins=model.n_clusters, rwidth=0.8)     #plt.xlabel('Cluster')     #plt.ylabel('Number of Tasks')     #plt.title('Task Cluster Assignments')      # 3. Task similarity matrix     #plt.subplot(3, 3, 3)     #similarity = model.get_task_similarity()     #plt.imshow(similarity, cmap='Blues', interpolation='nearest')     #plt.colorbar()     #plt.title('Task Similarity Matrix')      # 4. Cluster centers visualization     #plt.subplot(3, 3, 4)     #for cluster in range(model.n_clusters):     #    plt.plot(model.m[cluster], label=f'Cluster {cluster + 1}')     #plt.xlabel('Weight Index')     #plt.ylabel('Value')     #plt.title('Cluster Centers (m)')     #plt.legend()      # 5. Cluster probabilities (q)     #plt.subplot(3, 3, 5)     #plt.bar(range(model.n_clusters), model.q)     #plt.xlabel('Cluster')     #plt.ylabel('Probability')     #plt.title('Cluster Probabilities (q)')      # 6. First two dimensions of A weights colored by cluster     #plt.subplot(3, 3, 6)     #colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k'][:model.n_clusters]     #for i, A in enumerate(model.A_map):     #    cluster = assignments[i]     #    plt.scatter(A[0], A[1], color=colors[cluster], alpha=0.6)     #plt.xlabel('A[0]')     #plt.ylabel('A[1]')     #plt.title('A Weights Colored by Cluster')      # 7. Shared weights W     #plt.subplot(3, 3, 7)     #plt.imshow(model.W, aspect='auto', cmap='coolwarm')     #plt.colorbar()     #plt.title('Shared Weights W')      # 8. Cluster responsibilities (z)     #plt.subplot(3, 3, 8)     #plt.imshow(model.z.T, aspect='auto', cmap='viridis')     #plt.colorbar()     #plt.xlabel('Task')     #plt.ylabel('Cluster')     #plt.title('Cluster Responsibilities')      plt.tight_layout()     plt.show() In\u00a0[22]: Copied! <pre>def run_clustering_simulation(n_simulations=1, activation='tanh', with_plot=False,\n                              n_tasks=10, n_input=5, n_hidden=4, n_clusters=2):\n    results = []\n\n    for sim in range(n_simulations):\n        print(f\"\\nSimulation {sim + 1}/{n_simulations}\")\n\n        try:\n            # Generate clustered data\n            train_data, test_data, true_z = generate_clustered_data(\n                n_tasks=n_tasks, n_input=n_input, n_hidden=n_hidden,\n                n_clusters=n_clusters, activation=activation\n            )\n\n            # Initialize model\n            model = MultiTaskNNClustering(\n                n_input=n_input,\n                n_hidden=n_hidden,\n                n_tasks=n_tasks,\n                n_clusters=n_clusters,\n                activation=activation\n            )\n\n            X_list = [X for X, y in train_data]\n            y_list = [y for X, y in train_data]\n\n            # Fit model\n            model.fit(train_data, max_iter=100)\n\n            # Evaluate\n            metrics = evaluate_clustering_model(model, test_data)\n            results.append(metrics)\n\n            print(f\"Test MSE: {metrics['test_mse']:.4f}\")\n            #print(f\"Cluster assignments: {metrics['cluster_assignments']}\")\n\n            if with_plot:\n                plot_clustering_results(model, X_list, y_list)\n\n        except Exception as e:\n            print(f\"Error in simulation {sim + 1}: {str(e)}\")\n            continue\n\n    if not results:\n        print(\"Warning: All simulations failed\")\n        return None\n\n    return {\n        'avg_test_mse': np.mean([r['test_mse'] for r in results]),\n        'cluster_consistency': np.mean([\n            np.mean(r['cluster_assignments'] == np.array([i % n_clusters for i in range(n_tasks)]))\n            for r in results\n        ])\n    }\n</pre> def run_clustering_simulation(n_simulations=1, activation='tanh', with_plot=False,                               n_tasks=10, n_input=5, n_hidden=4, n_clusters=2):     results = []      for sim in range(n_simulations):         print(f\"\\nSimulation {sim + 1}/{n_simulations}\")          try:             # Generate clustered data             train_data, test_data, true_z = generate_clustered_data(                 n_tasks=n_tasks, n_input=n_input, n_hidden=n_hidden,                 n_clusters=n_clusters, activation=activation             )              # Initialize model             model = MultiTaskNNClustering(                 n_input=n_input,                 n_hidden=n_hidden,                 n_tasks=n_tasks,                 n_clusters=n_clusters,                 activation=activation             )              X_list = [X for X, y in train_data]             y_list = [y for X, y in train_data]              # Fit model             model.fit(train_data, max_iter=100)              # Evaluate             metrics = evaluate_clustering_model(model, test_data)             results.append(metrics)              print(f\"Test MSE: {metrics['test_mse']:.4f}\")             #print(f\"Cluster assignments: {metrics['cluster_assignments']}\")              if with_plot:                 plot_clustering_results(model, X_list, y_list)          except Exception as e:             print(f\"Error in simulation {sim + 1}: {str(e)}\")             continue      if not results:         print(\"Warning: All simulations failed\")         return None      return {         'avg_test_mse': np.mean([r['test_mse'] for r in results]),         'cluster_consistency': np.mean([             np.mean(r['cluster_assignments'] == np.array([i % n_clusters for i in range(n_tasks)]))             for r in results         ])     } In\u00a0[23]: Copied! <pre># Run simulation with visualization\nclustering_results = run_clustering_simulation(\n    n_simulations=1,\n    activation='tanh',\n    with_plot=True,\n    n_tasks=10,\n    n_input=5,\n    n_hidden=4,\n    n_clusters=2\n)\n</pre> # Run simulation with visualization clustering_results = run_clustering_simulation(     n_simulations=1,     activation='tanh',     with_plot=True,     n_tasks=10,     n_input=5,     n_hidden=4,     n_clusters=2 ) <pre>\nSimulation 1/1\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [02:29&lt;00:00,  1.49s/it]\n</pre> <pre>Test MSE: 0.0490\n</pre> In\u00a0[24]: Copied! <pre>print(\"\\nFinal Results tanh:\")\nprint(f\"Average Test MSE: {clustering_results['avg_test_mse']:.4f}\")\n#print(f\"Cluster Consistency: {clustering_results['cluster_consistency']:.2%}\")\n</pre> print(\"\\nFinal Results tanh:\") print(f\"Average Test MSE: {clustering_results['avg_test_mse']:.4f}\") #print(f\"Cluster Consistency: {clustering_results['cluster_consistency']:.2%}\") <pre>\nFinal Results tanh:\nAverage Test MSE: 0.0490\n</pre> In\u00a0[25]: Copied! <pre># Run simulation with visualization\nclustering_results = run_clustering_simulation(\n    n_simulations=1,\n    activation='linear',\n    with_plot=True,\n    n_tasks=10,\n    n_input=5,\n    n_hidden=4,\n    n_clusters=2\n)\n</pre> # Run simulation with visualization clustering_results = run_clustering_simulation(     n_simulations=1,     activation='linear',     with_plot=True,     n_tasks=10,     n_input=5,     n_hidden=4,     n_clusters=2 ) <pre>\nSimulation 1/1\n</pre> <pre> 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 95/100 [01:12&lt;00:03,  1.30it/s]</pre> <pre>Converged at iteration 95\nTest MSE: 0.0114\n</pre> <pre>\n</pre> In\u00a0[26]: Copied! <pre>print(\"\\nFinal Results linear:\")\nprint(f\"Average Test MSE: {clustering_results['avg_test_mse']:.4f}\")\n#print(f\"Cluster Consistency: {clustering_results['cluster_consistency']:.2%}\")\n</pre> print(\"\\nFinal Results linear:\") print(f\"Average Test MSE: {clustering_results['avg_test_mse']:.4f}\") #print(f\"Cluster Consistency: {clustering_results['cluster_consistency']:.2%}\") <pre>\nFinal Results linear:\nAverage Test MSE: 0.0114\n</pre> In\u00a0[27]: Copied! <pre>from MultiTask_Algo import MultiTaskNNGating\n</pre> from MultiTask_Algo import MultiTaskNNGating In\u00a0[28]: Copied! <pre>def generate_gating_data(n_tasks=10, n_samples_train=50, n_samples_test=300,\n                         n_input=5, n_hidden=4, n_clusters=2, n_features=3, activation='tanh'):\n    # True parameters with clear cluster separation\n    true_W = np.random.randn(n_hidden, n_input + 1) * 0.5\n\n    # Create distinct cluster centers\n    cluster_centers = np.random.randn(n_clusters, n_hidden + 1) * 2\n    for i in range(n_clusters):\n        cluster_centers[i] += i * 3  # Separate clusters\n\n    # Generate task features that correlate with cluster assignments\n    true_U = np.random.randn(n_clusters, n_features) * 1.5\n    task_features = np.random.randn(n_tasks, n_features)\n\n    # Assign tasks to clusters based on features\n    logits = np.dot(task_features, true_U.T)\n    true_z = softmax(logits, axis=1)\n\n    # Generate data for each task\n    train_data = []\n    test_data = []\n\n    for i in range(n_tasks):\n        # Generate covariates\n        X_train = np.random.randn(n_samples_train, n_input)\n        X_test = np.random.randn(n_samples_test, n_input)\n\n        # Scale per task\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n\n        # Compute hidden activations\n        X_train_bias = np.hstack([X_train, np.ones((n_samples_train, 1))])\n        X_test_bias = np.hstack([X_test, np.ones((n_samples_test, 1))])\n\n        if activation == 'tanh':\n            h_train = np.tanh(np.dot(X_train_bias, true_W.T))\n            h_test = np.tanh(np.dot(X_test_bias, true_W.T))\n        else:\n            h_train = np.dot(X_train_bias, true_W.T)\n            h_test = np.dot(X_test_bias, true_W.T)\n\n        # Add bias term\n        h_train = np.hstack([h_train, np.ones((n_samples_train, 1))])\n        h_test = np.hstack([h_test, np.ones((n_samples_test, 1))])\n\n        # Get cluster for this task\n        cluster_idx = np.argmax(true_z[i])\n\n        # Generate task weights from cluster distribution\n        A = np.random.multivariate_normal(\n            cluster_centers[cluster_idx],\n            np.eye(n_hidden + 1) * 0.1\n        )\n\n        # Generate responses\n        y_train = np.dot(h_train, A) + np.random.randn(n_samples_train) * 0.1\n        y_test = np.dot(h_test, A) + np.random.randn(n_samples_test) * 0.1\n\n        train_data.append((X_train, y_train))\n        test_data.append((X_test, y_test))\n\n    return train_data, test_data, task_features, true_z\n</pre> def generate_gating_data(n_tasks=10, n_samples_train=50, n_samples_test=300,                          n_input=5, n_hidden=4, n_clusters=2, n_features=3, activation='tanh'):     # True parameters with clear cluster separation     true_W = np.random.randn(n_hidden, n_input + 1) * 0.5      # Create distinct cluster centers     cluster_centers = np.random.randn(n_clusters, n_hidden + 1) * 2     for i in range(n_clusters):         cluster_centers[i] += i * 3  # Separate clusters      # Generate task features that correlate with cluster assignments     true_U = np.random.randn(n_clusters, n_features) * 1.5     task_features = np.random.randn(n_tasks, n_features)      # Assign tasks to clusters based on features     logits = np.dot(task_features, true_U.T)     true_z = softmax(logits, axis=1)      # Generate data for each task     train_data = []     test_data = []      for i in range(n_tasks):         # Generate covariates         X_train = np.random.randn(n_samples_train, n_input)         X_test = np.random.randn(n_samples_test, n_input)          # Scale per task         scaler = StandardScaler()         X_train = scaler.fit_transform(X_train)         X_test = scaler.transform(X_test)          # Compute hidden activations         X_train_bias = np.hstack([X_train, np.ones((n_samples_train, 1))])         X_test_bias = np.hstack([X_test, np.ones((n_samples_test, 1))])          if activation == 'tanh':             h_train = np.tanh(np.dot(X_train_bias, true_W.T))             h_test = np.tanh(np.dot(X_test_bias, true_W.T))         else:             h_train = np.dot(X_train_bias, true_W.T)             h_test = np.dot(X_test_bias, true_W.T)          # Add bias term         h_train = np.hstack([h_train, np.ones((n_samples_train, 1))])         h_test = np.hstack([h_test, np.ones((n_samples_test, 1))])          # Get cluster for this task         cluster_idx = np.argmax(true_z[i])          # Generate task weights from cluster distribution         A = np.random.multivariate_normal(             cluster_centers[cluster_idx],             np.eye(n_hidden + 1) * 0.1         )          # Generate responses         y_train = np.dot(h_train, A) + np.random.randn(n_samples_train) * 0.1         y_test = np.dot(h_test, A) + np.random.randn(n_samples_test) * 0.1          train_data.append((X_train, y_train))         test_data.append((X_test, y_test))      return train_data, test_data, task_features, true_z In\u00a0[29]: Copied! <pre>def evaluate_gating_model(model, test_data, true_z):\n    mse = 0\n    for i, (X_test, y_test) in enumerate(test_data):\n        y_pred = model.predict(X_test, i)\n        mse += np.mean((y_test - y_pred) ** 2)\n    mse /= len(test_data)\n\n    # Compute clustering accuracy\n    true_assignments = np.argmax(true_z, axis=1)\n    pred_assignments = model.get_cluster_assignments()\n    clustering_acc = np.mean(true_assignments == pred_assignments)\n\n    return {\n        'test_mse': mse,\n        'clustering_accuracy': clustering_acc,\n        'cluster_assignments': pred_assignments,\n        'task_similarity': model.get_task_similarity(),\n        'true_z': true_z,\n        'pred_z': model.z\n    }\n</pre> def evaluate_gating_model(model, test_data, true_z):     mse = 0     for i, (X_test, y_test) in enumerate(test_data):         y_pred = model.predict(X_test, i)         mse += np.mean((y_test - y_pred) ** 2)     mse /= len(test_data)      # Compute clustering accuracy     true_assignments = np.argmax(true_z, axis=1)     pred_assignments = model.get_cluster_assignments()     clustering_acc = np.mean(true_assignments == pred_assignments)      return {         'test_mse': mse,         'clustering_accuracy': clustering_acc,         'cluster_assignments': pred_assignments,         'task_similarity': model.get_task_similarity(),         'true_z': true_z,         'pred_z': model.z     } In\u00a0[30]: Copied! <pre>def plot_gating_results(model, X_list, y_list, task_features, true_z):\n    plt.figure(figsize=(24, 16))\n\n    # 1. Predictions vs True (first 3 tasks)\n    plt.subplot(3, 4, 1)\n    global_min = min(np.min(y) for y in y_list)\n    global_max = max(np.max(y) for y in y_list)\n\n    for i in range(min(3, len(X_list))):\n        y_pred = model.predict(X_list[i], i)\n        plt.scatter(y_list[i], y_pred, alpha=0.6, label=f'Task {i + 1}')\n    plt.plot([global_min, global_max], [global_min, global_max], 'k--')\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.title('Predictions vs True Values')\n    plt.legend()\n\n    # 2. True vs Predicted cluster assignments\n    # plt.subplot(3, 4, 2)\n    # true_assign = np.argmax(true_z, axis=1)\n    # pred_assign = model.get_cluster_assignments()\n    # plt.scatter(true_assign, pred_assign, alpha=0.6)\n    # plt.xlabel('True Cluster')\n    # plt.ylabel('Predicted Cluster')\n    # plt.title('Cluster Assignment Accuracy')\n\n    # 3. Gating network weights (U)\n    # plt.subplot(3, 4, 3)\n    # plt.imshow(model.U, aspect='auto', cmap='coolwarm')\n    # plt.colorbar()\n    # plt.title('Gating Network Weights (U)')\n\n    # 4. Cluster centers visualization\n    # plt.subplot(3, 4, 4)\n    # for cluster in range(model.n_clusters):\n    #    plt.plot(model.m[cluster], label=f'Cluster {cluster+1}')\n    # plt.xlabel('Weight Index')\n    # plt.ylabel('Value')\n    # plt.title('Cluster Centers (m)')\n    # plt.legend()\n\n    # 5. True vs Predicted responsibilities\n    # plt.subplot(3, 4, 5)\n    # plt.scatter(true_z[:,0], model.z[:,0], alpha=0.6)\n    # plt.xlabel('True Responsibility Cluster 0')\n    # plt.ylabel('Predicted Responsibility Cluster 0')\n    # plt.title('Cluster Responsibilities Comparison')\n\n    # 6. Task features colored by true cluster\n    # plt.subplot(3, 4, 6)\n    # true_assign = np.argmax(true_z, axis=1)\n    # for i in range(model.n_clusters):\n    #    mask = true_assign == i\n    #    plt.scatter(task_features[mask,0], task_features[mask,1],\n    #               label=f'True Cluster {i}', alpha=0.6)\n    # plt.xlabel('Feature 1')\n    # plt.ylabel('Feature 2')\n    # plt.title('Task Features (True Clusters)')\n    # plt.legend()\n\n    # 7. Task features colored by predicted cluster\n    # plt.subplot(3, 4, 7)\n    # pred_assign = model.get_cluster_assignments()\n    # for i in range(model.n_clusters):\n    #    mask = pred_assign == i\n    #    plt.scatter(task_features[mask,0], task_features[mask,1],\n    #               label=f'Pred Cluster {i}', alpha=0.6)\n    # plt.xlabel('Feature 1')\n    # plt.ylabel('Feature 2')\n    # plt.title('Task Features (Predicted Clusters)')\n    # plt.legend()\n\n    # 8. Shared weights W\n    # plt.subplot(3, 4, 8)\n    # plt.imshow(model.W, aspect='auto', cmap='coolwarm')\n    # plt.colorbar()\n    # plt.title('Shared Weights W')\n\n    # 9. Cluster responsibilities (z)\n    # plt.subplot(3, 4, 9)\n    # plt.imshow(model.z.T, aspect='auto', cmap='viridis')\n    # plt.colorbar()\n    # plt.xlabel('Task')\n    # plt.ylabel('Cluster')\n    # plt.title('Cluster Responsibilities (z)')\n\n    # 10. True responsibilities\n    # plt.subplot(3, 4, 10)\n    # plt.imshow(true_z.T, aspect='auto', cmap='viridis')\n    # plt.colorbar()\n    # plt.xlabel('Task')\n    # plt.ylabel('Cluster')\n    # plt.title('True Responsibilities')\n\n    plt.tight_layout()\n    plt.show()\n</pre> def plot_gating_results(model, X_list, y_list, task_features, true_z):     plt.figure(figsize=(24, 16))      # 1. Predictions vs True (first 3 tasks)     plt.subplot(3, 4, 1)     global_min = min(np.min(y) for y in y_list)     global_max = max(np.max(y) for y in y_list)      for i in range(min(3, len(X_list))):         y_pred = model.predict(X_list[i], i)         plt.scatter(y_list[i], y_pred, alpha=0.6, label=f'Task {i + 1}')     plt.plot([global_min, global_max], [global_min, global_max], 'k--')     plt.xlabel('True Values')     plt.ylabel('Predictions')     plt.title('Predictions vs True Values')     plt.legend()      # 2. True vs Predicted cluster assignments     # plt.subplot(3, 4, 2)     # true_assign = np.argmax(true_z, axis=1)     # pred_assign = model.get_cluster_assignments()     # plt.scatter(true_assign, pred_assign, alpha=0.6)     # plt.xlabel('True Cluster')     # plt.ylabel('Predicted Cluster')     # plt.title('Cluster Assignment Accuracy')      # 3. Gating network weights (U)     # plt.subplot(3, 4, 3)     # plt.imshow(model.U, aspect='auto', cmap='coolwarm')     # plt.colorbar()     # plt.title('Gating Network Weights (U)')      # 4. Cluster centers visualization     # plt.subplot(3, 4, 4)     # for cluster in range(model.n_clusters):     #    plt.plot(model.m[cluster], label=f'Cluster {cluster+1}')     # plt.xlabel('Weight Index')     # plt.ylabel('Value')     # plt.title('Cluster Centers (m)')     # plt.legend()      # 5. True vs Predicted responsibilities     # plt.subplot(3, 4, 5)     # plt.scatter(true_z[:,0], model.z[:,0], alpha=0.6)     # plt.xlabel('True Responsibility Cluster 0')     # plt.ylabel('Predicted Responsibility Cluster 0')     # plt.title('Cluster Responsibilities Comparison')      # 6. Task features colored by true cluster     # plt.subplot(3, 4, 6)     # true_assign = np.argmax(true_z, axis=1)     # for i in range(model.n_clusters):     #    mask = true_assign == i     #    plt.scatter(task_features[mask,0], task_features[mask,1],     #               label=f'True Cluster {i}', alpha=0.6)     # plt.xlabel('Feature 1')     # plt.ylabel('Feature 2')     # plt.title('Task Features (True Clusters)')     # plt.legend()      # 7. Task features colored by predicted cluster     # plt.subplot(3, 4, 7)     # pred_assign = model.get_cluster_assignments()     # for i in range(model.n_clusters):     #    mask = pred_assign == i     #    plt.scatter(task_features[mask,0], task_features[mask,1],     #               label=f'Pred Cluster {i}', alpha=0.6)     # plt.xlabel('Feature 1')     # plt.ylabel('Feature 2')     # plt.title('Task Features (Predicted Clusters)')     # plt.legend()      # 8. Shared weights W     # plt.subplot(3, 4, 8)     # plt.imshow(model.W, aspect='auto', cmap='coolwarm')     # plt.colorbar()     # plt.title('Shared Weights W')      # 9. Cluster responsibilities (z)     # plt.subplot(3, 4, 9)     # plt.imshow(model.z.T, aspect='auto', cmap='viridis')     # plt.colorbar()     # plt.xlabel('Task')     # plt.ylabel('Cluster')     # plt.title('Cluster Responsibilities (z)')      # 10. True responsibilities     # plt.subplot(3, 4, 10)     # plt.imshow(true_z.T, aspect='auto', cmap='viridis')     # plt.colorbar()     # plt.xlabel('Task')     # plt.ylabel('Cluster')     # plt.title('True Responsibilities')      plt.tight_layout()     plt.show() In\u00a0[31]: Copied! <pre>def run_gating_simulation(n_simulations=1, activation='tanh', with_plot=False,\n                          n_tasks=10, n_input=5, n_hidden=4,\n                          n_clusters=2, n_features=3):\n    results = []\n\n    for sim in range(n_simulations):\n        print(f\"\\nSimulation {sim + 1}/{n_simulations}\")\n\n        try:\n            # Generate data with task features\n            train_data, test_data, task_features, true_z = generate_gating_data(\n                n_tasks=n_tasks, n_input=n_input, n_hidden=n_hidden,\n                n_clusters=n_clusters, n_features=n_features, activation=activation\n            )\n\n            # Initialize model\n            model = MultiTaskNNGating(\n                n_input=n_input,\n                n_hidden=n_hidden,\n                n_tasks=n_tasks,\n                n_clusters=n_clusters,\n                n_features=n_features,\n                activation=activation\n            )\n\n            X_list = [X for X, y in train_data]\n            y_list = [y for X, y in train_data]\n\n            # Fit model\n            model.fit(train_data, task_features, max_iter=100)\n\n            # Evaluate\n            metrics = evaluate_gating_model(model, test_data, true_z)\n            results.append(metrics)\n\n            print(f\"Test MSE: {metrics['test_mse']:.4f}\")\n            # print(f\"Clustering Accuracy: {metrics['clustering_accuracy']:.2%}\")\n\n            if with_plot:\n                plot_gating_results(model, X_list, y_list, task_features, true_z)\n\n        except Exception as e:\n            print(f\"Error in simulation {sim + 1}: {str(e)}\")\n            continue\n\n    if not results:\n        print(\"Warning: All simulations failed\")\n        return None\n\n    return {\n        'avg_test_mse': np.mean([r['test_mse'] for r in results]),\n        'avg_clustering_acc': np.mean([r['clustering_accuracy'] for r in results])\n    }\n</pre> def run_gating_simulation(n_simulations=1, activation='tanh', with_plot=False,                           n_tasks=10, n_input=5, n_hidden=4,                           n_clusters=2, n_features=3):     results = []      for sim in range(n_simulations):         print(f\"\\nSimulation {sim + 1}/{n_simulations}\")          try:             # Generate data with task features             train_data, test_data, task_features, true_z = generate_gating_data(                 n_tasks=n_tasks, n_input=n_input, n_hidden=n_hidden,                 n_clusters=n_clusters, n_features=n_features, activation=activation             )              # Initialize model             model = MultiTaskNNGating(                 n_input=n_input,                 n_hidden=n_hidden,                 n_tasks=n_tasks,                 n_clusters=n_clusters,                 n_features=n_features,                 activation=activation             )              X_list = [X for X, y in train_data]             y_list = [y for X, y in train_data]              # Fit model             model.fit(train_data, task_features, max_iter=100)              # Evaluate             metrics = evaluate_gating_model(model, test_data, true_z)             results.append(metrics)              print(f\"Test MSE: {metrics['test_mse']:.4f}\")             # print(f\"Clustering Accuracy: {metrics['clustering_accuracy']:.2%}\")              if with_plot:                 plot_gating_results(model, X_list, y_list, task_features, true_z)          except Exception as e:             print(f\"Error in simulation {sim + 1}: {str(e)}\")             continue      if not results:         print(\"Warning: All simulations failed\")         return None      return {         'avg_test_mse': np.mean([r['test_mse'] for r in results]),         'avg_clustering_acc': np.mean([r['clustering_accuracy'] for r in results])     } In\u00a0[32]: Copied! <pre># Run simulation with visualization\ngating_results = run_gating_simulation(\n    n_simulations=1,\n    activation='tanh',\n    with_plot=True,\n    n_tasks=10,\n    n_input=5,\n    n_hidden=4,\n    n_clusters=2,\n    n_features=3\n)\n</pre> # Run simulation with visualization gating_results = run_gating_simulation(     n_simulations=1,     activation='tanh',     with_plot=True,     n_tasks=10,     n_input=5,     n_hidden=4,     n_clusters=2,     n_features=3 ) <pre>\nSimulation 1/1\nError at iteration 0: index 1 is out of bounds for axis 1 with size 1\nTest MSE: 8.5750\n</pre> In\u00a0[33]: Copied! <pre>print(\"\\nFinal Results tanh:\")\nprint(f\"Average Test MSE: {gating_results['avg_test_mse']:.4f}\")\n# print(f\"Average Clustering Accuracy: {gating_results['avg_clustering_acc']:.2%}\")\n</pre> print(\"\\nFinal Results tanh:\") print(f\"Average Test MSE: {gating_results['avg_test_mse']:.4f}\") # print(f\"Average Clustering Accuracy: {gating_results['avg_clustering_acc']:.2%}\") <pre>\nFinal Results tanh:\nAverage Test MSE: 8.5750\n</pre> In\u00a0[36]: Copied! <pre># Run simulation with visualization\ngating_results = run_gating_simulation(\n    n_simulations=1,\n    activation='linear',\n    with_plot=True,\n    n_tasks=10,\n    n_input=5,\n    n_hidden=4,\n    n_clusters=2,\n    n_features=3\n)\n</pre> # Run simulation with visualization gating_results = run_gating_simulation(     n_simulations=1,     activation='linear',     with_plot=True,     n_tasks=10,     n_input=5,     n_hidden=4,     n_clusters=2,     n_features=3 ) <pre>\nSimulation 1/1\nError at iteration 0: index 1 is out of bounds for axis 1 with size 1\nTest MSE: 0.0115\n</pre> In\u00a0[37]: Copied! <pre>print(\"\\nFinal Results linear:\")\nprint(f\"Average Test MSE: {gating_results['avg_test_mse']:.4f}\")\n# print(f\"Average Clustering Accuracy: {gating_results['avg_clustering_acc']:.2%}\")\n</pre> print(\"\\nFinal Results linear:\") print(f\"Average Test MSE: {gating_results['avg_test_mse']:.4f}\") # print(f\"Average Clustering Accuracy: {gating_results['avg_clustering_acc']:.2%}\") <pre>\nFinal Results linear:\nAverage Test MSE: 0.0115\n</pre>"},{"location":"task_clustering/multi-task-learning/#neural-network-model","title":"Neural Network Model\u00b6","text":""},{"location":"task_clustering/multi-task-learning/#task-dependent-prior-mean","title":"Task-dependent Prior Mean\u00b6","text":""},{"location":"task_clustering/multi-task-learning/#clustering-of-tasks","title":"Clustering of Tasks\u00b6","text":""},{"location":"task_clustering/multi-task-learning/#gating-of-tasks","title":"Gating of Tasks\u00b6","text":""},{"location":"task_clustering/reference/","title":"Reference for task clustering approach in multitask learning","text":""},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Base_Class","title":"<code>bmm_multitask_learning.task_clustering.MultiTask_Base_Class</code>","text":""},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Base_Class.MultiTaskNNBase","title":"<code>MultiTaskNNBase</code>","text":"<p>Base class for all multi-task neural network variants</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Base_Class.py</code> <pre><code>class MultiTaskNNBase:\n    \"\"\"Base class for all multi-task neural network variants\"\"\"\n\n    def __init__(self, n_input, n_hidden, n_tasks, activation='tanh'):\n        \"\"\"\n        Initialize base multi-task neural network\n\n        Args:\n            n_input: Number of input features\n            n_hidden: Number of hidden units\n            n_tasks: Number of tasks\n            activation: Activation function ('tanh' or 'linear')\n        \"\"\"\n        self.n_input = n_input\n        self.n_hidden = n_hidden\n        self.n_tasks = n_tasks\n        self.activation = activation\n\n        # Initialize weights\n        self._initialize_weights()\n\n    def _initialize_weights(self, scale=0.5):\n        \"\"\"Initialize network weights with given scale\"\"\"\n        self.W = np.random.randn(self.n_hidden, self.n_input + 1) * scale\n        self.A_map = [np.zeros(self.n_hidden + 1) for _ in range(self.n_tasks)]\n\n    def _activate(self, x):\n        \"\"\"Apply activation function to hidden units\"\"\"\n        if self.activation == 'tanh':\n            return np.tanh(x)\n        elif self.activation == 'linear':\n            return x\n        else:\n            raise ValueError(\"Activation must be 'tanh' or 'linear'\")\n\n    def compute_hidden_activations(self, X):\n        \"\"\"Compute hidden unit activations with bias\"\"\"\n        X_bias = np.hstack([X, np.ones((X.shape[0], 1))])\n        H = self._activate(np.dot(X_bias, self.W.T))\n        return np.hstack([H, np.ones((H.shape[0], 1))])\n\n    def predict(self, X, task_idx):\n        \"\"\"Make predictions for a specific task\"\"\"\n        H_bias = self.compute_hidden_activations(X)\n        return np.dot(H_bias, self.A_map[task_idx])\n\n    def compute_sufficient_statistics(self, X, y):\n        \"\"\"Compute sufficient statistics for a single task\"\"\"\n        H_bias = self.compute_hidden_activations(X)\n        return {\n            'sum_hhT': np.dot(H_bias.T, H_bias),\n            'sum_hy': np.dot(H_bias.T, y),\n            'sum_yy': np.dot(y, y),\n            'n_samples': X.shape[0]\n        }\n\n    def _normalize_data(self, X_list, y_list):\n        \"\"\"Normalize input data\"\"\"\n        X_norm = [(X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8) for X in X_list]\n        y_norm = [(y - np.mean(y)) / (np.std(y) + 1e-8) for y in y_list]\n        return X_norm, y_norm\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Base_Class.MultiTaskNNBase.__init__","title":"<code>__init__(n_input, n_hidden, n_tasks, activation='tanh')</code>","text":"<p>Initialize base multi-task neural network</p> <p>Parameters:</p> Name Type Description Default <code>n_input</code> <p>Number of input features</p> required <code>n_hidden</code> <p>Number of hidden units</p> required <code>n_tasks</code> <p>Number of tasks</p> required <code>activation</code> <p>Activation function ('tanh' or 'linear')</p> <code>'tanh'</code> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Base_Class.py</code> <pre><code>def __init__(self, n_input, n_hidden, n_tasks, activation='tanh'):\n    \"\"\"\n    Initialize base multi-task neural network\n\n    Args:\n        n_input: Number of input features\n        n_hidden: Number of hidden units\n        n_tasks: Number of tasks\n        activation: Activation function ('tanh' or 'linear')\n    \"\"\"\n    self.n_input = n_input\n    self.n_hidden = n_hidden\n    self.n_tasks = n_tasks\n    self.activation = activation\n\n    # Initialize weights\n    self._initialize_weights()\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Base_Class.MultiTaskNNBase.compute_hidden_activations","title":"<code>compute_hidden_activations(X)</code>","text":"<p>Compute hidden unit activations with bias</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Base_Class.py</code> <pre><code>def compute_hidden_activations(self, X):\n    \"\"\"Compute hidden unit activations with bias\"\"\"\n    X_bias = np.hstack([X, np.ones((X.shape[0], 1))])\n    H = self._activate(np.dot(X_bias, self.W.T))\n    return np.hstack([H, np.ones((H.shape[0], 1))])\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Base_Class.MultiTaskNNBase.compute_sufficient_statistics","title":"<code>compute_sufficient_statistics(X, y)</code>","text":"<p>Compute sufficient statistics for a single task</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Base_Class.py</code> <pre><code>def compute_sufficient_statistics(self, X, y):\n    \"\"\"Compute sufficient statistics for a single task\"\"\"\n    H_bias = self.compute_hidden_activations(X)\n    return {\n        'sum_hhT': np.dot(H_bias.T, H_bias),\n        'sum_hy': np.dot(H_bias.T, y),\n        'sum_yy': np.dot(y, y),\n        'n_samples': X.shape[0]\n    }\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Base_Class.MultiTaskNNBase.predict","title":"<code>predict(X, task_idx)</code>","text":"<p>Make predictions for a specific task</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Base_Class.py</code> <pre><code>def predict(self, X, task_idx):\n    \"\"\"Make predictions for a specific task\"\"\"\n    H_bias = self.compute_hidden_activations(X)\n    return np.dot(H_bias, self.A_map[task_idx])\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo","title":"<code>bmm_multitask_learning.task_clustering.MultiTask_Algo</code>","text":""},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNN","title":"<code>MultiTaskNN</code>","text":"<p>               Bases: <code>MultiTaskNNBase</code></p> <p>Basic multi-task neural network with shared hidden layer and task-specific output weights</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>class MultiTaskNN(MultiTaskNNBase):\n    \"\"\"Basic multi-task neural network with shared hidden layer and task-specific output weights\"\"\"\n\n    def __init__(self, n_input, n_hidden, n_tasks, activation='tanh'):\n        super().__init__(n_input, n_hidden, n_tasks, activation)\n\n        # Initialize hyperparameters\n        self.m = np.random.randn(n_hidden + 1) * 0.5\n        self.Sigma = np.eye(n_hidden + 1) * 0.5\n        self.sigma = 1.0\n\n    def log_likelihood(self, params, all_stats):\n        \"\"\"Compute the log likelihood with numerical stability\"\"\"\n        try:\n            # Unpack parameters\n            param_idx = 0\n\n            # W\n            W_size = self.n_hidden * (self.n_input + 1)\n            W = params[param_idx:param_idx + W_size].reshape(self.n_hidden, self.n_input + 1)\n            param_idx += W_size\n\n            # m\n            m_size = self.n_hidden + 1\n            m = params[param_idx:param_idx + m_size]\n            param_idx += m_size\n\n            # Sigma (Cholesky decomposition)\n            L = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n            tril_indices = np.tril_indices(self.n_hidden + 1)\n            L[tril_indices] = params[param_idx:param_idx + len(tril_indices[0])]\n            param_idx += len(tril_indices[0])\n\n            # sigma (log scale)\n            log_sigma = params[param_idx]\n            sigma = np.exp(log_sigma)\n\n            total_log_lik = 0.0\n            self.A_map = []\n\n            # Add regularization to Sigma\n            Sigma = np.dot(L, L.T) + 1e-6 * np.eye(self.n_hidden + 1)\n\n            # Precompute Sigma inverse using Cholesky\n            try:\n                L_sigma = cholesky(Sigma, lower=True)\n                Sigma_inv = solve_triangular(L_sigma, np.eye(self.n_hidden + 1), lower=True)\n                Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n            except np.linalg.LinAlgError:\n                return -np.inf\n\n            for stats in all_stats:\n                sum_hhT = stats['sum_hhT']\n                sum_hy = stats['sum_hy']\n                sum_yy = stats['sum_yy']\n                n_samples = stats['n_samples']\n\n                # Add small constant to avoid division by zero\n                sigma_sq = max(sigma ** 2, 1e-8)\n\n                # Compute Q_i with regularization\n                Q_i = (1.0 / sigma_sq) * sum_hhT + Sigma_inv\n\n                try:\n                    L_Q = cholesky(Q_i + 1e-6 * np.eye(self.n_hidden + 1), lower=True)\n                    Q_inv = solve_triangular(L_Q, np.eye(self.n_hidden + 1), lower=True)\n                    Q_inv = np.dot(Q_inv.T, Q_inv)\n                except np.linalg.LinAlgError:\n                    return -np.inf\n\n                R_i = (1.0 / sigma_sq) * sum_hy + np.dot(Sigma_inv, m)\n\n                # Compute MAP estimate with regularization\n                A_i = np.linalg.solve(Q_i + 1e-6 * np.eye(self.n_hidden + 1), R_i)\n                self.A_map.append(A_i)\n\n                # Compute log determinants\n                logdet_Q_i = 2 * np.sum(np.log(np.diag(L_Q)))\n                logdet_Sigma = 2 * np.sum(np.log(np.diag(L_sigma)))\n\n                # Compute log likelihood terms\n                term1 = -0.5 * (logdet_Sigma + n_samples * 2 * log_sigma + logdet_Q_i)\n                term2 = 0.5 * (\n                            np.dot(R_i, np.dot(Q_inv, R_i)) - (1.0 / sigma_sq) * sum_yy - np.dot(m, np.dot(Sigma_inv, m)))\n\n                if not np.isfinite(term1 + term2):\n                    return -np.inf\n\n                total_log_lik += term1 + term2\n\n            return total_log_lik if np.isfinite(total_log_lik) else -np.inf\n\n        except:\n            return -np.inf\n\n    def fit(self, X_list, y_list, max_iter=100):\n        \"\"\"Fit the model to data\"\"\"\n        # Normalize data\n        X_list, y_list = self._normalize_data(X_list, y_list)\n\n        # Compute sufficient statistics\n        all_stats = [self.compute_sufficient_statistics(X, y) for X, y in zip(X_list, y_list)]\n\n        # Initial parameters with better scaling\n        initial_params = []\n        initial_params.extend(self.W.flatten())\n        initial_params.extend(self.m)\n\n        # Initialize Sigma with Cholesky decomposition\n        L = np.linalg.cholesky(self.Sigma + 1e-6 * np.eye(self.n_hidden + 1))\n        tril_indices = np.tril_indices(self.n_hidden + 1)\n        initial_params.extend(L[tril_indices])\n\n        initial_params.append(np.log(self.sigma))\n\n        # Optimize with bounds for stability\n        bounds = []\n        bounds.extend([(None, None)] * (self.n_hidden * (self.n_input + 1)))  # W\n        bounds.extend([(None, None)] * (self.n_hidden + 1))  # m\n\n        # L - diagonal elements must be positive\n        for i in range(len(tril_indices[0])):\n            if tril_indices[0][i] == tril_indices[1][i]:  # diagonal\n                bounds.append((1e-8, None))\n            else:\n                bounds.append((None, None))\n\n        bounds.append((np.log(1e-8), None))  # log_sigma\n\n        # Optimization with error handling\n        try:\n            result = minimize(\n                lambda p: -self.log_likelihood(p, all_stats),\n                initial_params,\n                method='L-BFGS-B',\n                bounds=bounds,\n                options={\n                    'maxiter': max_iter,\n                    'disp': True,\n                    'maxfun': 15000,\n                    'maxls': 50\n                }\n            )\n\n            # Store optimized parameters\n            self._unpack_parameters(result.x)\n\n            # Recompute MAP estimates\n            _ = self.log_likelihood(result.x, all_stats)\n\n            return result\n\n        except Exception as e:\n            print(f\"Optimization failed: {str(e)}\")\n            return None\n\n    def _unpack_parameters(self, params):\n        \"\"\"Helper to unpack optimized parameters\"\"\"\n        param_idx = 0\n\n        # W\n        W_size = self.n_hidden * (self.n_input + 1)\n        self.W = params[param_idx:param_idx + W_size].reshape(self.n_hidden, self.n_input + 1)\n        param_idx += W_size\n\n        # m\n        m_size = self.n_hidden + 1\n        self.m = params[param_idx:param_idx + m_size]\n        param_idx += m_size\n\n        # Sigma (Cholesky)\n        L = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n        tril_indices = np.tril_indices(self.n_hidden + 1)\n        L[tril_indices] = params[param_idx:param_idx + len(tril_indices[0])]\n        param_idx += len(tril_indices[0])\n        self.Sigma = np.dot(L, L.T) + 1e-6 * np.eye(self.n_hidden + 1)\n\n        # sigma\n        self.sigma = max(np.exp(params[param_idx]), 1e-8)\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNN.fit","title":"<code>fit(X_list, y_list, max_iter=100)</code>","text":"<p>Fit the model to data</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>def fit(self, X_list, y_list, max_iter=100):\n    \"\"\"Fit the model to data\"\"\"\n    # Normalize data\n    X_list, y_list = self._normalize_data(X_list, y_list)\n\n    # Compute sufficient statistics\n    all_stats = [self.compute_sufficient_statistics(X, y) for X, y in zip(X_list, y_list)]\n\n    # Initial parameters with better scaling\n    initial_params = []\n    initial_params.extend(self.W.flatten())\n    initial_params.extend(self.m)\n\n    # Initialize Sigma with Cholesky decomposition\n    L = np.linalg.cholesky(self.Sigma + 1e-6 * np.eye(self.n_hidden + 1))\n    tril_indices = np.tril_indices(self.n_hidden + 1)\n    initial_params.extend(L[tril_indices])\n\n    initial_params.append(np.log(self.sigma))\n\n    # Optimize with bounds for stability\n    bounds = []\n    bounds.extend([(None, None)] * (self.n_hidden * (self.n_input + 1)))  # W\n    bounds.extend([(None, None)] * (self.n_hidden + 1))  # m\n\n    # L - diagonal elements must be positive\n    for i in range(len(tril_indices[0])):\n        if tril_indices[0][i] == tril_indices[1][i]:  # diagonal\n            bounds.append((1e-8, None))\n        else:\n            bounds.append((None, None))\n\n    bounds.append((np.log(1e-8), None))  # log_sigma\n\n    # Optimization with error handling\n    try:\n        result = minimize(\n            lambda p: -self.log_likelihood(p, all_stats),\n            initial_params,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={\n                'maxiter': max_iter,\n                'disp': True,\n                'maxfun': 15000,\n                'maxls': 50\n            }\n        )\n\n        # Store optimized parameters\n        self._unpack_parameters(result.x)\n\n        # Recompute MAP estimates\n        _ = self.log_likelihood(result.x, all_stats)\n\n        return result\n\n    except Exception as e:\n        print(f\"Optimization failed: {str(e)}\")\n        return None\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNN.log_likelihood","title":"<code>log_likelihood(params, all_stats)</code>","text":"<p>Compute the log likelihood with numerical stability</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>def log_likelihood(self, params, all_stats):\n    \"\"\"Compute the log likelihood with numerical stability\"\"\"\n    try:\n        # Unpack parameters\n        param_idx = 0\n\n        # W\n        W_size = self.n_hidden * (self.n_input + 1)\n        W = params[param_idx:param_idx + W_size].reshape(self.n_hidden, self.n_input + 1)\n        param_idx += W_size\n\n        # m\n        m_size = self.n_hidden + 1\n        m = params[param_idx:param_idx + m_size]\n        param_idx += m_size\n\n        # Sigma (Cholesky decomposition)\n        L = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n        tril_indices = np.tril_indices(self.n_hidden + 1)\n        L[tril_indices] = params[param_idx:param_idx + len(tril_indices[0])]\n        param_idx += len(tril_indices[0])\n\n        # sigma (log scale)\n        log_sigma = params[param_idx]\n        sigma = np.exp(log_sigma)\n\n        total_log_lik = 0.0\n        self.A_map = []\n\n        # Add regularization to Sigma\n        Sigma = np.dot(L, L.T) + 1e-6 * np.eye(self.n_hidden + 1)\n\n        # Precompute Sigma inverse using Cholesky\n        try:\n            L_sigma = cholesky(Sigma, lower=True)\n            Sigma_inv = solve_triangular(L_sigma, np.eye(self.n_hidden + 1), lower=True)\n            Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n        except np.linalg.LinAlgError:\n            return -np.inf\n\n        for stats in all_stats:\n            sum_hhT = stats['sum_hhT']\n            sum_hy = stats['sum_hy']\n            sum_yy = stats['sum_yy']\n            n_samples = stats['n_samples']\n\n            # Add small constant to avoid division by zero\n            sigma_sq = max(sigma ** 2, 1e-8)\n\n            # Compute Q_i with regularization\n            Q_i = (1.0 / sigma_sq) * sum_hhT + Sigma_inv\n\n            try:\n                L_Q = cholesky(Q_i + 1e-6 * np.eye(self.n_hidden + 1), lower=True)\n                Q_inv = solve_triangular(L_Q, np.eye(self.n_hidden + 1), lower=True)\n                Q_inv = np.dot(Q_inv.T, Q_inv)\n            except np.linalg.LinAlgError:\n                return -np.inf\n\n            R_i = (1.0 / sigma_sq) * sum_hy + np.dot(Sigma_inv, m)\n\n            # Compute MAP estimate with regularization\n            A_i = np.linalg.solve(Q_i + 1e-6 * np.eye(self.n_hidden + 1), R_i)\n            self.A_map.append(A_i)\n\n            # Compute log determinants\n            logdet_Q_i = 2 * np.sum(np.log(np.diag(L_Q)))\n            logdet_Sigma = 2 * np.sum(np.log(np.diag(L_sigma)))\n\n            # Compute log likelihood terms\n            term1 = -0.5 * (logdet_Sigma + n_samples * 2 * log_sigma + logdet_Q_i)\n            term2 = 0.5 * (\n                        np.dot(R_i, np.dot(Q_inv, R_i)) - (1.0 / sigma_sq) * sum_yy - np.dot(m, np.dot(Sigma_inv, m)))\n\n            if not np.isfinite(term1 + term2):\n                return -np.inf\n\n            total_log_lik += term1 + term2\n\n        return total_log_lik if np.isfinite(total_log_lik) else -np.inf\n\n    except:\n        return -np.inf\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNNClustering","title":"<code>MultiTaskNNClustering</code>","text":"<p>               Bases: <code>MultiTaskNNBase</code></p> <p>Multi-task NN with task clustering</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>class MultiTaskNNClustering(MultiTaskNNBase):\n    \"\"\"Multi-task NN with task clustering\"\"\"\n\n    def __init__(self, n_input, n_hidden, n_tasks, n_clusters, activation='tanh'):\n        super().__init__(n_input, n_hidden, n_tasks, activation)\n        self.n_clusters = n_clusters\n\n        # Initialize with larger scale and better conditioning\n        self.q = np.ones(n_clusters) / n_clusters\n        self.m = np.random.randn(n_clusters, n_hidden + 1) * 0.5\n\n        # Initialize Sigma with larger diagonal for numerical stability\n        self.Sigma = np.array([np.eye(n_hidden + 1) * 0.5 for _ in range(n_clusters)])\n        self.sigma = 1.0\n\n        self.z = np.zeros((n_tasks, n_clusters))\n\n    def _compute_task_log_likelihood(self, X_i, y_i, cluster_idx):\n        n_i = len(y_i)\n        h_i = self.compute_hidden_activations(X_i)\n\n        # Add small constant to avoid division by zero\n        sigma_sq = max(self.sigma ** 2, 1e-8)\n\n        try:\n            # Use Cholesky decomposition for numerical stability\n            L = cholesky(self.Sigma[cluster_idx], lower=True)\n            Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n            Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n\n            Q_i = (1 / sigma_sq) * np.dot(h_i.T, h_i) + Sigma_inv\n            L_Q = cholesky(Q_i, lower=True)\n            Q_inv = solve_triangular(L_Q, np.eye(self.n_hidden + 1), lower=True)\n            Q_inv = np.dot(Q_inv.T, Q_inv)\n\n            R_i = (1 / sigma_sq) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[cluster_idx])\n\n            # Compute log determinants efficiently\n            logdet_Sigma = 2 * np.sum(np.log(np.diag(L)))\n            logdet_Q_i = 2 * np.sum(np.log(np.diag(L_Q)))\n\n            term1 = -0.5 * (logdet_Sigma + n_i * np.log(sigma_sq) + logdet_Q_i)\n            term2 = 0.5 * (np.dot(R_i.T, np.dot(Q_inv, R_i)) - (1 / (2 * sigma_sq)) * np.sum(y_i ** 2) - np.dot(\n                self.m[cluster_idx].T, np.dot(Sigma_inv, self.m[cluster_idx])))\n\n            return term1 + term2\n\n        except np.linalg.LinAlgError:\n            # Return -inf if matrix is not positive definite\n            return -np.inf\n\n    def e_step(self, data):\n        log_responsibilities = np.zeros((self.n_tasks, self.n_clusters))\n\n        for i, (X_i, y_i) in enumerate(data):\n            for alpha in range(self.n_clusters):\n                log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n                log_responsibilities[i, alpha] = np.log(self.q[alpha] + 1e-8) + log_lik\n\n            # Normalize using logsumexp for numerical stability\n            log_responsibilities[i] -= logsumexp(log_responsibilities[i])\n\n        self.z = np.exp(log_responsibilities)\n\n    def m_step(self, data):\n        def objective(params):\n            W = params[:self.n_hidden * (self.n_input + 1)].reshape(self.n_hidden, self.n_input + 1)\n            log_sigma = params[-1]\n            sigma = np.exp(log_sigma)\n\n            self.W = W\n            self.sigma = max(sigma, 1e-8)  # Prevent sigma from becoming too small\n\n            total_log_lik = 0.0\n            for i, (X_i, y_i) in enumerate(data):\n                for alpha in range(self.n_clusters):\n                    log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n                    total_log_lik += self.z[i, alpha] * log_lik\n\n            return -total_log_lik if np.isfinite(total_log_lik) else np.inf\n\n        # Initial parameters with bounds\n        initial_params = np.concatenate([\n            self.W.flatten(),\n            [np.log(self.sigma)]\n        ])\n\n        # Add bounds for sigma (log_sigma &gt; log(1e-8))\n        bounds = [(None, None)] * len(initial_params)\n        bounds[-1] = (np.log(1e-8), None)\n\n        result = minimize(\n            objective,\n            initial_params,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 50, 'disp': True}\n        )\n\n        opt_params = result.x\n        W_size = self.n_hidden * (self.n_input + 1)\n        self.W = opt_params[:W_size].reshape(self.n_hidden, self.n_input + 1)\n        self.sigma = max(np.exp(opt_params[-1]), 1e-8)\n\n        # Update cluster parameters with regularization\n\n        for alpha in range(self.n_clusters):\n            self.q[alpha] = max(np.sum(self.z[:, alpha]) / self.n_tasks, 1e-8)\n\n            sum_z = np.sum(self.z[:, alpha])\n            if sum_z &gt; 1e-8:\n                weighted_R = np.zeros(self.n_hidden + 1)\n                weighted_Q = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n\n                for i, (X_i, y_i) in enumerate(data):\n                    h_i = self.compute_hidden_activations(X_i)\n                    L = cholesky(self.Sigma[alpha] + 1e-6 * np.eye(self.n_hidden + 1), lower=True)\n                    Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n                    Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n\n                    Q_i = (1 / max(self.sigma ** 2, 1e-8)) * np.dot(h_i.T, h_i) + Sigma_inv\n                    R_i = (1 / max(self.sigma ** 2, 1e-8)) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[alpha])\n\n                    weighted_R += self.z[i, alpha] * R_i\n                    weighted_Q += self.z[i, alpha] * Q_i\n\n                try:\n                    self.m[alpha] = np.linalg.solve(weighted_Q + 1e-6 * np.eye(self.n_hidden + 1), weighted_R)\n                except:\n                    pass\n\n                    # Update covariance with regularization\n                    weighted_cov = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n                    for i, (X_i, y_i) in enumerate(data):\n                        h_i = self.compute_hidden_activations(X_i)\n                        A_i = self._compute_map_estimate(X_i, y_i, alpha)\n                        diff = A_i - self.m[alpha]\n                        weighted_cov += self.z[i, alpha] * np.outer(diff, diff)\n\n                    self.Sigma[alpha] = weighted_cov / sum_z + 1e-6 * np.eye(self.n_hidden + 1)\n\n    def _compute_map_estimate(self, X_i, y_i, cluster_idx):\n        h_i = self.compute_hidden_activations(X_i)\n        sigma_sq = max(self.sigma ** 2, 1e-8)\n\n        L = cholesky(self.Sigma[cluster_idx] + 1e-6 * np.eye(self.n_hidden + 1), lower=True)\n        Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n        Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n\n        Q_i = (1 / sigma_sq) * np.dot(h_i.T, h_i) + Sigma_inv\n        R_i = (1 / sigma_sq) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[cluster_idx])\n\n        return np.linalg.solve(Q_i + 1e-6 * np.eye(self.n_hidden + 1), R_i)\n\n    def fit(self, data, max_iter=100, tol=1e-4):\n        prev_log_lik = -np.inf\n\n        for iteration in tqdm((range(max_iter))):\n            self.e_step(data)\n            self.m_step(data)\n\n            # Compute current log likelihood\n            current_log_lik = 0.0\n            for i, (X_i, y_i) in enumerate(data):\n                cluster_log_liks = []\n                for alpha in range(self.n_clusters):\n                    log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n                    cluster_log_liks.append(np.log(self.q[alpha] + 1e-8) + log_lik)\n                current_log_lik += logsumexp(cluster_log_liks)\n\n            if np.isnan(current_log_lik):\n                print(\"Warning: log likelihood is nan, stopping early\")\n                break\n\n            if iteration &gt; 0 and np.abs(current_log_lik - prev_log_lik) &lt; tol:\n                print(f\"Converged at iteration {iteration}\")\n                break\n\n            prev_log_lik = current_log_lik\n\n            self._compute_final_weights(data)\n\n        def _compute_final_weights(self, data):\n            for i, (X_i, y_i) in enumerate(data):\n                most_likely_cluster = np.argmax(self.z[i])\n                self.A_map[i] = self._compute_map_estimate(X_i, y_i, most_likely_cluster)\n\n        def get_cluster_assignments(self):\n            return np.argmax(self.z, axis=1)\n\n        def get_task_similarity(self):\n            assignments = self.get_cluster_assignments()\n            return np.array([[1.0 if a == b else 0.0 for b in assignments] for a in assignments])\n\n    def _compute_final_weights(self, data):\n        for i, (X_i, y_i) in enumerate(data):\n            most_likely_cluster = np.argmax(self.z[i])\n            self.A_map[i] = self._compute_map_estimate(X_i, y_i, most_likely_cluster)\n\n    def get_cluster_assignments(self):\n        return np.argmax(self.z, axis=1)\n\n    def get_task_similarity(self):\n        assignments = self.get_cluster_assignments()\n        return np.array([[1.0 if a == b else 0.0 for b in assignments] for a in assignments])\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNNDependentMean","title":"<code>MultiTaskNNDependentMean</code>","text":"<p>               Bases: <code>MultiTaskNNBase</code></p> <p>Multi-task NN with task-dependent prior means</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>class MultiTaskNNDependentMean(MultiTaskNNBase):\n    \"\"\"Multi-task NN with task-dependent prior means\"\"\"\n\n    def __init__(self, n_input, n_hidden, n_tasks, n_features, activation='tanh'):\n        super().__init__(n_input, n_hidden, n_tasks, activation)\n        self.n_features = n_features\n\n        # Initialize hyperparameters with better scaling\n        self.M = np.random.randn(n_hidden + 1, n_features) * 0.1\n        self.Sigma = np.eye(n_hidden + 1) * 0.5  # Start with smaller variance\n        self.sigma = 1.0\n\n    def log_likelihood(self, params, all_stats, all_task_features):\n        \"\"\"Compute the log likelihood with numerical stability improvements\"\"\"\n        # Unpack parameters\n        param_idx = 0\n\n        # W\n        W_size = self.n_hidden * (self.n_input + 1)\n        W = params[param_idx:param_idx + W_size].reshape(self.n_hidden, self.n_input + 1)\n        param_idx += W_size\n\n        # M: (n_hidden + 1 x n_features)\n        M_size = (self.n_hidden + 1) * self.n_features\n        M = params[param_idx:param_idx + M_size].reshape(self.n_hidden + 1, self.n_features)\n        param_idx += M_size\n\n        # Sigma (Cholesky decomposition)\n        L = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n        tril_indices = np.tril_indices(self.n_hidden + 1)\n        L[tril_indices] = params[param_idx:param_idx + len(tril_indices[0])]\n        param_idx += len(tril_indices[0])\n\n        # sigma (log scale)\n        log_sigma = params[param_idx]\n        sigma = np.exp(log_sigma)\n\n        total_log_lik = 0.0\n        self.A_map = []\n\n        # Precompute Sigma inverse using Cholesky\n        try:\n            Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n            Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n        except np.linalg.LinAlgError:\n            return -np.inf  # Invalid covariance matrix\n\n        for stats, task_features in zip(all_stats, all_task_features):\n            sum_hhT = stats['sum_hhT']\n            sum_hy = stats['sum_hy']\n            sum_yy = stats['sum_yy']\n            n_samples = stats['n_samples']\n\n            # Compute task-dependent prior mean\n            m_i = np.dot(M, task_features)\n\n            # Compute Q_i using Cholesky for stability\n            Q_i = (1.0 / (sigma ** 2)) * sum_hhT + Sigma_inv\n\n            try:\n                L_Q = np.linalg.cholesky(Q_i)\n                Q_inv = solve_triangular(L_Q, np.eye(self.n_hidden + 1), lower=True)\n                Q_inv = np.dot(Q_inv.T, Q_inv)\n            except np.linalg.LinAlgError:\n                return -np.inf\n\n            R_i = (1.0 / (sigma ** 2)) * sum_hy + np.dot(Sigma_inv, m_i)\n\n            # Compute MAP estimate\n            A_i = np.dot(Q_inv, R_i)\n            self.A_map.append(A_i)\n\n            # Compute log determinants efficiently\n            logdet_Q_i = 2 * np.sum(np.log(np.diag(L_Q)))\n            logdet_Sigma = 2 * np.sum(np.log(np.diag(L)))\n\n            # Compute log likelihood terms\n            term1 = -0.5 * (logdet_Sigma + n_samples * 2 * log_sigma + logdet_Q_i)\n            term2 = 0.5 * (np.dot(R_i, np.dot(Q_inv, R_i)) - (1.0 / (sigma ** 2)) * sum_yy - np.dot(m_i,\n                                                                                                    np.dot(Sigma_inv,\n                                                                                                           m_i)))\n\n            total_log_lik += term1 + term2\n\n        return total_log_lik\n\n    def fit(self, X_list, y_list, task_features_list, max_iter=100):\n        \"\"\"Fit the model with improved optimization\"\"\"\n        # Normalize data\n        X_list = [(X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8) for X in X_list]\n        y_list = [(y - np.mean(y)) / (np.std(y) + 1e-8) for y in y_list]\n\n        # Compute sufficient statistics\n        all_stats = [self.compute_sufficient_statistics(X, y) for X, y in zip(X_list, y_list)]\n\n        # Initial parameters with better scaling\n        initial_params = []\n        initial_params.extend(self.W.flatten())\n        initial_params.extend(self.M.flatten())\n\n        L = np.linalg.cholesky(self.Sigma + 1e-6 * np.eye(self.n_hidden + 1))\n        tril_indices = np.tril_indices(self.n_hidden + 1)\n        initial_params.extend(L[tril_indices])\n\n        initial_params.append(np.log(self.sigma))\n\n        # Optimize with bounds for stability\n        bounds = []\n\n        # W - no bounds\n        bounds.extend([(None, None)] * (self.n_hidden * (self.n_input + 1)))\n\n        # M - no bounds\n        bounds.extend([(None, None)] * ((self.n_hidden + 1) * self.n_features))\n\n        # L - diagonal elements must be positive\n        for i in range(len(tril_indices[0])):\n            if tril_indices[0][i] == tril_indices[1][i]:  # diagonal\n                bounds.append((1e-8, None))\n            else:\n                bounds.append((None, None))\n\n        # log_sigma must be &gt; log(1e-8)\n        bounds.append((np.log(1e-8), None))\n\n        # Optimize\n        result = minimize(\n            lambda p: -self.log_likelihood(p, all_stats, task_features_list),\n            initial_params,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': max_iter, 'disp': True}\n        )\n\n        # Store optimized parameters\n        self._unpack_parameters(result.x)\n\n        # Recompute MAP estimates\n        _ = self.log_likelihood(result.x, all_stats, task_features_list)\n\n        return result\n\n    def _unpack_parameters(self, params):\n        \"\"\"Helper to unpack optimized parameters\"\"\"\n        param_idx = 0\n\n        # W\n        W_size = self.n_hidden * (self.n_input + 1)\n        self.W = params[param_idx:param_idx + W_size].reshape(self.n_hidden, self.n_input + 1)\n        param_idx += W_size\n\n        # M: (n_hidden + 1 x n_features)\n        M_size = (self.n_hidden + 1) * self.n_features\n        M = params[param_idx:param_idx + M_size].reshape(self.n_hidden + 1, self.n_features)\n        param_idx += M_size\n\n        # Sigma (Cholesky)\n        L = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n        tril_indices = np.tril_indices(self.n_hidden + 1)\n        L[tril_indices] = params[param_idx:param_idx + len(tril_indices[0])]\n        param_idx += len(tril_indices[0])\n        self.Sigma = np.dot(L, L.T)\n\n        # sigma\n        self.sigma = np.exp(params[param_idx])\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNNDependentMean.fit","title":"<code>fit(X_list, y_list, task_features_list, max_iter=100)</code>","text":"<p>Fit the model with improved optimization</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>def fit(self, X_list, y_list, task_features_list, max_iter=100):\n    \"\"\"Fit the model with improved optimization\"\"\"\n    # Normalize data\n    X_list = [(X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8) for X in X_list]\n    y_list = [(y - np.mean(y)) / (np.std(y) + 1e-8) for y in y_list]\n\n    # Compute sufficient statistics\n    all_stats = [self.compute_sufficient_statistics(X, y) for X, y in zip(X_list, y_list)]\n\n    # Initial parameters with better scaling\n    initial_params = []\n    initial_params.extend(self.W.flatten())\n    initial_params.extend(self.M.flatten())\n\n    L = np.linalg.cholesky(self.Sigma + 1e-6 * np.eye(self.n_hidden + 1))\n    tril_indices = np.tril_indices(self.n_hidden + 1)\n    initial_params.extend(L[tril_indices])\n\n    initial_params.append(np.log(self.sigma))\n\n    # Optimize with bounds for stability\n    bounds = []\n\n    # W - no bounds\n    bounds.extend([(None, None)] * (self.n_hidden * (self.n_input + 1)))\n\n    # M - no bounds\n    bounds.extend([(None, None)] * ((self.n_hidden + 1) * self.n_features))\n\n    # L - diagonal elements must be positive\n    for i in range(len(tril_indices[0])):\n        if tril_indices[0][i] == tril_indices[1][i]:  # diagonal\n            bounds.append((1e-8, None))\n        else:\n            bounds.append((None, None))\n\n    # log_sigma must be &gt; log(1e-8)\n    bounds.append((np.log(1e-8), None))\n\n    # Optimize\n    result = minimize(\n        lambda p: -self.log_likelihood(p, all_stats, task_features_list),\n        initial_params,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': max_iter, 'disp': True}\n    )\n\n    # Store optimized parameters\n    self._unpack_parameters(result.x)\n\n    # Recompute MAP estimates\n    _ = self.log_likelihood(result.x, all_stats, task_features_list)\n\n    return result\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNNDependentMean.log_likelihood","title":"<code>log_likelihood(params, all_stats, all_task_features)</code>","text":"<p>Compute the log likelihood with numerical stability improvements</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>def log_likelihood(self, params, all_stats, all_task_features):\n    \"\"\"Compute the log likelihood with numerical stability improvements\"\"\"\n    # Unpack parameters\n    param_idx = 0\n\n    # W\n    W_size = self.n_hidden * (self.n_input + 1)\n    W = params[param_idx:param_idx + W_size].reshape(self.n_hidden, self.n_input + 1)\n    param_idx += W_size\n\n    # M: (n_hidden + 1 x n_features)\n    M_size = (self.n_hidden + 1) * self.n_features\n    M = params[param_idx:param_idx + M_size].reshape(self.n_hidden + 1, self.n_features)\n    param_idx += M_size\n\n    # Sigma (Cholesky decomposition)\n    L = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n    tril_indices = np.tril_indices(self.n_hidden + 1)\n    L[tril_indices] = params[param_idx:param_idx + len(tril_indices[0])]\n    param_idx += len(tril_indices[0])\n\n    # sigma (log scale)\n    log_sigma = params[param_idx]\n    sigma = np.exp(log_sigma)\n\n    total_log_lik = 0.0\n    self.A_map = []\n\n    # Precompute Sigma inverse using Cholesky\n    try:\n        Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n        Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n    except np.linalg.LinAlgError:\n        return -np.inf  # Invalid covariance matrix\n\n    for stats, task_features in zip(all_stats, all_task_features):\n        sum_hhT = stats['sum_hhT']\n        sum_hy = stats['sum_hy']\n        sum_yy = stats['sum_yy']\n        n_samples = stats['n_samples']\n\n        # Compute task-dependent prior mean\n        m_i = np.dot(M, task_features)\n\n        # Compute Q_i using Cholesky for stability\n        Q_i = (1.0 / (sigma ** 2)) * sum_hhT + Sigma_inv\n\n        try:\n            L_Q = np.linalg.cholesky(Q_i)\n            Q_inv = solve_triangular(L_Q, np.eye(self.n_hidden + 1), lower=True)\n            Q_inv = np.dot(Q_inv.T, Q_inv)\n        except np.linalg.LinAlgError:\n            return -np.inf\n\n        R_i = (1.0 / (sigma ** 2)) * sum_hy + np.dot(Sigma_inv, m_i)\n\n        # Compute MAP estimate\n        A_i = np.dot(Q_inv, R_i)\n        self.A_map.append(A_i)\n\n        # Compute log determinants efficiently\n        logdet_Q_i = 2 * np.sum(np.log(np.diag(L_Q)))\n        logdet_Sigma = 2 * np.sum(np.log(np.diag(L)))\n\n        # Compute log likelihood terms\n        term1 = -0.5 * (logdet_Sigma + n_samples * 2 * log_sigma + logdet_Q_i)\n        term2 = 0.5 * (np.dot(R_i, np.dot(Q_inv, R_i)) - (1.0 / (sigma ** 2)) * sum_yy - np.dot(m_i,\n                                                                                                np.dot(Sigma_inv,\n                                                                                                       m_i)))\n\n        total_log_lik += term1 + term2\n\n    return total_log_lik\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNNGating","title":"<code>MultiTaskNNGating</code>","text":"<p>               Bases: <code>MultiTaskNNBase</code></p> <p>Multi-task NN with gating network for task clustering</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>class MultiTaskNNGating(MultiTaskNNBase):\n    \"\"\"Multi-task NN with gating network for task clustering\"\"\"\n\n    def __init__(self, n_input, n_hidden, n_tasks, n_clusters, n_features, activation='tanh'):\n        super().__init__(n_input, n_hidden, n_tasks, activation)\n        self.n_clusters = n_clusters\n        self.n_features = n_features\n\n        # Initialize with larger scale for better convergence\n        self.U = np.random.randn(n_clusters, n_features) * 0.5\n        self.m = np.random.randn(n_clusters, n_hidden + 1) * 0.5\n\n        # Initialize covariance matrices with larger diagonal\n        self.Sigma = np.array([np.eye(n_hidden + 1) * 0.5 for _ in range(n_clusters)])\n        self.sigma = 1.0\n\n        self.z = np.zeros((n_tasks, n_clusters))\n\n    def compute_gating_probabilities(self, F):\n        \"\"\"Compute task-cluster assignment probabilities with numerical stability\"\"\"\n        # Ensure F is 2D array\n        F = np.atleast_2d(F)\n        if F.shape[0] == 1 and self.n_tasks &gt; 1:\n            F = np.repeat(F, self.n_tasks, axis=0)\n\n        logits = np.dot(F, self.U.T)\n        return softmax(logits, axis=1)\n\n    def _compute_task_log_likelihood(self, X_i, y_i, cluster_idx):\n        n_i = len(y_i)\n        h_i = self.compute_hidden_activations(X_i)\n\n        # Add small constant to avoid division by zero\n        sigma_sq = max(self.sigma ** 2, 1e-8)\n\n        try:\n            # Use Cholesky decomposition for numerical stability\n            L = cholesky(self.Sigma[cluster_idx] + 1e-6 * np.eye(self.n_hidden + 1), lower=True)\n            Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n            Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n\n            Q_i = (1 / sigma_sq) * np.dot(h_i.T, h_i) + Sigma_inv\n            L_Q = cholesky(Q_i, lower=True)\n            Q_inv = solve_triangular(L_Q, np.eye(self.n_hidden + 1), lower=True)\n            Q_inv = np.dot(Q_inv.T, Q_inv)\n\n            R_i = (1 / sigma_sq) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[cluster_idx])\n\n            # Compute log determinants\n            logdet_Sigma = 2 * np.sum(np.log(np.diag(L)))\n            logdet_Q_i = 2 * np.sum(np.log(np.diag(L_Q)))\n\n            term1 = -0.5 * (logdet_Sigma + n_i * np.log(sigma_sq) + logdet_Q_i)\n            term2 = 0.5 * (np.dot(R_i.T, np.dot(Q_inv, R_i)) - (1 / (2 * sigma_sq)) * np.sum(y_i ** 2) - np.dot(\n                self.m[cluster_idx].T, np.dot(Sigma_inv, self.m[cluster_idx])))\n\n            return term1 + term2\n\n        except np.linalg.LinAlgError:\n            return -np.inf\n\n    def e_step(self, data, task_features):\n        \"\"\"Expectation step with improved numerical stability\"\"\"\n        # Ensure task_features is 2D array\n        task_features = np.atleast_2d(task_features)\n        if task_features.shape[0] == 1 and self.n_tasks &gt; 1:\n            task_features = np.repeat(task_features, self.n_tasks, axis=0)\n\n        q = self.compute_gating_probabilities(task_features)\n        log_responsibilities = np.zeros((self.n_tasks, self.n_clusters))\n\n        for i, (X_i, y_i) in enumerate(data):\n            for alpha in range(self.n_clusters):\n                log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n                log_responsibilities[i, alpha] = np.log(q[i, alpha] + 1e-8) + log_lik\n\n            # Normalize using logsumexp\n            log_responsibilities[i] -= logsumexp(log_responsibilities[i])\n\n        self.z = np.exp(log_responsibilities)\n\n    def m_step(self, data, task_features):\n        \"\"\"Maximization step with regularization\"\"\"\n\n        # Optimize W and sigma\n        def objective(params):\n            W = params[:self.n_hidden * (self.n_input + 1)].reshape(self.n_hidden, self.n_input + 1)\n            log_sigma = params[-1]\n            sigma = np.exp(log_sigma)\n\n            self.W = W\n            self.sigma = max(sigma, 1e-8)\n\n            total_log_lik = 0.0\n            for i, (X_i, y_i) in enumerate(data):\n                for alpha in range(self.n_clusters):\n                    log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n                    total_log_lik += self.z[i, alpha] * log_lik\n\n            return -total_log_lik if np.isfinite(total_log_lik) else np.inf\n\n        # Initial parameters with bounds\n        initial_params = np.concatenate([\n            self.W.flatten(),\n            [np.log(self.sigma)]\n        ])\n\n        bounds = [(None, None)] * len(initial_params)\n        bounds[-1] = (np.log(1e-8), None)  # sigma &gt; 1e-8\n\n        result = minimize(\n            objective,\n            initial_params,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 50, 'disp': True}\n        )\n\n        # Update parameters\n        opt_params = result.x\n        W_size = self.n_hidden * (self.n_input + 1)\n        self.W = opt_params[:W_size].reshape(self.n_hidden, self.n_input + 1)\n        self.sigma = max(np.exp(opt_params[-1]), 1e-8)\n\n        # Update cluster parameters with regularization\n        for alpha in range(self.n_clusters):\n            sum_z = np.sum(self.z[:, alpha])\n            if sum_z &gt; 1e-8:\n                # Update m_\u03b1\n                weighted_R = np.zeros(self.n_hidden + 1)\n                weighted_Q = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n\n                for i, (X_i, y_i) in enumerate(data):\n                    h_i = self.compute_hidden_activations(X_i)\n                    L = cholesky(self.Sigma[alpha] + 1e-6 * np.eye(self.n_hidden + 1), lower=True)\n                    Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n                    Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n\n                    Q_i = (1 / max(self.sigma ** 2, 1e-8)) * np.dot(h_i.T, h_i) + Sigma_inv\n                    R_i = (1 / max(self.sigma ** 2, 1e-8)) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[alpha])\n\n                    weighted_R += self.z[i, alpha] * R_i\n                    weighted_Q += self.z[i, alpha] * Q_i\n\n                try:\n                    self.m[alpha] = np.linalg.solve(weighted_Q + 1e-6 * np.eye(self.n_hidden + 1), weighted_R)\n                except:\n                    pass\n\n                    # Update \u03a3_\u03b1 with regularization\n                    weighted_cov = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n                    for i, (X_i, y_i) in enumerate(data):\n                        h_i = self.compute_hidden_activations(X_i)\n                        A_i = self._compute_map_estimate(X_i, y_i, alpha)\n                        diff = A_i - self.m[alpha]\n                        weighted_cov += self.z[i, alpha] * np.outer(diff, diff)\n\n                    self.Sigma[alpha] = weighted_cov / sum_z + 1e-6 * np.eye(self.n_hidden + 1)\n\n                # Update gating parameters U\n                if self.n_clusters &gt; 1:\n                    task_features = np.atleast_2d(task_features)\n                    if task_features.shape[0] == 1 and self.n_tasks &gt; 1:\n                        task_features = np.repeat(task_features, self.n_tasks, axis=0)\n\n                    lr = LogisticRegression(\n                        multi_class='multinomial',\n                        solver='lbfgs',\n                        fit_intercept=False,\n                        max_iter=100,\n                        penalty='l2',\n                        C=1.0\n                    )\n                    try:\n                        lr.fit(task_features, self.get_cluster_assignments(), sample_weight=np.max(self.z, axis=1))\n                        self.U = lr.coef_\n                    except:\n                        pass\n\n    def _compute_map_estimate(self, X_i, y_i, cluster_idx):\n        h_i = self.compute_hidden_activations(X_i)\n        sigma_sq = max(self.sigma ** 2, 1e-8)\n\n        L = cholesky(self.Sigma[cluster_idx] + 1e-6 * np.eye(self.n_hidden + 1), lower=True)\n        Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n        Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n\n        Q_i = (1 / sigma_sq) * np.dot(h_i.T, h_i) + Sigma_inv\n        R_i = (1 / sigma_sq) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[cluster_idx])\n\n        return np.linalg.solve(Q_i + 1e-6 * np.eye(self.n_hidden + 1), R_i)\n\n    def fit(self, data, task_features, max_iter=100, tol=1e-4):\n        \"\"\"Improved fitting with better initialization and checks\"\"\"\n        prev_log_lik = -np.inf\n\n        # Normalize task features\n        task_features = np.atleast_2d(task_features)\n        if task_features.shape[0] == 1 and self.n_tasks &gt; 1:\n            task_features = np.repeat(task_features, self.n_tasks, axis=0)\n\n        self.task_feature_mean = np.mean(task_features, axis=0)\n        self.task_feature_std = np.std(task_features, axis=0) + 1e-8\n        task_features = (task_features - self.task_feature_mean) / self.task_feature_std\n\n        for iteration in range(max_iter):\n            try:\n                self.e_step(data, task_features)\n                self.m_step(data, task_features)\n\n                # Compute current log likelihood\n                current_log_lik = 0.0\n                q = self.compute_gating_probabilities(task_features)\n\n                for i, (X_i, y_i) in enumerate(data):\n                    cluster_log_liks = []\n                    for alpha in range(self.n_clusters):\n                        log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n                        cluster_log_liks.append(np.log(q[i, alpha] + 1e-8) + log_lik)\n                    current_log_lik += logsumexp(cluster_log_liks)\n\n                if np.isnan(current_log_lik):\n                    print(\"Warning: log likelihood is nan, stopping early\")\n                    break\n\n                if iteration &gt; 0 and abs(current_log_lik - prev_log_lik) &lt; tol:\n                    print(f\"Converged at iteration {iteration}\")\n                    break\n\n                prev_log_lik = current_log_lik\n                print(f\"Iteration {iteration}, log likelihood: {current_log_lik}\")\n\n            except Exception as e:\n                print(f\"Error at iteration {iteration}: {str(e)}\")\n                break\n\n        self._compute_final_weights(data)\n        return self\n\n\n    def _compute_final_weights(self, data):\n        for i, (X_i, y_i) in enumerate(data):\n            most_likely_cluster = np.argmax(self.z[i])\n            self.A_map[i] = self._compute_map_estimate(X_i, y_i, most_likely_cluster)\n\n    def get_cluster_assignments(self):\n        return np.argmax(self.z, axis=1)\n\n\n    def get_task_similarity(self):\n        assignments = self.get_cluster_assignments()\n        return np.array([[1.0 if a == b else 0.0 for b in assignments] for a in assignments])\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNNGating.compute_gating_probabilities","title":"<code>compute_gating_probabilities(F)</code>","text":"<p>Compute task-cluster assignment probabilities with numerical stability</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>def compute_gating_probabilities(self, F):\n    \"\"\"Compute task-cluster assignment probabilities with numerical stability\"\"\"\n    # Ensure F is 2D array\n    F = np.atleast_2d(F)\n    if F.shape[0] == 1 and self.n_tasks &gt; 1:\n        F = np.repeat(F, self.n_tasks, axis=0)\n\n    logits = np.dot(F, self.U.T)\n    return softmax(logits, axis=1)\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNNGating.e_step","title":"<code>e_step(data, task_features)</code>","text":"<p>Expectation step with improved numerical stability</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>def e_step(self, data, task_features):\n    \"\"\"Expectation step with improved numerical stability\"\"\"\n    # Ensure task_features is 2D array\n    task_features = np.atleast_2d(task_features)\n    if task_features.shape[0] == 1 and self.n_tasks &gt; 1:\n        task_features = np.repeat(task_features, self.n_tasks, axis=0)\n\n    q = self.compute_gating_probabilities(task_features)\n    log_responsibilities = np.zeros((self.n_tasks, self.n_clusters))\n\n    for i, (X_i, y_i) in enumerate(data):\n        for alpha in range(self.n_clusters):\n            log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n            log_responsibilities[i, alpha] = np.log(q[i, alpha] + 1e-8) + log_lik\n\n        # Normalize using logsumexp\n        log_responsibilities[i] -= logsumexp(log_responsibilities[i])\n\n    self.z = np.exp(log_responsibilities)\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNNGating.fit","title":"<code>fit(data, task_features, max_iter=100, tol=0.0001)</code>","text":"<p>Improved fitting with better initialization and checks</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>def fit(self, data, task_features, max_iter=100, tol=1e-4):\n    \"\"\"Improved fitting with better initialization and checks\"\"\"\n    prev_log_lik = -np.inf\n\n    # Normalize task features\n    task_features = np.atleast_2d(task_features)\n    if task_features.shape[0] == 1 and self.n_tasks &gt; 1:\n        task_features = np.repeat(task_features, self.n_tasks, axis=0)\n\n    self.task_feature_mean = np.mean(task_features, axis=0)\n    self.task_feature_std = np.std(task_features, axis=0) + 1e-8\n    task_features = (task_features - self.task_feature_mean) / self.task_feature_std\n\n    for iteration in range(max_iter):\n        try:\n            self.e_step(data, task_features)\n            self.m_step(data, task_features)\n\n            # Compute current log likelihood\n            current_log_lik = 0.0\n            q = self.compute_gating_probabilities(task_features)\n\n            for i, (X_i, y_i) in enumerate(data):\n                cluster_log_liks = []\n                for alpha in range(self.n_clusters):\n                    log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n                    cluster_log_liks.append(np.log(q[i, alpha] + 1e-8) + log_lik)\n                current_log_lik += logsumexp(cluster_log_liks)\n\n            if np.isnan(current_log_lik):\n                print(\"Warning: log likelihood is nan, stopping early\")\n                break\n\n            if iteration &gt; 0 and abs(current_log_lik - prev_log_lik) &lt; tol:\n                print(f\"Converged at iteration {iteration}\")\n                break\n\n            prev_log_lik = current_log_lik\n            print(f\"Iteration {iteration}, log likelihood: {current_log_lik}\")\n\n        except Exception as e:\n            print(f\"Error at iteration {iteration}: {str(e)}\")\n            break\n\n    self._compute_final_weights(data)\n    return self\n</code></pre>"},{"location":"task_clustering/reference/#bmm_multitask_learning.task_clustering.MultiTask_Algo.MultiTaskNNGating.m_step","title":"<code>m_step(data, task_features)</code>","text":"<p>Maximization step with regularization</p> Source code in <code>bmm_multitask_learning/task_clustering/MultiTask_Algo.py</code> <pre><code>def m_step(self, data, task_features):\n    \"\"\"Maximization step with regularization\"\"\"\n\n    # Optimize W and sigma\n    def objective(params):\n        W = params[:self.n_hidden * (self.n_input + 1)].reshape(self.n_hidden, self.n_input + 1)\n        log_sigma = params[-1]\n        sigma = np.exp(log_sigma)\n\n        self.W = W\n        self.sigma = max(sigma, 1e-8)\n\n        total_log_lik = 0.0\n        for i, (X_i, y_i) in enumerate(data):\n            for alpha in range(self.n_clusters):\n                log_lik = self._compute_task_log_likelihood(X_i, y_i, alpha)\n                total_log_lik += self.z[i, alpha] * log_lik\n\n        return -total_log_lik if np.isfinite(total_log_lik) else np.inf\n\n    # Initial parameters with bounds\n    initial_params = np.concatenate([\n        self.W.flatten(),\n        [np.log(self.sigma)]\n    ])\n\n    bounds = [(None, None)] * len(initial_params)\n    bounds[-1] = (np.log(1e-8), None)  # sigma &gt; 1e-8\n\n    result = minimize(\n        objective,\n        initial_params,\n        method='L-BFGS-B',\n        bounds=bounds,\n        options={'maxiter': 50, 'disp': True}\n    )\n\n    # Update parameters\n    opt_params = result.x\n    W_size = self.n_hidden * (self.n_input + 1)\n    self.W = opt_params[:W_size].reshape(self.n_hidden, self.n_input + 1)\n    self.sigma = max(np.exp(opt_params[-1]), 1e-8)\n\n    # Update cluster parameters with regularization\n    for alpha in range(self.n_clusters):\n        sum_z = np.sum(self.z[:, alpha])\n        if sum_z &gt; 1e-8:\n            # Update m_\u03b1\n            weighted_R = np.zeros(self.n_hidden + 1)\n            weighted_Q = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n\n            for i, (X_i, y_i) in enumerate(data):\n                h_i = self.compute_hidden_activations(X_i)\n                L = cholesky(self.Sigma[alpha] + 1e-6 * np.eye(self.n_hidden + 1), lower=True)\n                Sigma_inv = solve_triangular(L, np.eye(self.n_hidden + 1), lower=True)\n                Sigma_inv = np.dot(Sigma_inv.T, Sigma_inv)\n\n                Q_i = (1 / max(self.sigma ** 2, 1e-8)) * np.dot(h_i.T, h_i) + Sigma_inv\n                R_i = (1 / max(self.sigma ** 2, 1e-8)) * np.dot(h_i.T, y_i) + np.dot(Sigma_inv, self.m[alpha])\n\n                weighted_R += self.z[i, alpha] * R_i\n                weighted_Q += self.z[i, alpha] * Q_i\n\n            try:\n                self.m[alpha] = np.linalg.solve(weighted_Q + 1e-6 * np.eye(self.n_hidden + 1), weighted_R)\n            except:\n                pass\n\n                # Update \u03a3_\u03b1 with regularization\n                weighted_cov = np.zeros((self.n_hidden + 1, self.n_hidden + 1))\n                for i, (X_i, y_i) in enumerate(data):\n                    h_i = self.compute_hidden_activations(X_i)\n                    A_i = self._compute_map_estimate(X_i, y_i, alpha)\n                    diff = A_i - self.m[alpha]\n                    weighted_cov += self.z[i, alpha] * np.outer(diff, diff)\n\n                self.Sigma[alpha] = weighted_cov / sum_z + 1e-6 * np.eye(self.n_hidden + 1)\n\n            # Update gating parameters U\n            if self.n_clusters &gt; 1:\n                task_features = np.atleast_2d(task_features)\n                if task_features.shape[0] == 1 and self.n_tasks &gt; 1:\n                    task_features = np.repeat(task_features, self.n_tasks, axis=0)\n\n                lr = LogisticRegression(\n                    multi_class='multinomial',\n                    solver='lbfgs',\n                    fit_intercept=False,\n                    max_iter=100,\n                    penalty='l2',\n                    C=1.0\n                )\n                try:\n                    lr.fit(task_features, self.get_cluster_assignments(), sample_weight=np.max(self.z, axis=1))\n                    self.U = lr.coef_\n                except:\n                    pass\n</code></pre>"},{"location":"variational/elementary/","title":"Variational multitask learning elementary example","text":"<p>This is a practical demonstration on how to use <code>variational</code> subpackage on a simple classification example. We are going to solve 3 classification tasks with logistic regression as a model. Additionally, we will add prior on the weight so the tasks become bayessian. Two of the tasks will be probabilsitcally connected, the last will have no probabilistic connections with others.</p> <p>First, we will apply variational principle to learn each task individually. We will use <code>pyro</code> package to automatically compute ELBO and minimize it.</p> <p>Secondly, we will use <code>variational</code> subpackage and learn 3 tasks alltogether. Learning here is the same ELBO minimizing, but for special variational structure - see doc for more details.</p> <p>Lastly, we will compare two approaches in accuracy terms.</p> In\u00a0[1]: Copied! <pre>from typing import Optional\n\nfrom pipe import select\n\nfrom omegaconf import OmegaConf\n\nimport pandas as pd\nimport numpy as np\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, random_split, StackDataset\nfrom torch import optim\nfrom torch import distributions as distr\n\nimport pyro\nimport pyro.nn as pnn\nimport pyro.distributions as pdistr\nfrom pyro.distributions import Delta\nfrom pyro.infer import Trace_ELBO\nfrom pyro.infer.autoguide import AutoNormal\n\nimport lightning as L\nfrom lightning.pytorch.loggers import TensorBoardLogger\nfrom lightning.pytorch.callbacks import EarlyStopping\n\nfrom torchmetrics.classification import Accuracy\n\nfrom bmm_multitask_learning.variational.elbo import MultiTaskElbo\nfrom bmm_multitask_learning.variational.distr import build_predictive\n</pre> from typing import Optional  from pipe import select  from omegaconf import OmegaConf  import pandas as pd import numpy as np  import plotly.express as px import plotly.graph_objects as go  import matplotlib.pyplot as plt  import torch import torch.nn as nn from torch.utils.data import DataLoader, random_split, StackDataset from torch import optim from torch import distributions as distr  import pyro import pyro.nn as pnn import pyro.distributions as pdistr from pyro.distributions import Delta from pyro.infer import Trace_ELBO from pyro.infer.autoguide import AutoNormal  import lightning as L from lightning.pytorch.loggers import TensorBoardLogger from lightning.pytorch.callbacks import EarlyStopping  from torchmetrics.classification import Accuracy  from bmm_multitask_learning.variational.elbo import MultiTaskElbo from bmm_multitask_learning.variational.distr import build_predictive In\u00a0[2]: Copied! <pre>%load_ext tensorboard\n</pre> %load_ext tensorboard <p>The experiment is configurable via yaml config.</p> In\u00a0[3]: Copied! <pre>config = OmegaConf.load(\"config.yaml\")\n</pre> config = OmegaConf.load(\"config.yaml\") In\u00a0[4]: Copied! <pre>torch.manual_seed(config.seed);\n</pre> torch.manual_seed(config.seed); <p>For each task we generate 2-dimensional inputs $X$ where each individual input $x$ is</p> <p>$$     x \\sim \\mathcal{N}(\\mu, \\sigma_x^2) $$</p> <p>For task 1 and 2 these are very close to each other.</p> <p>Then we generate logistic regression parameter $w$ as</p> <p>$$     w_1 \\sim \\mathcal{N}(\\mathbb{E}[X_2], \\sigma_w^2) \\\\      w_2 \\sim \\mathcal{N}(\\mathbb{E}[X_1], \\sigma_w^2) \\\\     w_3 \\sim \\mathcal{N}(\\mathbf{1}, \\sigma_w^2) $$</p> <p>Because $X_1$ and $X_2$ are close, distribuitions for $w_1$ and $w_2$ will be close too. This connection can be utilized by variational multitask approach.</p> <p>Finally, label $y$ for input $x$ generated as</p> <p>$$     y \\sim \\text{Bern}(\\sigma(x^{T}w)) $$</p> <p>Data generation with given schema is defined in data.py</p> In\u00a0[5]: Copied! <pre>from data import build_linked_datasets, build_solo_dataset\n\nNUM_MODELS = 3\n\n\ndatasets = [*build_linked_datasets(config.size, config.dim), build_solo_dataset(config.size, config.dim)]\n# extract w\nw_list = list(\n    datasets | select(lambda w_dataset: w_dataset[0])\n)\n# extract (X, y) pairs\ndatasets = list(\n    datasets |\n    select(lambda w_dataset: w_dataset[1]) |\n    select(lambda d: random_split(d, [1 - config.test_ratio, config.test_ratio]))\n)\n\ntrain_datasets, test_datasets = zip(*datasets)\n</pre> from data import build_linked_datasets, build_solo_dataset  NUM_MODELS = 3   datasets = [*build_linked_datasets(config.size, config.dim), build_solo_dataset(config.size, config.dim)] # extract w w_list = list(     datasets | select(lambda w_dataset: w_dataset[0]) ) # extract (X, y) pairs datasets = list(     datasets |     select(lambda w_dataset: w_dataset[1]) |     select(lambda d: random_split(d, [1 - config.test_ratio, config.test_ratio])) )  train_datasets, test_datasets = zip(*datasets) <p>Let's vizualize generated inputs and $w$ vectors for each task</p> In\u00a0[6]: Copied! <pre>points_df = []\n\nfor i, dataset in enumerate(train_datasets):\n    X = dataset.dataset.tensors[0].numpy()\n    cur_df = pd.DataFrame(X, columns=[\"x1\", \"x2\"])\n    cur_df[\"dataset\"] = str(i)\n    points_df.append(cur_df)\npoints_df = pd.concat(points_df, axis=0)\n\npoints_df.head()\n</pre> points_df = []  for i, dataset in enumerate(train_datasets):     X = dataset.dataset.tensors[0].numpy()     cur_df = pd.DataFrame(X, columns=[\"x1\", \"x2\"])     cur_df[\"dataset\"] = str(i)     points_df.append(cur_df) points_df = pd.concat(points_df, axis=0)  points_df.head() Out[6]: x1 x2 dataset 0 -0.094634 -2.335958 0 1 10.453235 3.087356 0 2 -2.813742 -2.073199 0 3 -6.502639 -0.509825 0 4 -5.492137 -2.334638 0 In\u00a0[7]: Copied! <pre>plane_df = []\nfor i, w in enumerate(w_list):\n    w = w.numpy()\n    cur_df = pd.DataFrame(np.linspace(-20, 20, 10)[:, None] * w[None, :], columns=[\"x1\", \"x2\"])\n    cur_df[\"dataset\"] = str(i+1)\n    plane_df.append(cur_df)\nplane_df = pd.concat(plane_df, axis=0)\n\nplane_df.head()\n</pre> plane_df = [] for i, w in enumerate(w_list):     w = w.numpy()     cur_df = pd.DataFrame(np.linspace(-20, 20, 10)[:, None] * w[None, :], columns=[\"x1\", \"x2\"])     cur_df[\"dataset\"] = str(i+1)     plane_df.append(cur_df) plane_df = pd.concat(plane_df, axis=0)  plane_df.head() Out[7]: x1 x2 dataset 0 9.379683 -1.807698 1 1 7.295309 -1.405987 1 2 5.210935 -1.004277 1 3 3.126561 -0.602566 1 4 1.042187 -0.200855 1 In\u00a0[8]: Copied! <pre>fig_points = px.scatter(points_df, x=\"x1\", y=\"x2\", color=\"dataset\", symbol=\"dataset\")\nfig_plane = px.line(plane_df, x=\"x1\", y=\"x2\", color=\"dataset\")\nfig_plane.update_traces(line=dict(width=3))\nfig = go.Figure(data=fig_points.data + fig_plane.data)\nfig.show()\n</pre> fig_points = px.scatter(points_df, x=\"x1\", y=\"x2\", color=\"dataset\", symbol=\"dataset\") fig_plane = px.line(plane_df, x=\"x1\", y=\"x2\", color=\"dataset\") fig_plane.update_traces(line=dict(width=3)) fig = go.Figure(data=fig_points.data + fig_plane.data) fig.show() <p>As we can see inputs for tasks 1 and 2 are indeed close. Because of small $\\sigma_w$ we got $w_1$ and $w_2$ also close.</p> In\u00a0[9]: Copied! <pre># here we define probabilistic model of each task using pyro\nclass SoloModel(pnn.PyroModule):\n    def __init__(\n        self,\n        dim: int = 2,\n        num_data_samples: Optional[int] = None\n    ):\n        super().__init__()\n\n        self.num_data_samples = num_data_samples\n\n        # set parametric prior on w\n        self.w_loc = pnn.PyroParam(torch.zeros((dim, )))\n        self.log_w_scale = pnn.PyroParam(torch.zeros((dim, )))\n        self.w = pnn.PyroSample(\n            lambda self: pdistr.Normal(self.w_loc, torch.exp(self.log_w_scale)).to_event(1)\n        )\n\n    def forward(self, X: torch.Tensor, y: torch.Tensor = None):\n        batch_size = X.shape[0]\n        if self.num_data_samples:\n            size = self.num_data_samples\n            subsample_size = batch_size\n        else:\n            size = batch_size\n            subsample_size = None\n\n        p = torch.sigmoid(X.matmul(self.w))\n        with pyro.plate(\"data_batch\", size=size, subsample_size=subsample_size):\n            pyro.sample(\"y\", pdistr.Bernoulli(p), obs=y)\n</pre> # here we define probabilistic model of each task using pyro class SoloModel(pnn.PyroModule):     def __init__(         self,         dim: int = 2,         num_data_samples: Optional[int] = None     ):         super().__init__()          self.num_data_samples = num_data_samples          # set parametric prior on w         self.w_loc = pnn.PyroParam(torch.zeros((dim, )))         self.log_w_scale = pnn.PyroParam(torch.zeros((dim, )))         self.w = pnn.PyroSample(             lambda self: pdistr.Normal(self.w_loc, torch.exp(self.log_w_scale)).to_event(1)         )      def forward(self, X: torch.Tensor, y: torch.Tensor = None):         batch_size = X.shape[0]         if self.num_data_samples:             size = self.num_data_samples             subsample_size = batch_size         else:             size = batch_size             subsample_size = None          p = torch.sigmoid(X.matmul(self.w))         with pyro.plate(\"data_batch\", size=size, subsample_size=subsample_size):             pyro.sample(\"y\", pdistr.Bernoulli(p), obs=y) <p>We are going to use <code>lightning</code> to perform training and logging</p> In\u00a0[10]: Copied! <pre>class LitSoloModel(L.LightningModule):\n    def __init__(\n        self,\n        elbo_f: pyro.infer.elbo.ELBOModule,\n        predictive: pyro.infer.Predictive,\n        num_data_samples: int,\n    ):\n        super().__init__()\n\n        self.num_data_samples = num_data_samples\n        self.accuracy_computer = Accuracy('binary')\n\n        self.elbo_f = elbo_f\n        self.model: SoloModel = elbo_f.model\n        self.guide = elbo_f.guide\n\n        self.predictive = predictive\n\n    def training_step(self, batch: tuple[torch.Tensor], batch_idx: int):\n        X, y = batch\n\n        elbo_loss = self.elbo_f(X, y)\n\n        self.log(\"Train/ELBO\", elbo_loss, prog_bar=True)\n\n        return elbo_loss\n    \n    def validation_step(self, batch: tuple[torch.Tensor], batch_idx: int):\n        X, y = batch\n\n        y_pred = (self.predictive(X, y=None)[\"y\"].mean(dim=0) &gt; 0.5).to(torch.float32)\n\n        self.accuracy_computer.update(y_pred, y)\n\n    def on_validation_epoch_end(self):\n        self.log(\"Test/Accuracy\", self.accuracy_computer.compute())\n        self.accuracy_computer.reset()  \n\n    def configure_optimizers(self):\n        return optim.Adam(self.elbo_f.parameters())\n</pre> class LitSoloModel(L.LightningModule):     def __init__(         self,         elbo_f: pyro.infer.elbo.ELBOModule,         predictive: pyro.infer.Predictive,         num_data_samples: int,     ):         super().__init__()          self.num_data_samples = num_data_samples         self.accuracy_computer = Accuracy('binary')          self.elbo_f = elbo_f         self.model: SoloModel = elbo_f.model         self.guide = elbo_f.guide          self.predictive = predictive      def training_step(self, batch: tuple[torch.Tensor], batch_idx: int):         X, y = batch          elbo_loss = self.elbo_f(X, y)          self.log(\"Train/ELBO\", elbo_loss, prog_bar=True)          return elbo_loss          def validation_step(self, batch: tuple[torch.Tensor], batch_idx: int):         X, y = batch          y_pred = (self.predictive(X, y=None)[\"y\"].mean(dim=0) &gt; 0.5).to(torch.float32)          self.accuracy_computer.update(y_pred, y)      def on_validation_epoch_end(self):         self.log(\"Test/Accuracy\", self.accuracy_computer.compute())         self.accuracy_computer.reset()        def configure_optimizers(self):         return optim.Adam(self.elbo_f.parameters()) <p>Now train individual models</p> In\u00a0[11]: Copied! <pre>for i in range(NUM_MODELS):\n    print(f\"Training model {i}\\n\")\n\n    model = SoloModel(config.dim, config.size)\n    guide = AutoNormal(model)\n\n    # num elbo particles is equivallent to variational multitask\n    num_elbo_particles = config.mt_elbo.classifier_num_particles * config.mt_elbo.latent_num_particles\n    \n    elbo_f = Trace_ELBO(num_elbo_particles)(model, guide)\n\n    # All relevant parameters need to be initialized before ``configure_optimizer`` is called.\n    # Since we used AutoNormal guide our parameters have not be initialized yet.\n    # Therefore we initialize the model and guide by running one mini-batch through the loss.\n    mini_batch = next(iter(DataLoader(train_datasets[0], batch_size=1)))\n    elbo_f(*mini_batch)\n\n    # this choice of num_predictive_particles is rather balancing\n    num_predictive_particles = num_elbo_particles\n    predictive = pyro.infer.Predictive(model, guide=guide, num_samples=num_predictive_particles)\n\n    lit_model = LitSoloModel(elbo_f, predictive, config.size)\n \n    train_dataloader = DataLoader(train_datasets[i], batch_size=config.batch_size, shuffle=True)\n    test_dataloader = DataLoader(test_datasets[i], batch_size=config.batch_size)\n\n    logger = TensorBoardLogger(\"mt_logs/solo\", name=f\"solo_{i}\")\n\n    callbacks = [\n        EarlyStopping(monitor=\"Train/ELBO\", min_delta=1e-3, patience=10, mode=\"min\")\n    ]\n\n    trainer = L.Trainer(logger=logger, callbacks=callbacks, **dict(config.trainer))\n    trainer.fit(lit_model, train_dataloader, test_dataloader)\n</pre> for i in range(NUM_MODELS):     print(f\"Training model {i}\\n\")      model = SoloModel(config.dim, config.size)     guide = AutoNormal(model)      # num elbo particles is equivallent to variational multitask     num_elbo_particles = config.mt_elbo.classifier_num_particles * config.mt_elbo.latent_num_particles          elbo_f = Trace_ELBO(num_elbo_particles)(model, guide)      # All relevant parameters need to be initialized before ``configure_optimizer`` is called.     # Since we used AutoNormal guide our parameters have not be initialized yet.     # Therefore we initialize the model and guide by running one mini-batch through the loss.     mini_batch = next(iter(DataLoader(train_datasets[0], batch_size=1)))     elbo_f(*mini_batch)      # this choice of num_predictive_particles is rather balancing     num_predictive_particles = num_elbo_particles     predictive = pyro.infer.Predictive(model, guide=guide, num_samples=num_predictive_particles)      lit_model = LitSoloModel(elbo_f, predictive, config.size)       train_dataloader = DataLoader(train_datasets[i], batch_size=config.batch_size, shuffle=True)     test_dataloader = DataLoader(test_datasets[i], batch_size=config.batch_size)      logger = TensorBoardLogger(\"mt_logs/solo\", name=f\"solo_{i}\")      callbacks = [         EarlyStopping(monitor=\"Train/ELBO\", min_delta=1e-3, patience=10, mode=\"min\")     ]      trainer = L.Trainer(logger=logger, callbacks=callbacks, **dict(config.trainer))     trainer.fit(lit_model, train_dataloader, test_dataloader) <pre>Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\nGPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/Users/sem-k32/10 sem/bmm-multitask-learning/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: PossibleUserWarning:\n\nGPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n\n\n  | Name              | Type           | Params | Mode \n-------------------------------------------------------------\n0 | accuracy_computer | BinaryAccuracy | 0      | train\n1 | elbo_f            | ELBOModule     | 8      | train\n2 | model             | SoloModel      | 4      | train\n3 | guide             | AutoNormal     | 4      | train\n4 | predictive        | Predictive     | 8      | train\n-------------------------------------------------------------\n8         Trainable params\n0         Non-trainable params\n8         Total params\n0.000     Total estimated model params size (MB)\n7         Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Training model 0\n\n</pre> <pre>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>/Users/sem-k32/10 sem/bmm-multitask-learning/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: PossibleUserWarning:\n\nThe 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n\n/Users/sem-k32/10 sem/bmm-multitask-learning/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: PossibleUserWarning:\n\nThe 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n\n</pre> <pre>Training: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=20` reached.\nUsing default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\nGPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\n  | Name              | Type           | Params | Mode \n-------------------------------------------------------------\n0 | accuracy_computer | BinaryAccuracy | 0      | train\n1 | elbo_f            | ELBOModule     | 8      | train\n2 | model             | SoloModel      | 4      | train\n3 | guide             | AutoNormal     | 4      | train\n4 | predictive        | Predictive     | 8      | train\n-------------------------------------------------------------\n8         Trainable params\n0         Non-trainable params\n8         Total params\n0.000     Total estimated model params size (MB)\n7         Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Training model 1\n\n</pre> <pre>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Training: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\nGPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\n  | Name              | Type           | Params | Mode \n-------------------------------------------------------------\n0 | accuracy_computer | BinaryAccuracy | 0      | train\n1 | elbo_f            | ELBOModule     | 8      | train\n2 | model             | SoloModel      | 4      | train\n3 | guide             | AutoNormal     | 4      | train\n4 | predictive        | Predictive     | 8      | train\n-------------------------------------------------------------\n8         Trainable params\n0         Non-trainable params\n8         Total params\n0.000     Total estimated model params size (MB)\n7         Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Training model 2\n\n</pre> <pre>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Training: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=20` reached.\n</pre> <p>Metrics are saved in the tensorboard</p> In\u00a0[27]: Copied! <pre>%tensorboard --logdir mt_logs/solo\n</pre> %tensorboard --logdir mt_logs/solo <pre>Reusing TensorBoard on port 6006 (pid 30339), started 0:00:16 ago. (Use '!kill 30339' to kill it.)</pre> <p>First, register KL computation for $\\delta$-distribution. Here we don't have state variables and won't compute its KL at all.</p> In\u00a0[13]: Copied! <pre>@distr.kl.register_kl(Delta, Delta)\ndef kl_delta_delta(d1: Delta, d2: Delta):\n    return torch.zeros(d1.batch_shape)\n    # this is how it should be\n    # return torch.zeros(d1.batch_shape) if torch.allclose(d1.v, d2.v) else torch.full(torch.inf, d1.batch_shape)\n</pre> @distr.kl.register_kl(Delta, Delta) def kl_delta_delta(d1: Delta, d2: Delta):     return torch.zeros(d1.batch_shape)     # this is how it should be     # return torch.zeros(d1.batch_shape) if torch.allclose(d1.v, d2.v) else torch.full(torch.inf, d1.batch_shape) <p>Define batched distributions for tasks. Here we assume that latents come with (batch_size, num_latent_particles, ...) shape, classifiers come with (num_classifier_samples, ...) shape</p> In\u00a0[14]: Copied! <pre># same for all tasks\ndef target_distr(Z: torch.Tensor, W: torch.Tensor) -&gt; distr.Distribution:\n    return distr.Bernoulli(logits=torch.tensordot(Z, W, dims=[[-1], [-1]]))\n\n# same for all tasks\ndef predictive_distr(Z: torch.Tensor, W: torch.Tensor) -&gt; distr.Distribution:\n    return distr.Bernoulli(logits=torch.tensordot(Z, W, dims=[[-1], [-1]]).flatten(1, 2))\n</pre> # same for all tasks def target_distr(Z: torch.Tensor, W: torch.Tensor) -&gt; distr.Distribution:     return distr.Bernoulli(logits=torch.tensordot(Z, W, dims=[[-1], [-1]]))  # same for all tasks def predictive_distr(Z: torch.Tensor, W: torch.Tensor) -&gt; distr.Distribution:     return distr.Bernoulli(logits=torch.tensordot(Z, W, dims=[[-1], [-1]]).flatten(1, 2)) In\u00a0[15]: Copied! <pre>task_distrs = [target_distr for _ in range(NUM_MODELS)]\ntask_num_samples = list(train_datasets | select(len))\n</pre> task_distrs = [target_distr for _ in range(NUM_MODELS)] task_num_samples = list(train_datasets | select(len)) In\u00a0[16]: Copied! <pre># we don't have latents here, but we need it formaly as distribution\ndef latent_distr(X: torch.Tensor) -&gt; distr.Distribution:\n    return Delta(X, event_dim=1)\n</pre> # we don't have latents here, but we need it formaly as distribution def latent_distr(X: torch.Tensor) -&gt; distr.Distribution:     return Delta(X, event_dim=1) In\u00a0[17]: Copied! <pre>class NormalLogits(distr.Normal):\n    \"\"\"Normal distribution with scale parametrized via logits\n    \"\"\"\n    def __init__(self, loc, logit, validate_args=None):\n        self.logit = logit\n        super().__init__(loc, torch.exp(logit), validate_args)\n\n    def __getattribute__(self, name):\n        if name == \"scale\":\n            return self.logit.exp()\n        else:\n            return super().__getattribute__(name)\n\n\n# parametric variational distr for classifiers\nclassifier_distrs_params = {}\nclassifier_distrs = []\nfor i in range(NUM_MODELS):\n    # set inital values for distribution's parameters\n    loc, scale_logit = nn.Parameter(torch.zeros((config.dim, ))), nn.Parameter(torch.zeros((config.dim, )))\n    classifier_distrs_params.update({\n        f\"distr_{i}\": [loc, scale_logit]\n    })\n    classifier_distrs.append(\n        distr.Independent(\n            NormalLogits(loc, scale_logit),\n            reinterpreted_batch_ndims=1\n        )\n    )\n# parametric variational distr for latents\nlatent_distrs = [latent_distr for _ in range(NUM_MODELS)]\n</pre> class NormalLogits(distr.Normal):     \"\"\"Normal distribution with scale parametrized via logits     \"\"\"     def __init__(self, loc, logit, validate_args=None):         self.logit = logit         super().__init__(loc, torch.exp(logit), validate_args)      def __getattribute__(self, name):         if name == \"scale\":             return self.logit.exp()         else:             return super().__getattribute__(name)   # parametric variational distr for classifiers classifier_distrs_params = {} classifier_distrs = [] for i in range(NUM_MODELS):     # set inital values for distribution's parameters     loc, scale_logit = nn.Parameter(torch.zeros((config.dim, ))), nn.Parameter(torch.zeros((config.dim, )))     classifier_distrs_params.update({         f\"distr_{i}\": [loc, scale_logit]     })     classifier_distrs.append(         distr.Independent(             NormalLogits(loc, scale_logit),             reinterpreted_batch_ndims=1         )     ) # parametric variational distr for latents latent_distrs = [latent_distr for _ in range(NUM_MODELS)] In\u00a0[18]: Copied! <pre># temperature must decrease over steps\ntemp_scheduler = lambda step: 1. / torch.sqrt(torch.tensor(step + 1))\n</pre> # temperature must decrease over steps temp_scheduler = lambda step: 1. / torch.sqrt(torch.tensor(step + 1)) In\u00a0[19]: Copied! <pre># create variational multitask elbo module\nmt_elbo = MultiTaskElbo(\n    task_distrs,\n    task_num_samples,\n    classifier_distrs,\n    latent_distrs,\n    temp_scheduler=temp_scheduler,\n    **dict(config.mt_elbo)\n)\n</pre> # create variational multitask elbo module mt_elbo = MultiTaskElbo(     task_distrs,     task_num_samples,     classifier_distrs,     latent_distrs,     temp_scheduler=temp_scheduler,     **dict(config.mt_elbo) ) <p>We are going to use <code>lightning</code> to perform training and logging</p> In\u00a0[20]: Copied! <pre>class LitMtModel(L.LightningModule):\n    def __init__(\n        self,\n        mt_elbo: MultiTaskElbo\n    ):\n        super().__init__()\n\n        num_tasks = mt_elbo.num_tasks\n        self.accuracy_computers = [Accuracy('binary') for _ in range(num_tasks)]\n        self.mt_elbo = mt_elbo\n\n        self.distr_params = nn.ParameterList()\n        for param_list in classifier_distrs_params.values():\n            self.distr_params.extend(\n                param_list\n            )\n        \n\n    def training_step(self, batch: tuple[tuple[torch.Tensor]], batch_idx: int):\n        mt_loss_dict = self.mt_elbo(*list(zip(*batch)), step=self.global_step)\n        self.log_dict(mt_loss_dict, prog_bar=True)\n\n        # DEBUG\n        return mt_loss_dict[\"elbo\"]\n    \n    def on_train_batch_end(self, outputs, batch, batch_idx):\n        with torch.no_grad():\n            for distr_name, distr_params in classifier_distrs_params.items():\n                params_grad_norm = sum(distr_params | select(lambda param: param.grad.norm()))\n                self.log(f\"{distr_name}_grad\", params_grad_norm)\n\n    def on_train_epoch_end(self):\n        # log mixing\n\n        fig, ax = plt.subplots()\n        cax = ax.matshow(self.mt_elbo.latent_mixings_params.detach().numpy())\n        fig.colorbar(cax)\n        self.logger.experiment.add_figure(\"Latent_mixing\", fig, self.global_step)\n\n        fig, ax = plt.subplots()\n        cax = ax.matshow(self.mt_elbo.classifier_mixings_params.detach().numpy())\n        fig.colorbar(cax)\n        self.logger.experiment.add_figure(\"Classifier_mixing\", fig, self.global_step)\n\n    def validation_step(self, batch: tuple[tuple[torch.Tensor]], batch_idx: int):\n        for i, (X, y) in enumerate(batch):\n            NUM_PREDICTIVE_SAMPLES = 10\n            cur_predictive = build_predictive(\n                predictive_distr,\n                classifier_distrs[i],\n                latent_distrs[i],\n                X,\n                config.mt_elbo.classifier_num_particles,\n                config.mt_elbo.latent_num_particles\n            )\n            y_pred = (cur_predictive.sample((NUM_PREDICTIVE_SAMPLES, )).mean(dim=0) &gt; 0.5).float()\n            self.accuracy_computers[i].update(y_pred, y)\n\n    def on_validation_epoch_end(self):\n        for i, accuracy_computer in enumerate(self.accuracy_computers):\n            self.log(f\"Test/Accuracy_{i}\", accuracy_computer.compute())\n            accuracy_computer.reset()\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=1e-3)\n</pre> class LitMtModel(L.LightningModule):     def __init__(         self,         mt_elbo: MultiTaskElbo     ):         super().__init__()          num_tasks = mt_elbo.num_tasks         self.accuracy_computers = [Accuracy('binary') for _ in range(num_tasks)]         self.mt_elbo = mt_elbo          self.distr_params = nn.ParameterList()         for param_list in classifier_distrs_params.values():             self.distr_params.extend(                 param_list             )               def training_step(self, batch: tuple[tuple[torch.Tensor]], batch_idx: int):         mt_loss_dict = self.mt_elbo(*list(zip(*batch)), step=self.global_step)         self.log_dict(mt_loss_dict, prog_bar=True)          # DEBUG         return mt_loss_dict[\"elbo\"]          def on_train_batch_end(self, outputs, batch, batch_idx):         with torch.no_grad():             for distr_name, distr_params in classifier_distrs_params.items():                 params_grad_norm = sum(distr_params | select(lambda param: param.grad.norm()))                 self.log(f\"{distr_name}_grad\", params_grad_norm)      def on_train_epoch_end(self):         # log mixing          fig, ax = plt.subplots()         cax = ax.matshow(self.mt_elbo.latent_mixings_params.detach().numpy())         fig.colorbar(cax)         self.logger.experiment.add_figure(\"Latent_mixing\", fig, self.global_step)          fig, ax = plt.subplots()         cax = ax.matshow(self.mt_elbo.classifier_mixings_params.detach().numpy())         fig.colorbar(cax)         self.logger.experiment.add_figure(\"Classifier_mixing\", fig, self.global_step)      def validation_step(self, batch: tuple[tuple[torch.Tensor]], batch_idx: int):         for i, (X, y) in enumerate(batch):             NUM_PREDICTIVE_SAMPLES = 10             cur_predictive = build_predictive(                 predictive_distr,                 classifier_distrs[i],                 latent_distrs[i],                 X,                 config.mt_elbo.classifier_num_particles,                 config.mt_elbo.latent_num_particles             )             y_pred = (cur_predictive.sample((NUM_PREDICTIVE_SAMPLES, )).mean(dim=0) &gt; 0.5).float()             self.accuracy_computers[i].update(y_pred, y)      def on_validation_epoch_end(self):         for i, accuracy_computer in enumerate(self.accuracy_computers):             self.log(f\"Test/Accuracy_{i}\", accuracy_computer.compute())             accuracy_computer.reset()      def configure_optimizers(self):         return optim.Adam(self.parameters(), lr=1e-3) In\u00a0[21]: Copied! <pre>lit_mt_model = LitMtModel(mt_elbo)\n</pre> lit_mt_model = LitMtModel(mt_elbo) In\u00a0[22]: Copied! <pre># stack task datasets\nunified_train_dataset = StackDataset(*train_datasets)\nunified_test_dataset = StackDataset(*test_datasets)\n\nmt_train_dataloader = DataLoader(unified_train_dataset, config.batch_size, shuffle=True)\nmt_test_dataloader = DataLoader(unified_test_dataset, config.batch_size, shuffle=False)\n</pre> # stack task datasets unified_train_dataset = StackDataset(*train_datasets) unified_test_dataset = StackDataset(*test_datasets)  mt_train_dataloader = DataLoader(unified_train_dataset, config.batch_size, shuffle=True) mt_test_dataloader = DataLoader(unified_test_dataset, config.batch_size, shuffle=False) In\u00a0[23]: Copied! <pre>logger = TensorBoardLogger(\"mt_logs\", name=\"multitask\")\n\ncallbacks = [\n    EarlyStopping(monitor=\"elbo\", min_delta=1e-3, patience=10, mode=\"min\")\n]\n</pre> logger = TensorBoardLogger(\"mt_logs\", name=\"multitask\")  callbacks = [     EarlyStopping(monitor=\"elbo\", min_delta=1e-3, patience=10, mode=\"min\") ] In\u00a0[24]: Copied! <pre>trainer = L.Trainer(logger=logger, callbacks=callbacks, **dict(config.trainer))\ntrainer.fit(lit_mt_model, mt_train_dataloader, mt_test_dataloader)\n</pre> trainer = L.Trainer(logger=logger, callbacks=callbacks, **dict(config.trainer)) trainer.fit(lit_mt_model, mt_train_dataloader, mt_test_dataloader) <pre>Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\nGPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/Users/sem-k32/10 sem/bmm-multitask-learning/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: PossibleUserWarning:\n\nGPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n\n\n  | Name         | Type          | Params | Mode \n-------------------------------------------------------\n0 | mt_elbo      | MultiTaskElbo | 9      | train\n1 | distr_params | ParameterList | 12     | train\n-------------------------------------------------------\n21        Trainable params\n0         Non-trainable params\n21        Total params\n0.000     Total estimated model params size (MB)\n2         Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>/Users/sem-k32/10 sem/bmm-multitask-learning/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: PossibleUserWarning:\n\nThe 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n\n/Users/sem-k32/10 sem/bmm-multitask-learning/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: PossibleUserWarning:\n\nThe 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n\n</pre> <pre>Training: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Validation: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>`Trainer.fit` stopped: `max_epochs=20` reached.\n</pre> <p>Metrics are saved in the tensorboard</p> In\u00a0[26]: Copied! <pre>%tensorboard --logdir mt_logs/multitask\n</pre> %tensorboard --logdir mt_logs/multitask <pre>Reusing TensorBoard on port 6007 (pid 30356), started 0:00:04 ago. (Use '!kill 30356' to kill it.)</pre> <p>As we can see, the training is successful. From the vizualized classifier mixing paramters it is clear that the algorithm has found connection between task 1 and 2.</p> <p>Final accuracy metrics are equal within the error margin for solo and multitask models. Because the tasks are simple and synthetic, we have not seen the performance difference. But for more serious tasks with possible probabilistic connections the variational multitask may give significant boost.</p>"},{"location":"variational/elementary/#variational-multitask-learning-elementary-example","title":"Variational multitask learning elementary example\u00b6","text":""},{"location":"variational/elementary/#data","title":"Data\u00b6","text":""},{"location":"variational/elementary/#solo-models","title":"Solo models\u00b6","text":""},{"location":"variational/elementary/#variational-multitask-models","title":"Variational multitask models\u00b6","text":""},{"location":"variational/elementary/#solo-and-multitask-comparasion","title":"Solo and multitask comparasion\u00b6","text":""},{"location":"variational/hooks/","title":"Hooks","text":"In\u00a0[\u00a0]: Copied! <pre>import shutil\n</pre> import shutil In\u00a0[\u00a0]: Copied! <pre>def on_pre_build(*args, **kwargs):\n    shutil.copy2(\n        \"examples/variational/elementary/elementary.ipynb\", \n        \"docs/variational/elementary.ipynb\"\n    )\n</pre> def on_pre_build(*args, **kwargs):     shutil.copy2(         \"examples/variational/elementary/elementary.ipynb\",          \"docs/variational/elementary.ipynb\"     )"},{"location":"variational/intro/","title":"Theory of variational multitask learning and its python API","text":""},{"location":"variational/intro/#abstract-introduction","title":"Abstract introduction","text":"<p>Variational multitask learning is one of the multitask approaches that uses variational inference to define probabilistic connections between tasks and do statistical inference on them (a.k.a. learning). This material is a short adaptation of the original paper<sup>1</sup>. It is assummed that the reader is familiar with bayessian statistics and variational inderence<sup>2</sup>. </p> <p>Imagine you have only one task to predict stochastic output \\(y\\) by given input \\(\\mathbf{x}\\). Our approach always assumes specific relation between \\(y\\) and \\(\\mathbf{x}\\). Namely, it will be bayessian: we introduce two hidden random variables. First is classifier \\(\\mathbf{w}\\) with it's prior distribution \\(p(\\mathbf{w})\\). Second is  latent \\(\\mathbf{z}\\) with its prior \\(p(\\mathbf{z} | \\mathbf{x})\\). The \\(y\\) is then modeled as \\(p(y | \\mathbf{z}, \\mathbf{w})\\), so there is implicit dependency on input.</p> <p>The typical situation is that you know your priors up to some parameters. To estimate them from given data \\(\\mathcal{D} = \\{ \\mathcal{X}, \\mathcal{Y}\\}\\) (\\(\\mathcal{X} = \\{\\mathbf{x}_j\\}_{j=1}^N\\), \\(\\mathcal{Y} = \\{y_j\\}_{j=1}^N\\)) and to build predictions for new inputs we will use variational inference. That means we itroduce variational distribtions \\(q(\\mathbf{w} | \\mathcal{D})\\) and \\(q(\\mathbf{z} | \\mathcal{D})\\) and maximize ELBO objective with respect to var. distributions and priors' parameters. The main method to do it is iterative algorithm called EM-algorithm.</p> <p>Now imagine you have \\(T\\) tasks of the form given above. Variational multitask introduce connection between tasks via classifier and latent priors of each task. Namely, the classifer prior of task \\(i\\) is now \\(p(\\mathbf{w}_i | \\mathcal{D}_{-i})\\) and the latent prior for sample \\(j\\) is \\(p(\\mathbf{z}_{ij} | \\mathcal{D}_{-i})\\). Here \\(\\mathcal{D}_{-i}\\) means samples form all tasks but \\(i\\)-th. As one can see this is quite general construction which requires some inital information about priors. To overcome these issues it was proposed to model priors in the following way</p> \\[ \\begin{aligned}     p(\\mathbf{w}_{i}|D_{-i}) &amp;= \\sum_{k \\not= i}\\alpha_{ki}q(\\mathbf{w}_{i}|D_{k}), \\\\     p(\\mathbf{z}_{ij} | \\mathbf{x}_{ij},D_{-i}) &amp;= \\sum_{k \\neq i}\\beta_{ki}q(\\mathbf{z}_{ij} | \\mathbf{x}_{ij}, D_{k}), \\end{aligned} \\] <p>so that prior for task \\(i\\) is a mixture of variational distributions for all other tasks. Mixture coefficients \\(\\alpha_{ki}\\) and \\(\\beta_{ki}\\) are generated from gumbel softmax, for example</p> \\[     \\alpha_{ki} = \\frac{\\exp((\\log \\pi_{ki} + g_{ki})/\\tau)}{\\sum_{s \\neq i} \\exp((\\log \\pi_{si} + g_{si})/\\tau)}, \\] <p>where \\(\\tau\\) is a temperature parameter, \\(g_{ki} \\sim Gumbel(0, 1)\\), \\(\\pi_{ki}\\) are learnable parameters that represent \"connection strength\" between task \\(k\\) and task \\(i\\). Note, this connection is not symmetrical.</p> <p>Overall, empirical objective to minimize is sampled ELBO:</p> \\[ \\begin{align*} \\hat{\\mathcal{L}}_{\\text{VMTL}}(\\theta, \\phi, \\alpha, \\beta)  &amp;= \\frac{1}{T} \\sum_{t=1}^{T} \\Bigg\\{     \\sum_{n=1}^{N_t} \\Bigg\\{     \\frac{1}{ML} \\sum_{\\ell=1}^{L} \\sum_{m=1}^{M} \\Big[     -\\log p(\\mathbf{y}_t|\\mathbf{z}_{t,n}^{(\\ell)}, \\mathbf{w}_t^{(m)})     \\Big] \\\\    &amp;\\quad + \\mathbb{D}_{\\text{KL}} \\Big[     q_\\phi(\\mathbf{z}_{t,n}|\\mathbf{x}_{t,n}) \\,\\Big|\\Big|\\,     \\sum_{i \\neq t} \\beta_{ti} q_\\phi(\\mathbf{z}_{t,n}|\\mathbf{x}_{t,n}, \\mathcal{D}_i)     \\Big] \\Bigg\\} \\\\    &amp;\\quad + \\mathbb{D}_{\\text{KL}} \\Big[    q_\\theta(\\mathbf{w}_t|\\mathcal{D}_i) \\,\\Big|\\Big|\\,    \\sum_{i \\neq t} \\alpha_{ti} q_\\theta(\\mathbf{w}_i|\\mathcal{D}_i)    \\Big] \\Bigg\\}. \\end{align*} \\] <p>Here we have introduced parameters \\(\\theta, \\phi\\) of the variational distributions; all latents are sampled accordingly \\(\\mathbf{z}_{t,n}\\sim q_{\\phi}(\\mathbf{z}_{t,n}|\\mathbf{x}_{t,n})\\) and \\(\\mathbf{w}_{t} \\sim q_{\\theta}(\\mathbf{w}_{t}|\\mathcal{D}_{t})\\). The loss structure is intuitively a sum of individual tasks' loss and flexible penalty for tasks disconnection. Due to learnable \\(\\alpha, \\beta\\) each task can connect to the most relevant neighbours.</p> <p>After learning procedure, one can obtain predictions using learnt variational distributions:</p> \\[ \\begin{align}     p(\\mathbf{y}_t | \\mathbf{x}_t) \\approx \\frac{1}{ML} \\sum_{l=1}^L \\sum_{m=1}^M p(\\mathbf{y}_t | \\mathbf{z}_t^{(l)}, \\mathbf{w}_t^{(m)}),  \\end{align}\\label{eq:pred} \\] <p>where we draw samples \\(\\mathbf{z}_t^{(l)} \\sim q_\\phi(\\mathbf{z}_t|\\mathbf{x}_t)\\) and  \\(\\mathbf{w}_t^{(m)} \\sim q_\\theta(\\mathbf{w}_t|\\mathcal{D}_t)\\). Given results follow from the general variational inference theory.</p>"},{"location":"variational/intro/#python-api","title":"Python API","text":"<p><code>variational</code> subpackage of the <code>bmm_multitask_learning</code> directly follows the theoretical concept of variational multitask learning. Implementaion is based on pytorch framework and its <code>distribution</code> package.</p> <p>The <code>elbo</code> module stores \\(\\alpha, \\beta\\) parameters and implements \\(\\hat{\\mathcal{L}}_{\\text{VMTL}}\\) objective. User is required to make and pass <code>Callable</code>s with conditional distributions \\(p(y | \\mathbf{z}, \\mathbf{w})\\), \\(q(\\mathbf{z} | x)\\) and unconditional \\(q(w)\\) as <code>Distriution</code> objects. User must also provide number of samples for each tasks to properly compute batched objective. User can also control temperature sheduling, number of latent samples used in empirical objective, and number of samples used in KL estimation (used if there is no explicit formula for given distributions).</p> <p>The <code>distr</code> module implements prediction distribution (1) and \\(\\mathbb{D}_{\\text{KL}}(\\cdot, \\cdot)\\) estimation via sampling. </p>"},{"location":"variational/intro/#elemtary-example","title":"Elemtary example","text":"<p>Go to the simple demostration of how <code>variational</code> subpackage can be used.</p> <ol> <li> <p>Shen, Jiayi, et al. \"Variational multi-task learning with gumbel-softmax priors.\" Advances in Neural Information Processing Systems 34 (2021): 21031-21042.\u00a0\u21a9</p> </li> <li> <p>Bishop, Christopher M. Pattern Recognition and Machine Learning. New York :Springer, 2006.\u00a0\u21a9</p> </li> </ol>"},{"location":"variational/reference/","title":"Reference for variational approach in multitask learning","text":""},{"location":"variational/reference/#bmm_multitask_learning.variational.distr","title":"<code>bmm_multitask_learning.variational.distr</code>","text":"<p>Utils for working with distributions</p>"},{"location":"variational/reference/#bmm_multitask_learning.variational.distr.build_predictive","title":"<code>build_predictive(pred_distr, classifier_distr, latent_distr, X, classifier_num_particles=1, latent_num_particles=1)</code>","text":"<p>Constructs torch.distribution as an approximation to the true predictive distribution (in bayessian sense) using variational distributions</p> <p>Parameters:</p> Name Type Description Default <code>pred_distr</code> <code>PredictiveDistr</code> <p>see MultiTaskElbo</p> required <code>classifier_distr</code> <code>Distribution</code> <p>see MultiTaskElbo</p> required <code>latent_distr</code> <code>LatentDistr</code> <p>see MultiTaskElbo</p> required <code>X</code> <code>Tensor</code> <p>new inputs for which to build predictive distr</p> required <code>classifier_num_particles</code> <code>int</code> <p>see MultiTaskElbo. Defaults to 1.</p> <code>1</code> <code>latent_num_particles</code> <code>int</code> <p>see MultiTaskElbo. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>MixtureSameFamily</code> <p>distr.MixtureSameFamily: the predictive distr can be seen as mixture distr</p> Source code in <code>bmm_multitask_learning/variational/distr.py</code> <pre><code>def build_predictive(\n    pred_distr: PredictiveDistr,\n    classifier_distr: distr.Distribution,\n    latent_distr: LatentDistr,\n    X: torch.Tensor,\n    classifier_num_particles: int = 1,\n    latent_num_particles: int = 1\n) -&gt; distr.MixtureSameFamily:\n    \"\"\"Constructs torch.distribution as an approximation to the true predictive distribution\n    (in bayessian sense) using variational distributions\n\n    Args:\n        pred_distr (PredictiveDistr): see MultiTaskElbo\n        classifier_distr (distr.Distribution): see MultiTaskElbo\n        latent_distr (LatentDistr): see MultiTaskElbo\n        X (torch.Tensor): new inputs for which to build predictive distr\n        classifier_num_particles (int, optional): see MultiTaskElbo. Defaults to 1.\n        latent_num_particles (int, optional): see MultiTaskElbo. Defaults to 1.\n\n    Returns:\n        distr.MixtureSameFamily: the predictive distr can be seen as mixture distr\n    \"\"\"\n    # sample hidden state (classifier + latent) from posterior\n    classifier_samples = classifier_distr.sample((classifier_num_particles, ))\n    latent_samples = latent_distr(X).sample((latent_num_particles, )).swapaxes(0, 1)\n    # build conditional distribution objects for target\n    pred_distr = pred_distr(latent_samples, classifier_samples)\n\n    mixing_distr = distr.Categorical(torch.ones(pred_distr.batch_shape))\n\n    return distr.MixtureSameFamily(mixing_distr, pred_distr)\n</code></pre>"},{"location":"variational/reference/#bmm_multitask_learning.variational.distr.kl_sample_estimation","title":"<code>kl_sample_estimation(distr_1, distr_2, num_particles=1)</code>","text":"<p>Make sample estimation of the KL divirgence</p> <p>Parameters:</p> Name Type Description Default <code>num_particles</code> <code>int</code> <p>number of samples for estimation. Defaults to 1.</p> <code>1</code> Source code in <code>bmm_multitask_learning/variational/distr.py</code> <pre><code>def kl_sample_estimation(\n    distr_1: distr.Distribution,\n    distr_2: distr.Distribution,\n    num_particles: int = 1\n) -&gt; torch.Tensor:\n    \"\"\"Make sample estimation of the KL divirgence\n\n    Args:\n        num_particles (int, optional): number of samples for estimation. Defaults to 1.\n    \"\"\"\n    samples = distr_1.rsample([num_particles])\n    log_p_1 = distr_1.log_prob(samples)\n    log_p_2 = distr_2.log_prob(samples)\n\n    return (log_p_1 - log_p_2).mean()\n</code></pre>"},{"location":"variational/reference/#bmm_multitask_learning.variational.elbo","title":"<code>bmm_multitask_learning.variational.elbo</code>","text":""},{"location":"variational/reference/#bmm_multitask_learning.variational.elbo.MultiTaskElbo","title":"<code>MultiTaskElbo</code>","text":"<p>               Bases: <code>Module</code></p> <p>General ELBO computer for variational multitask problem.</p> Source code in <code>bmm_multitask_learning/variational/elbo.py</code> <pre><code>class MultiTaskElbo(nn.Module):\n    \"\"\"General ELBO computer for variational multitask problem. \n    \"\"\"\n    def __init__(\n        self,\n        task_distrs: list[TargetDistr],\n        task_num_samples: list[int],\n        classifier_distr: list[distr.Distribution],\n        latent_distr: list[LatentDistr],\n        classifier_num_particles: int = 1,\n        latent_num_particles: int = 1,\n        temp_scheduler: Callable[[int], float] | Literal[\"const\"] = Literal[\"const\"],\n        kl_estimator_num_samples: int = 10\n    ):\n        \"\"\"\n        Args:\n            task_distrs (list[TargetDistr]): Data distribution for each task p_t(y | z, w)\n            task_num_samples (list[int]): Number of train samples for each task. Needed for unbiased ELBO computation in case of batched data.\n            classifier_distr (list[distr.Distribution]): Distribution for the classifier q(w | D)\n            latent_distr (list[LatentDistr]): Distribution for the latent state q(z | x, D)\n            classifier_num_particles (int, optional): num samples from classifier distr. Defaults to 1.\n            latent_num_particles (int, optional):  num samples from latent distr. Defaults to 1.\n            temp_scheduler (Callable[[int], float] | Literal[&amp;quot;const&amp;quot;], optional): _description_. Defaults to Literal[\"const\"].\n            kl_estimator_num_samples (int, optional): if your distrs does not have implicit kl computation, \n            it will be approximated using this number of samples. Defaults to 10.\n\n            Warning:\n                This nn.Module does not register nn.Parameters from the distributions inside itself\n        Raises:\n            ValueError: if number of tasks &lt;= 2\n        \"\"\"\n        super().__init__()\n\n        self.task_distrs = task_distrs\n        self.classifier_distr = classifier_distr\n        self.latent_distr = latent_distr\n\n        self.num_tasks = len(task_distrs)\n        if self.num_tasks &lt; 2:\n            raise ValueError(f\"Number of tasks should be &gt; 2, {self.num_tasks} was given\")\n        self.task_num_samples = task_num_samples\n        self.classifier_num_particles = classifier_num_particles\n        self.latent_num_particles = latent_num_particles\n        self.kl_estimator_num_samples = kl_estimator_num_samples\n\n        self.temp_scheduler = temp_scheduler if temp_scheduler is not \"const\" else lambda t: 1.\n\n        # define gumbel-softmax parameters for classifier and latent\n        # initialize uniform\n        self._classifier_mixings_params, self._latent_mixings_params = [\n            nn.Parameter(\n                torch.full((self.num_tasks, self.num_tasks), 1 / (self.num_tasks - 1))\n            )\n        ] * 2\n\n    def forward(self, data: list[torch.Tensor], targets: list[torch.Tensor], step: int) -&gt; torch.Tensor:\n        \"\"\"Computes ELBO estimation for variational multitask problem.\n\n        Args:\n            targets (list[torch.Tensor]): batched targets (y) for each task \n            data (list[torch.Tensor]): batched data (X) for each task \n            step: needed for temperature func\n\n        Returns:\n            torch.Tensor: ELBO estimation\n        \"\"\"\n        # get mixing values in form of matrix\n        temp = self.temp_scheduler(step)\n        classifier_mixing = self._get_gumbelsm_mixing(self._classifier_mixings_params, temp)\n        latent_mixing = self._get_gumbelsm_mixing(self._latent_mixings_params, temp)\n\n        # sample classifiers\n        # shape = (num_tasks, classifier_num_particles, classifier_shape)\n        classifiers = torch.stack(\n            list(\n                self.classifier_distr |\n                select(lambda d: d.rsample((self.classifier_num_particles, )))\n            )\n        )\n\n        # sample latents\n        # shape = [num_tasks, (num_samples(num_tasks), latent_num_particles, latent_shape)]\n        latents = []\n        for i, latent_cond_distr in enumerate(self.latent_distr):\n            latents.append(\n                latent_cond_distr(data[i]).rsample((self.latent_num_particles, )).swapaxes(0, 1)\n            )\n\n        # get log liklyhood for task + sampled averaged across latent and classifier particles\n        lh_per_task = []\n        for i in range(self.num_tasks):\n            cur_lh = self._compute_lh_per_task(i, latents[i], classifiers[i], targets[i])\n            lh_per_task.append(cur_lh)\n        # average lh samples across tasks\n        lh_val = torch.stack(lh_per_task).mean()\n\n        # get summed latents kl for each task\n        latents_kl = []\n        for i in range(self.num_tasks):\n            cur_data = data[i]\n            cur_mixing = latent_mixing[i]\n            cur_kl = self._compute_latent_kl_per_task(i, cur_data, cur_mixing)\n            latents_kl.append(cur_kl)\n        # average kl among tasks\n        latents_kl = torch.stack(latents_kl).mean()\n\n        # get classifiers kl for each task\n        classifiers_kl = []\n        for i in range(self.num_tasks):\n            cur_mixing = classifier_mixing[i]\n            cur_kl = self._compute_cls_kl_per_task(i, cur_mixing)\n            classifiers_kl.append(cur_kl)\n        # average kl among tasks\n        classifiers_kl = torch.stack(classifiers_kl).mean()\n\n        elbo = lh_val + latents_kl + classifiers_kl\n\n        return {\n            \"elbo\": elbo,\n            \"lh_loss\": lh_val,\n            \"lat_kl\": latents_kl,\n            \"cls_kl\": classifiers_kl\n        }\n\n    def _compute_lh_per_task(\n        self,\n        task_num: int,\n        latents: torch.Tensor,\n        classifiers: torch.Tensor,\n        targets: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute -log prob for each latent and classifier particle for each batch,\n        mean across classifiers and latents, sum across targets with batch size correction\n        \"\"\"\n        task_cond_distr = self.task_distrs[task_num]\n        target_shape = targets.shape[1:]\n        batch_size = targets.shape[0]\n\n        # log_prob shape=(batch_size, lat_num_part, classifier_num_part, target_shape)\n        return -task_cond_distr(latents, classifiers).log_prob(\n                targets[:, None, None, ...].expand(-1, self.latent_num_particles, self.classifier_num_particles, *target_shape)\n            ).mean(dim=(1, 2)).sum(dim=0) * (self.task_num_samples[task_num] / batch_size)\n\n    def _compute_latent_kl_per_task(\n        self,\n        task_num: int,\n        inputs: torch.Tensor,\n        latent_mixing: torch.Tensor\n    ) -&gt; torch.Tensor:\n        batch_size = inputs.shape[0]\n        cur_distr = self.latent_distr[task_num](inputs)\n\n        return torch.stack(\n            [self._compute_kl(cur_distr, lat_cond_distr(inputs)) for lat_cond_distr in self.latent_distr],\n            dim=1\n        ).matmul(latent_mixing).sum() * \\\n            (self.task_num_samples[task_num] / batch_size) # sum across batch with batch size correction\n\n    def _compute_cls_kl_per_task(\n        self,\n        task_num: int,\n        clas_mixing: torch.Tensor\n    ) -&gt; torch.Tensor:\n        cur_distr = self.classifier_distr[task_num]\n\n        return torch.stack(\n            [self._compute_kl(cur_distr, cl_cond_distr) for cl_cond_distr in self.classifier_distr]\n        ).dot(clas_mixing)\n\n    def _compute_kl(self, distr_1: distr.Distribution, distr_2: distr.Distribution) -&gt; torch.Tensor:\n        \"\"\"Computes KL analytically if possible else make a sample estimation\n        \"\"\"\n        if distr_1 is distr_2:\n            return torch.zeros(distr_1.batch_shape)\n\n        try:\n            return distr.kl_divergence(distr_1, distr_2)\n        except NotImplementedError:\n            return kl_sample_estimation(distr_1, distr_2, self.kl_estimator_num_samples)\n\n    def _get_gumbelsm_mixing(self, mixings_params: torch.Tensor, temp: float) -&gt; torch.Tensor:\n        # mixing with self is prohibited, so we mask diagonal to get zeros after softmax\n        mask = torch.diag(torch.full((self.num_tasks, ), -torch.inf))\n\n        mixing = distr.Gumbel(0., 1.).sample((self.num_tasks, self.num_tasks))\n        mixing += mixings_params.log()\n        mixing = mixing / temp\n        mixing += mask\n        mixing = torch.softmax(mixing, dim=1)\n\n        return mixing\n\n    @property\n    def classifier_mixings_params(self):\n        \"\"\"Accesses classifer mixing params\n        \"\"\"\n        return self._classifier_mixings_params\n\n    @property\n    def latent_mixings_params(self):\n        \"\"\"Accesses latent mixing params\n        \"\"\"\n        return self._latent_mixings_params\n</code></pre>"},{"location":"variational/reference/#bmm_multitask_learning.variational.elbo.MultiTaskElbo.classifier_mixings_params","title":"<code>classifier_mixings_params</code>  <code>property</code>","text":"<p>Accesses classifer mixing params</p>"},{"location":"variational/reference/#bmm_multitask_learning.variational.elbo.MultiTaskElbo.latent_mixings_params","title":"<code>latent_mixings_params</code>  <code>property</code>","text":"<p>Accesses latent mixing params</p>"},{"location":"variational/reference/#bmm_multitask_learning.variational.elbo.MultiTaskElbo.__init__","title":"<code>__init__(task_distrs, task_num_samples, classifier_distr, latent_distr, classifier_num_particles=1, latent_num_particles=1, temp_scheduler=Literal['const'], kl_estimator_num_samples=10)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>task_distrs</code> <code>list[TargetDistr]</code> <p>Data distribution for each task p_t(y | z, w)</p> required <code>task_num_samples</code> <code>list[int]</code> <p>Number of train samples for each task. Needed for unbiased ELBO computation in case of batched data.</p> required <code>classifier_distr</code> <code>list[Distribution]</code> <p>Distribution for the classifier q(w | D)</p> required <code>latent_distr</code> <code>list[LatentDistr]</code> <p>Distribution for the latent state q(z | x, D)</p> required <code>classifier_num_particles</code> <code>int</code> <p>num samples from classifier distr. Defaults to 1.</p> <code>1</code> <code>latent_num_particles</code> <code>int</code> <p>num samples from latent distr. Defaults to 1.</p> <code>1</code> <code>temp_scheduler</code> <code>Callable[[int], float] | Literal[&amp;quot;const&amp;quot;]</code> <p>description. Defaults to Literal[\"const\"].</p> <code>Literal['const']</code> <code>kl_estimator_num_samples</code> <code>int</code> <p>if your distrs does not have implicit kl computation, </p> <code>10</code> <code>Warning</code> <p>This nn.Module does not register nn.Parameters from the distributions inside itself</p> required <p>Raises:     ValueError: if number of tasks &lt;= 2</p> Source code in <code>bmm_multitask_learning/variational/elbo.py</code> <pre><code>def __init__(\n    self,\n    task_distrs: list[TargetDistr],\n    task_num_samples: list[int],\n    classifier_distr: list[distr.Distribution],\n    latent_distr: list[LatentDistr],\n    classifier_num_particles: int = 1,\n    latent_num_particles: int = 1,\n    temp_scheduler: Callable[[int], float] | Literal[\"const\"] = Literal[\"const\"],\n    kl_estimator_num_samples: int = 10\n):\n    \"\"\"\n    Args:\n        task_distrs (list[TargetDistr]): Data distribution for each task p_t(y | z, w)\n        task_num_samples (list[int]): Number of train samples for each task. Needed for unbiased ELBO computation in case of batched data.\n        classifier_distr (list[distr.Distribution]): Distribution for the classifier q(w | D)\n        latent_distr (list[LatentDistr]): Distribution for the latent state q(z | x, D)\n        classifier_num_particles (int, optional): num samples from classifier distr. Defaults to 1.\n        latent_num_particles (int, optional):  num samples from latent distr. Defaults to 1.\n        temp_scheduler (Callable[[int], float] | Literal[&amp;quot;const&amp;quot;], optional): _description_. Defaults to Literal[\"const\"].\n        kl_estimator_num_samples (int, optional): if your distrs does not have implicit kl computation, \n        it will be approximated using this number of samples. Defaults to 10.\n\n        Warning:\n            This nn.Module does not register nn.Parameters from the distributions inside itself\n    Raises:\n        ValueError: if number of tasks &lt;= 2\n    \"\"\"\n    super().__init__()\n\n    self.task_distrs = task_distrs\n    self.classifier_distr = classifier_distr\n    self.latent_distr = latent_distr\n\n    self.num_tasks = len(task_distrs)\n    if self.num_tasks &lt; 2:\n        raise ValueError(f\"Number of tasks should be &gt; 2, {self.num_tasks} was given\")\n    self.task_num_samples = task_num_samples\n    self.classifier_num_particles = classifier_num_particles\n    self.latent_num_particles = latent_num_particles\n    self.kl_estimator_num_samples = kl_estimator_num_samples\n\n    self.temp_scheduler = temp_scheduler if temp_scheduler is not \"const\" else lambda t: 1.\n\n    # define gumbel-softmax parameters for classifier and latent\n    # initialize uniform\n    self._classifier_mixings_params, self._latent_mixings_params = [\n        nn.Parameter(\n            torch.full((self.num_tasks, self.num_tasks), 1 / (self.num_tasks - 1))\n        )\n    ] * 2\n</code></pre>"},{"location":"variational/reference/#bmm_multitask_learning.variational.elbo.MultiTaskElbo.forward","title":"<code>forward(data, targets, step)</code>","text":"<p>Computes ELBO estimation for variational multitask problem.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>list[Tensor]</code> <p>batched targets (y) for each task </p> required <code>data</code> <code>list[Tensor]</code> <p>batched data (X) for each task </p> required <code>step</code> <code>int</code> <p>needed for temperature func</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: ELBO estimation</p> Source code in <code>bmm_multitask_learning/variational/elbo.py</code> <pre><code>def forward(self, data: list[torch.Tensor], targets: list[torch.Tensor], step: int) -&gt; torch.Tensor:\n    \"\"\"Computes ELBO estimation for variational multitask problem.\n\n    Args:\n        targets (list[torch.Tensor]): batched targets (y) for each task \n        data (list[torch.Tensor]): batched data (X) for each task \n        step: needed for temperature func\n\n    Returns:\n        torch.Tensor: ELBO estimation\n    \"\"\"\n    # get mixing values in form of matrix\n    temp = self.temp_scheduler(step)\n    classifier_mixing = self._get_gumbelsm_mixing(self._classifier_mixings_params, temp)\n    latent_mixing = self._get_gumbelsm_mixing(self._latent_mixings_params, temp)\n\n    # sample classifiers\n    # shape = (num_tasks, classifier_num_particles, classifier_shape)\n    classifiers = torch.stack(\n        list(\n            self.classifier_distr |\n            select(lambda d: d.rsample((self.classifier_num_particles, )))\n        )\n    )\n\n    # sample latents\n    # shape = [num_tasks, (num_samples(num_tasks), latent_num_particles, latent_shape)]\n    latents = []\n    for i, latent_cond_distr in enumerate(self.latent_distr):\n        latents.append(\n            latent_cond_distr(data[i]).rsample((self.latent_num_particles, )).swapaxes(0, 1)\n        )\n\n    # get log liklyhood for task + sampled averaged across latent and classifier particles\n    lh_per_task = []\n    for i in range(self.num_tasks):\n        cur_lh = self._compute_lh_per_task(i, latents[i], classifiers[i], targets[i])\n        lh_per_task.append(cur_lh)\n    # average lh samples across tasks\n    lh_val = torch.stack(lh_per_task).mean()\n\n    # get summed latents kl for each task\n    latents_kl = []\n    for i in range(self.num_tasks):\n        cur_data = data[i]\n        cur_mixing = latent_mixing[i]\n        cur_kl = self._compute_latent_kl_per_task(i, cur_data, cur_mixing)\n        latents_kl.append(cur_kl)\n    # average kl among tasks\n    latents_kl = torch.stack(latents_kl).mean()\n\n    # get classifiers kl for each task\n    classifiers_kl = []\n    for i in range(self.num_tasks):\n        cur_mixing = classifier_mixing[i]\n        cur_kl = self._compute_cls_kl_per_task(i, cur_mixing)\n        classifiers_kl.append(cur_kl)\n    # average kl among tasks\n    classifiers_kl = torch.stack(classifiers_kl).mean()\n\n    elbo = lh_val + latents_kl + classifiers_kl\n\n    return {\n        \"elbo\": elbo,\n        \"lh_loss\": lh_val,\n        \"lat_kl\": latents_kl,\n        \"cls_kl\": classifiers_kl\n    }\n</code></pre>"},{"location":"coverage/","title":"Coverage report","text":""}]}